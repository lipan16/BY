实战

Python爬虫开发与项目实战

范传辉　编著

本书由“行行”整理，如果你不知道读什么书或者想获得更多免费电子书请加小编微信或QQ：2338856113 小编也和结交一些喜欢读书的朋友 或者关注小编个人微信公众号名称：幸福的味道 为了方便书友朋友找书和看书，小编自己做了一个电子书下载网站，网站的名称为：周读 网址：www.ireadweek.com





目录

前言

基础篇

第1章　回顾Python编程

1.1　安装Python

1.2　搭建开发环境

1.3　IO编程

1.4　进程和线程

1.5　网络编程

1.6　小结

第2章　Web前端基础

2.1　W3C标准

2.2　HTTP标准

2.3　小结

第3章　初识网络爬虫

3.1　网络爬虫概述

3.2　HTTP请求的Python实现

3.3　小结

第4章　HTML解析大法

4.1　初识Firebug

4.2　正则表达式

4.3　强大的BeautifulSoup

4.4　小结

第5章　数据存储（无数据库版）

5.1　HTML正文抽取

5.2　多媒体文件抽取

5.3　Email提醒

5.4　小结

第6章　实战项目：基础爬虫

6.1　基础爬虫架构及运行流程

6.2　URL管理器

6.3　HTML下载器

6.4　HTML解析器

6.5　数据存储器

6.6　爬虫调度器

6.7　小结

第7章　实战项目：简单分布式爬虫

7.1　简单分布式爬虫结构

7.2　控制节点

7.3　爬虫节点

7.4　小结

中级篇

第8章　数据存储（数据库版）

8.1　SQLite

8.2　MySQL

8.3　更适合爬虫的MongoDB

8.4　小结

第9章　动态网站抓取

9.1　Ajax和动态HTML

9.2　动态爬虫1：爬取影评信息

9.3　PhantomJS

9.4　Selenium

9.5　动态爬虫2：爬取去哪网

9.6　小结

第10章　Web端协议分析

10.1　网页登录POST分析

10.2　验证码问题

10.3　www>m>wap

10.4　小结

第11章　终端协议分析

11.1　PC客户端抓包分析

11.2　App抓包分析

11.3　API爬虫：爬取mp3资源信息

11.4　小结

第12章　初窥Scrapy爬虫框架

12.1　Scrapy爬虫架构





12.2　安装Scrapy


12.3　创建cnblogs项目

12.4　创建爬虫模块

12.5　选择器

12.6　命令行工具

12.7　定义Item

12.8　翻页功能

12.9　构建Item　Pipeline

12.10　内置数据存储

12.11　内置图片和文件下载方式

12.12　启动爬虫

12.13　强化爬虫

12.14　小结

第13章　深入Scrapy爬虫框架

13.1　再看Spider

13.2　Item　Loader

13.3　再看Item　Pipeline

13.4　请求与响应

13.5　下载器中间件

13.6　Spider中间件

13.7　扩展

13.8　突破反爬虫

13.9　小结

第14章　实战项目：Scrapy爬虫

14.1　创建知乎爬虫

14.2　定义Item

14.3　创建爬虫模块

14.4　Pipeline

14.5　优化措施

14.6　部署爬虫

14.7　小结

深入篇

第15章　增量式爬虫

15.1　去重方案

15.2　BloomFilter算法

15.3　Scrapy和BloomFilter

15.4　小结

第16章　分布式爬虫与Scrapy

16.1　Redis基础

16.2　Python和Redis

16.3　MongoDB集群

16.4　小结

第17章　实战项目：Scrapy分布式爬虫

17.1　创建云起书院爬虫

17.2　定义Item

17.3　编写爬虫模块

17.4　Pipeline

17.5　应对反爬虫机制

17.6　去重优化

17.7　小结

第18章　人性化PySpider爬虫框架

18.1　PySpider与Scrapy

18.2　安装PySpider

18.3　创建豆瓣爬虫

18.4　选择器

18.5　Ajax和HTTP请求

18.6　PySpider和PhantomJS

18.7　数据存储

18.8　PySpider爬虫架构

18.9　小结





如果你不知道读什么书，

就关注这个微信号。



微信公众号名称：幸福的味道

加小编微信一起读书

小编微信号：2338856113



【幸福的味道】已提供200个不同类型的书单

1、 历届茅盾文学奖获奖作品

2、 每年豆瓣，当当，亚马逊年度图书销售排行榜

3、 25岁前一定要读的25本书

4、 有生之年，你一定要看的25部外国纯文学名著

5、 有生之年，你一定要看的20部中国现当代名著

6、 美国亚马逊编辑推荐的一生必读书单100本

7、 30个领域30本不容错过的入门书

8、 这20本书，是各领域的巅峰之作

9、 这7本书，教你如何高效读书

10、 80万书虫力荐的“给五星都不够”的30本书

关注“幸福的味道”微信公众号，即可查看对应书单和得到电子书

也可以在我的网站（周读）www.ireadweek.com 自行下载





前言


为什么写这本书

当你看前言的时候，不得不说你做出了一个聪明的选择，因为前言中有作者对整本书的概括和学习建议，这会对大家之后的阅读产生事半功倍的效果。在聊这本书之前，首先给大家一个本书所有配套源码和说明的链接：https://github.com/qiyeboy/SpiderBook 。大家可以在Github中对不懂的内容进行提问，我会尽可能地帮助大家解决问题。其实在前言开头放这个链接是挺突兀的，不过确实是担心大家不会完整地看完前言。

接下来聊一聊这本书，写这本书的原因来自于我个人的微信公众号：七夜安全博客。我经常在博客园、知乎和微信平台上发布技术文章，分享一些知识和见解，有很多热心的朋友愿意和我进行交流讨论。记得2016年4月初的某一天，有一个朋友在微信后台留言，问我怎样将Python爬虫技术学好，有什么书籍可以推荐。我当时回答了好长一段建议，但是那个朋友依然希望能推荐一本书籍帮助入门和提高。其实我特别能理解初学者的心情，毕竟我也是从初学者走过来的，但是确实挺纠结，不知从何推荐。于是，我专门找了一下这方面的书籍，只找到一本外国人写的书，中文版刚出版没多久，名字为《Python网络数据采集》。我花了半天看了一下里面的内容，整本书条理比较清晰，容易理解，但是很多知识点都谈得很浅，系统的实战项目基本上没有，更多的是一些代码片段，仅仅适合一些刚刚入门的朋友。自从这件事情以后，我就下定决心写一本Python爬虫方面的书籍，既然国内还没有人写这方面的书籍，我愿意做一个抛砖引玉的人，帮助大家更好地学习爬虫技术。

有了写书的想法后，开始列提纲，确定书的主题和内容。由于爬虫是一项实践性很强的技术，因此书的主题是以实战项目为驱动，由浅及深地讲解爬虫技术，希望你看这本书的时候是个菜鸟，认真学习完之后不再是个菜鸟，可以自主地开发Python爬虫项目了。从写书的那一刻开始，我就知道在书写完之前，我应该是没有周末了。这本书写了大半年的时间，由于我平时有写笔记、做总结的习惯，因此写书的时间不是特别长，不过直到2017年年初我依然在更新内容，毕竟爬虫技术更新得比较快，我努力将比较新的知识贡献给大家。

在写书的过程中，我的内心变得越来越平静，越来越有耐心，不断地修改更新，对每个实战项目进行反复验证和敲定，尽可能地贴近初学者的需求，希望能帮助他们完成蜕变。

最后做一下自我介绍，本人是一位信息安全研究人员，比较擅长网络安全、软件逆向，同时对大数据、机器学习和深度学习有非常浓厚的兴趣，欢迎大家和我交流，共同进步。

前路多艰，学习的道路不可能一帆风顺，爬虫技术只是个开始，愿与诸君一道共克难关。

本书结构

本书总共分为三个部分：基础篇、中级篇和深入篇。

基础篇包括第1～7章，主要讲解了什么是网络爬虫、如何分析静态网站、如何开发一个完整的爬虫。

第1～2章帮助大家回顾了Python和Web方面的知识，主要是为之后的爬虫学习打下基础，毕竟之后要和Python、Web打交道。

第3～5章详细介绍了什么是网络爬虫、如何分析静态网站、如何从HTML页面中提取出有效的数据，以及对如何将数据合理地存储成各类文件以实现持久化。

第6～7章包含了两个实战项目。第一个项目是基础爬虫，也就是一个单机爬虫，功能是爬取百度百科的词条，并据此讲解了一个爬虫所应该具有的全部功能组件以及编码实现。第二个项目是分布式爬虫，功能和基础爬虫一致，在单机爬虫的基础上进行分布式改进，帮助大家从根本上了解分布式爬虫，消除分布式爬虫的神秘感。

中级篇包括第8～14章，主要讲解了三种数据库的存储方式、动态网站的抓取、协议分析和Scrapy爬虫框架。

第8章详细介绍了SQLite、MySQL和MongoDB三种数据库的操作方式，帮助大家实现爬取数据存储的多样化。

第9章主要讲解了动态网站分析和爬取的两种思路，并通过两个实战项目帮助大家理解。

第10章首先探讨了爬虫开发中遇到的两个问题——登录爬取问题和验证码问题，并提供了解决办法和分析实例。接着对Web端的爬取提供了另外的思路，当在PC网页端爬取遇到困难时，爬取方式可以向手机网页端转变。

第11章接着延伸第10章的问题，又提出了两种爬取思路。当在网页站点爬取遇到困难时，爬取思路可以向PC客户端和移动客户端转变，并通过两个实战项目帮助大家了解实施过程。

第12～14章由浅及深地讲解了著名爬虫框架Scrapy的运用，并通过知乎爬虫这个实战项目演示了Scrapy开发和部署爬虫的整个过程。

深入篇为第15～18章，详细介绍了大规模爬取中的去重问题以及如何通过Scrapy框架开发分布式爬虫，最后又介绍了一个较新的爬虫框架PySpider。

第15章主要讲解了海量数据的去重方式以及各种去重方式的优劣比较。

第16～17章详细介绍了如何通过Redis和Scrapy的结合实现分布式爬虫，并通过云起书院实战项目帮助大家了解整个的实现过程以及注意事项。

第18章介绍了一个较为人性化的爬虫框架PySpider，并通过爬取豆瓣读书信息来演示其基本功能。

以上就是本书的全部内容，看到以上介绍之后，是不是有赶快阅读的冲动呢？不要着急，接着往下看。

本书特点及建议

本书总体来说是一本实战型书籍，以大量系统的实战项目为驱动，由浅及深地讲解了爬虫开发中所需的知识和技能。本书是一本适合初学者的书籍，既有对基础知识点的讲解，也涉及关键问题和难点的分析和解决，本书的初衷是帮助初学者夯实基础，实现提高。还有一点要说明，这本书对编程能力是有一定要求的，希望读者尽量熟悉Python编程。

对于学习本书有两点建议，希望能引起读者的注意。第一点，读者可根据自己的实际情况选择性地学习本书的章节，假如之前学过Python或者Web前端的知识，前两章就可以蜻蜓点水地看一下。第二点，本书中的实战项目是根据当时网页的情况进行编写的，可能当书籍出版的时候，网页的解析规则发生改变而使项目代码失效，因此大家从实战项目中应该学习分析过程和编码的实现方式，而不是具体的代码，授人以渔永远比授人以鱼更加有价值，即使代码失效了，大家也可以根据实际情况进行修改。

致谢





写完这本书，才感觉到写书不是一件容易的事情，挺耗费心血的。不过除此之外，更多的是一种满足感，像一种别样的创业，既紧张又刺激，同时也实现了我分享知识的心愿，算是做了一件值得回忆的事情。这是我写的第一本书，希望是一次有益的尝试。


感谢父母的养育之恩，是他们的默默付出支持我走到今天。

感谢我的女朋友，在每个写书的周末都没有办法陪伴她，正是她的理解和支持才让我如此准时地完稿。

感谢长春理工大学电子学会实验室，如果没有当年实验室的培养，没有兄弟们的同甘共苦，就没有今天的我。

感谢西安电子科技大学，它所营造的氛围使我的视野更加开阔，使我的技术水平更上一层楼。

感谢机械工业出版社的吴怡编辑，没有她的信任和鼓励，就没有这本书的顺利出版。

感谢Python中文社区的大力支持。

感谢本书中所用开源项目的作者，正是他们无私的奉献才有了开发的便利。

由于作者水平有限，书中难免有误，欢迎各位业界同仁斧正！





基础篇


·第1章　回顾Python编程

·第2章　Web前端基础

·第3章　初识网络爬虫

·第4章　HTML解析大法

·第5章　数据存储（无数据库版）

·第6章　实战项目：基础爬虫

·第7章　实战项目：简单分布式爬虫





第1章　回顾Python编程


本书所要讲解的爬虫技术是基于Python语言进行开发的，拥有Python编程能力对于本书的学习是至关重要的，因此本章的目标是帮助之前接触过Python语言的读者回顾一下Python编程中的内容，尤其是与爬虫技术相关的内容。





1.1　安装Python


Python是跨平台语言，它可以运行在Windows、Mac和各种Linux/Unix系统上。在Windows上编写的程序，可以在Mac和Linux上正常运行。Python是一种面向对象、解释型计算机程序设计语言，需要Python解释器进行解释运行。目前，Python有两个版本，一个是2.x版，一个是3.x版，这两个版本是不兼容的。现在Python的整体方向是朝着3.x发展的，但是在发展过程中，大量针对2.x版本的代码都需要修改才能运行，导致现在许多第三方库无法在3.x版本上直接使用，因此现在大部分的云服务器默认的Python版本依然是2.x版。考虑到上述原因，本书采用的Python版本为2.x，确切地说是2.7版本。





1.1.1　Windows上安装Python


首先，从Python的官方网站www.python.org 下载最新的2.7.12版本，地址是https://www.python.org/ftp/python/2.7.12/python-2.7.12.msi 。然后，运行下载的MSI安装包，在选择安装组件时，勾选上所有的组件，如图1-1所示。



图1-1　Python安装界面

特别要注意勾选pip和Add python.exe to Path，然后一路点击Next即可完成安装。

pip是Python安装扩展模块的工具，通常会用pip下载扩展模块的源代码并编译安装。

Add python.exe to Path是将Python添加到Windows环境中。

安装完成后，打开命令提示窗口，输入python后出现如图1-2情况，说明Python安装成功。

当看到提示符“>>>”就表示我们已经在Python交互式环境中了，可以输入任何Python代码，回车后会立刻得到执行结果。现在，输入exit（）并回车，就可以退出Python交互式环境。





1.1.2　Ubuntu上的Python


本书采用Ubuntu 16.04版本，系统自带了Python 2.7.11的环境，如图1-3所示，所以不需要额外进行安装。



图1-2　Python命令行窗口



图1-3　Python环境

拥有了Python环境，但为了以后方便安装扩展模块，还需要安装python-pip和python-dev，在shell中执行：sudo apt-get install python-pip python-dev即可安装，如图1-4所示。



图1-4　安装pip和python-dev





1.2　搭建开发环境


俗话说：“工欲善其事必先利其器”，在做Python爬虫开发之前，一个好的IDE将会使编程效率得到大幅度提高。下面主要介绍两种IDE：Eclipse和PyCharm，并以在Windows 7上安装为例进行介绍。





1.2.1　Eclipse+PyDev


Eclipse是一个强大的编辑器，并通过插件的方式不断拓展功能。Eclipse比较常见的功能是编写Java程序，但是通过扩展PyDev插件，Eclipse就具有了编写Python程序的功能。所以本书搭建的开发环境是Eclipset+PyDev。

Eclipse是运行在Java虚拟机上的，所以要先安装Java环境。

第一步， 安装Java环境。Java JDK的下载地址为：http://www.oracle.com/technetwork/java/javase/downloads/index.html 。下载页面如图1-5所示。



图1-5　JDK下载界面

下载好JDK之后，双击进行安装，一直点击“下一步”即可完成安装，安装界面如图1-6所示。

安装完JDK，需要配置Java环境变量。

1）首先右键“我的电脑”，选择“属性”，如图1-7所示。

2）接着在出现的对话框中选择“高级系统设置”，如图1-8所示。



图1-6　JDK安装界面



图1-7　电脑属性



图1-8　高级系统设置

3）在出现的对话框中选择“环境变量”，如图1-9所示。

4）新建名为classpath的变量名，变量的值可以设置为：.；%JAVA_HOME\lib；%JAVA_HOME\lib\tools.jar，如图1-10所示。

5）新建名为JAVA_HOME的变量名，变量的值为之前安装的JDK路径位置，默认是C：\Program Files\Java\jdk1.8.0_101\，如图1-11所示。

6）在已有的系统变量path的变量值中加上：；%JAVA_HOME%\bin；%JAVA_HOME%\jre\bin，如图1-12所示，自此配置完成。

下面检验是否配置成功，运行cmd命令，在出现的对话框中输入“java-version”命令，如果出现图1-13的结果，则表明配置成功。

第二步， 下载Eclipse，下载地址为：http://www.eclipse.org/downloads/eclipse-packages/ ，下载完后，解压就可以直接使用，Eclipse不需要安装。下载界面如图1-14所示。

第三步， 在Eclipse中安装pydev插件。启动Eclipse，点击Help->Install New Software...，如图1-15所示。



图1-9　环境变量



图1-10　classpath环境变量



图1-11　JAVA_HOME环境变量



图1-12　path环境变量



图1-13　java-version



图1-14　下载界面



图1-15　安装新软件

在弹出的对话框中，点击Add按钮。在Name中填：Pydev，在Location中填http://pydev.org/updates ，然后一步一步安装下去。过程如图1-16和图1-17所示。



图1-16　安装过程1



图1-17　安装过程2

第四步， 安装完pydev插件后，需要配置pydev解释器。在Eclipse菜单栏中，点击Windows→Preferences。在对话框中，点击PyDev→Interpreter-Python。点击New按钮，选择python.exe的路径，打开后显示出一个包含很多复选框的窗口，点击OK即可，如图1-18所示。

经过上述四个步骤，Eclipse就可以进行Python开发了。如需创建一个新的项目，选择File→New→Projects...，再选择PyDev→PyDevProject并输入项目名称，点击Finish即可完成项目的创建，如图1-19所示。

然后新建PyDev Package，就可以写代码了，如图1-20所示。



图1-18　配置PyDev



图1-19　新建Python工程



图1-20　新建Python包





1.2.2　PyCharm


PyCharm是本人用过的Python编辑器中，比较顺手，而且可以跨平台，在MacOS、Linux和Windows下都可以用。PyCharm主要分为专业版和社区版，两者的区别在于专业版一开始有30天的试用期，之后就要收费；社区版一直免费，当然专业版的功能更加强大。我们进行Python爬虫开发，社区版基本上可以满足需要，所以接下来就以社区版为例。大家可以根据自己的系统版本，进行下载安装，下载地址为：http://www.jetbrains.com/pycharm/download/# 。下载界面如图1-21所示。



图1-21　下载界面

以Windows为例，下载后双击进行安装，一步一步点击Next，即可完成安装。安装界面如图1-22所示。

安装完成后，运行PyCharm，创建Python项目就可以进行Python开发了，如图1-23所示。



图1-22　安装界面



图1-23　创建项目开发





1.3　IO编程


IO在计算机中指的是Input/Output，也就是输入输出。凡是用到数据交换的地方，都会涉及IO编程，例如磁盘、网络的数据传输。在IO编程中，Stream（流）是一种重要的概念，分为输入流（Input Stream）和输出流（Output Stream）。我们可以把流理解为一个水管，数据相当于水管中的水，但是只能单向流动，所以数据传输过程中需要架设两个水管，一个负责输入，一个负责输出，这样读写就可以实现同步。本节主要讲解磁盘IO操作，网络IO操作放到之后的1.5节进行讨论。





1.3.1　文件读写


1.打开文件

读写文件是最常见的IO操作。Python内置了读写文件的函数，方便了文件的IO操作。

文件读写之前需要打开文件，确定文件的读写模式。open函数用来打开文件，语法如下：



* * *



open(name[.mode[.buffering]])



* * *



open函数使用一个文件名作为唯一的强制参数，然后返回一个文件对象。模式（mode）和缓冲区（buffering）参数都是可选的，默认模式是读模式，默认缓冲区是无。

假设有个名为qiye.txt的文本文件，其存储路径是c：\text（或者是在Linux下的~/text），那么可以像下面这样打开文件。在交互式环境的提示符“>>>”下，输入如下内容：



* * *



>>> f = open(r'c:\text\qiye.txt')



* * *



如果文件不存在，将会看到一个类似下面的异常回溯：



* * *



Traceback (most recent call last): File "<stdin>", line 1, in <module> IOError: [Errno 2] No such file or directory: 'C:\\qiye.txt'



* * *



2.文件模式

下面主要说一下open函数中的mode参数（如表1-1所示），通过改变mode参数可以实现对文件的不同操作。

表1-1　open函数中的mode参数



这里主要是提醒一下‘b’参数的使用，一般处理文本文件时，是用不到‘b’参数的，但处理一些其他类型的文件（二进制文件），比如mp3音乐或者图像，那么应该在模式参数中增加‘b’，这在爬虫中处理媒体文件很常用。参数‘rb’可以用来读取一个二进制文件。

3.文件缓冲区

open函数中第三个可选参数buffering控制着文件的缓冲。如果参数是0，I/O操作就是无缓冲的，直接将数据写到硬盘上；如果参数是1，I/O操作就是有缓冲的，数据先写到内存里，只有使用flush函数或者close函数才会将数据更新到硬盘；如果参数为大于1的数字则代表缓冲区的大小（单位是字节），-1（或者是任何负数）代表使用默认缓冲区的大小。

4.文件读取

文件读取主要是分为按字节读取和按行进行读取，经常用到的方法有read（）、readlines（）、close（）。

在“>>>”输入f=open（r‘c：\text\qiye.txt’）后，如果成功打开文本文件，接下来调用read（）方法则可以一次性将文件内容全部读到内存中，最后返回的是str类型的对象：



* * *



>>> f.read() "qiye"



* * *



最后一步调用close（），可以关闭对文件的引用。文件使用完毕后必须关闭，因为文件对象会占用操作系统资源，影响系统的IO操作。



* * *



>>> f.close()



* * *



由于文件操作可能会出现IO异常，一旦出现IO异常，后面的close（）方法就不会调用。所以为了保证程序的健壮性，我们需要使用try...finally来实现。



* * *



try: f = open(r'c:\text\qiye.txt','r') print f.read() finally: if f: f.close()



* * *



上面的代码略长，Python提供了一种简单的写法，使用with语句来替代try...finally代码块和close（）方法，如下所示：



* * *



with open(r'c:\text\qiye.txt','r') as fileReader: print fileReader.read()



* * *



调用read（）一次将文件内容读到内存，但是如果文件过大，将会出现内存不足的问题。一般对于大文件，可以反复调用read（size）方法，一次最多读取size个字节。如果文件是文本文件，Python提供了更加合理的做法，调用readline（）可以每次读取一行内容，调用readlines（）一次读取所有内容并按行返回列表。大家可以根据自己的具体需求采取不同的读取方式，例如小文件可以直接采取read（）方法读到内存，大文件更加安全的方式是连续调用read（size），而对于配置文件等文本文件，使用readline（）方法更加合理。

将上面的代码进行修改，采用readline（）的方式实现如下所示：



* * *



with open(r'c:\text\qiye.txt','r') as fileReader: for line in fileReader.readlines(): print line.strip()



* * *



5.文件写入

写文件和读文件是一样的，唯一的区别是在调用open方法时，传入标识符‘w’或者‘wb’表示写入文本文件或者写入二进制文件，示例如下：



* * *



f = open(r'c:\text\qiye.txt','w') f.write('qiye') f.close()



* * *



我们可以反复调用write（）方法写入文件，最后必须使用close（）方法来关闭文件。使用write（）方法的时候，操作系统不是立即将数据写入文件中的，而是先写入内存中缓存起来，等到空闲时候再写入文件中，最后使用close（）方法就将数据完整地写入文件中了。当然也可以使用f.flush（）方法，不断将数据立即写入文件中，最后使用close（）方法来关闭文件。和读文件同样道理，文件操作中可能会出现IO异常，所以还是推荐使用with语句：



* * *



with open(r'c:\text\qiye.txt','w') as fileWriter: fileWriter.write('qiye')



* * *





1.3.2　操作文件和目录


在Python中对文件和目录的操作经常用到os模块和shutil模块。接下来主要介绍一些操作文件和目录的常用方法：

·获得当前Python脚本工作的目录路径：os.getcwd（）。

·返回指定目录下的所有文件和目录名：os.listdir（）。例如返回C盘下的文件：os.listdir（“C：\\”）

·删除一个文件：os.remove（filepath）。

·删除多个空目录：os.removedirs（r“d：\python”）。

·检验给出的路径是否是一个文件：os.path.isfile（filepath）。

·检验给出的路径是否是一个目录：os.path.isdir（filepath）。

·判断是否是绝对路径：os.path.isabs（）。





·检验路径是否真的存在：os.path.exists（）。例如检测D盘下是否有Python文件夹：os.path.exists（r“d：\python”）


·分离一个路径的目录名和文件名：os.path.split（）。例如：

·os.path.split（r“/home/qiye/qiye.txt”），返回结果是一个元组：（‘/home/qiye’，‘qiye.txt’）。

·分离扩展名：os.path.splitext（）。例如os.path.splitext（r“/home/qiye/qiye.txt”），返回结果是一个元组：（‘/home/qiye/qiye’，‘.txt’）。

·获取路径名：os.path.dirname（filetpah）。

·获取文件名：os.path.basename（filepath）。

·读取和设置环境变量：os.getenv（）与os.putenv（）。

·给出当前平台使用的行终止符：os.linesep。Windows使用‘\r\n’，Linux使用‘\n’而Mac使用‘\r’。

·指示你正在使用的平台：os.name。对于Windows，它是‘nt’，而对于Linux/Unix用户，它是‘posix’。

·重命名文件或者目录：os.rename（old，new）。

·创建多级目录：os.makedirs（r“c：\python\test”）。

·创建单个目录：os.mkdir（“test”）。

·获取文件属性：os.stat（file）。

·修改文件权限与时间戳：os.chmod（file）。

·获取文件大小：os.path.getsize（filename）。

·复制文件夹：shutil.copytree（“olddir”，“newdir”）。olddir和newdir都只能是目录，且newdir必须不存在。

·复制文件：shutil.copyfile（“oldfile”，“newfile”），oldfile和newfile都只能是文件；shutil.copy（“oldfile”，“newfile”），oldfile只能是文件，newfile可以是文件，也可以是目标目录。

·移动文件（目录）：shutil.move（“oldpos”，“newpos”）。

·删除目录：os.rmdir（“dir”），只能删除空目录；shutil.rmtree（“dir”），空目录、有内容的目录都可以删。





1.3.3　序列化操作


对象的序列化在很多高级编程语言中都有相应的实现，Python也不例外。程序运行时，所有的变量都是在内存中的，例如在程序中声明一个dict对象，里面存储着爬取的页面的链接、页面的标题、页面的摘要等信息：



* * *



d = dict(url='index.html',title='首页',content='首页')



* * *



在程序运行的过程中爬取的页面的链接会不断变化，比如把url改成了second.html，但是程序一结束或意外中断，程序中的内存变量都会被操作系统进行回收。如果没有把修改过的url存储起来，下次运行程序的时候，url被初始化为index.html，又是从首页开始，这是我们不愿意看到的。所以把内存中的变量变成可存储或可传输的过程，就是序列化。

将内存中的变量序列化之后，可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上，实现程序状态的保存和共享。反过来，把变量内容从序列化的对象重新读取到内存，称为反序列化。

在Python中提供了两个模块：cPickle和pickle来实现序列化，前者是由C语言编写的，效率比后者高很多，但是两个模块的功能是一样的。一般编写程序的时候，采取的方案是先导入cPickle模块，如果此模块不存在，再导入pickle模块。示例如下：



* * *



try: import cPickle as pickle except ImportError: import pickle



* * *



pickle实现序列化主要使用的是dumps方法或dump方法。dumps方法可以将任意对象序列化成一个str，然后可以将这个str写入文件进行保存。在Python Shell中示例如下：



* * *



>>> import cPickle as pickle >>> d = dict(url='index.html',title='首页',content='首页') >>> pickle.dumps(d) "(dp1\nS'content'\np2\nS'\\xca\\xd7\\xd2\\xb3'\np3\nsS'url'\np4\nS'index.html'\n p5\nsS'title'\np6\ng3\ns."



* * *



如果使用dump方法，可以将序列化后的对象直接写入文件中：



* * *



>>> f=open(r'D:\dump.txt','wb') >>> pickle.dump(d,f) >>> f.close()



* * *



pickle实现反序列化使用的是loads方法或load方法。把序列化后的文件从磁盘上读取为一个str，然后使用loads方法将这个str反序列化为对象，或者直接使用load方法将文件直接反序列化为对象，如下所示：



* * *



>>> f=open(r'D:\dump.txt','rb') >>> d=pickle.load(f) >>> f.close() >>> d {'content': '\xca\xd7\xd2\xb3', 'url': 'index.html', 'title': '\xca\xd7\xd2\xb3'}



* * *



通过反序列化，存储为文件的dict对象，又重新恢复出来，但是这个变量和原变量没有什么关系，只是内容一样。以上就是序列化操作的整个过程。

假如我们想在不同的编程语言之间传递对象，把对象序列化为标准格式是关键，例如XML，但是现在更加流行的是序列化为JSON格式，既可以被所有的编程语言读取解析，也可以方便地存储到磁盘或者通过网络传输。对于JSON的操作，将在第5章进行讲解。





1.4　进程和线程


在爬虫开发中，进程和线程的概念是非常重要的。提高爬虫的工作效率，打造分布式爬虫，都离不开进程和线程的身影。本节将从多进程、多线程、协程和分布式进程等四个方面，帮助大家回顾Python语言中进程和线程中的常用操作，以便在接下来的爬虫开发中灵活运用进程和线程。





1.4.1　多进程


Python实现多进程的方式主要有两种，一种方法是使用os模块中的fork方法，另一种方法是使用multiprocessing模块。这两种方法的区别在于前者仅适用于Unix/Linux操作系统，对Windows不支持，后者则是跨平台的实现方式。由于现在很多爬虫程序都是运行在Unix/Linux操作系统上，所以本节对两种方式都进行讲解。

1.使用os模块中的fork方式实现多进程

Python的os模块封装了常见的系统调用，其中就有fork方法。fork方法来自于Unix/Linux操作系统中提供的一个fork系统调用，这个方法非常特殊。普通的方法都是调用一次，返回一次，而fork方法是调用一次，返回两次，原因在于操作系统将当前进程（父进程）复制出一份进程（子进程），这两个进程几乎完全相同，于是fork方法分别在父进程和子进程中返回。子进程中永远返回0，父进程中返回的是子进程的ID。下面举个例子，对Python使用fork方法创建进程进行讲解。其中os模块中的getpid方法用于获取当前进程的ID，getppid方法用于获取父进程的ID。代码如下：



* * *



import os if __name__ == '__main__': print 'current Process (%s) start ...'%(os.getpid()) pid = os.fork() if pid < 0: print 'error in fork' elif pid == 0: print 'I am child process(%s) and my parent process is (%s)',(os.getpid(), os.getppid()) else: print 'I(%s) created a chlid process (%s).',(os.getpid(),pid)



* * *



运行结果如下：



* * *



current Process (3052) start ... I(3052) created a chlid process (3053). I am child process(3053) and my parent process is (3052)



* * *



2.使用multiprocessing模块创建多进程

multiprocessing模块提供了一个Process类来描述一个进程对象。创建子进程时，只需要传入一个执行函数和函数的参数，即可完成一个Process实例的创建，用start（）方法启动进程，用join（）方法实现进程间的同步。下面通过一个例子来演示创建多进程的流程，代码如下：



* * *



import os from multiprocessing import Process # 子进程要执行的代码 def run_proc(name): print 'Child process %s (%s) Running...' % (name, os.getpid()) if __name__ == '__main__': print 'Parent process %s.' % os.getpid() for i in range(5): p = Process(target=run_proc, args=(str(i),)) print 'Process will start.' p.start() p.join() print 'Process end.'



* * *



运行结果如下：



* * *



Parent process 2392. Process will start. Process will start. Process will start. Process will start. Process will start. Child process 2 (10748) Running... Child process 0 (5324) Running... Child process 1 (3196) Running... Child process 3 (4680) Running... Child process 4 (10696) Running... Process end.



* * *



以上介绍了创建进程的两种方法，但是要启动大量的子进程，使用进程池批量创建子进程的方式更加常见，因为当被操作对象数目不大时，可以直接利用multiprocessing中的Process动态生成多个进程，如果是上百个、上千个目标，手动去限制进程数量却又太过繁琐，这时候进程池Pool发挥作用的时候就到了。

3.multiprocessing模块提供了一个Pool类来代表进程池对象

Pool可以提供指定数量的进程供用户调用，默认大小是CPU的核数。当有新的请求提交到Pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来处理它。下面通过一个例子来演示进程池的工作流程，代码如下：



* * *



from multiprocessing import Pool import os, time, random def run_task(name): print 'Task %s (pid = %s) is running...' % (name, os.getpid()) time.sleep(random.random() * 3) print 'Task %s end.' % name if __name__=='__main__': print 'Current process %s.' % os.getpid() p = Pool(processes=3) for i in range(5): p.apply_async(run_task, args=(i,)) print 'Waiting for all subprocesses done...' p.close() p.join() print 'All subprocesses done.'



* * *



运行结果如下：



* * *



Current process 9176. Waiting for all subprocesses done... Task 0 (pid = 11012) is running... Task 1 (pid = 12464) is running... Task 2 (pid = 11260) is running... Task 2 end. Task 3 (pid = 11260) is running... Task 0 end. Task 4 (pid = 11012) is running... Task 1 end. Task 3 end. Task 4 end. All subprocesses done.



* * *



上述程序先创建了容量为3的进程池，依次向进程池中添加了5个任务。从运行结果中可以看到虽然添加了5个任务，但是一开始只运行了3个，而且每次最多运行3个进程。当一个任务结束了，新的任务依次添加进来，任务执行使用的进程依然是原来的进程，这一点通过进程的pid就可以看出来。

注意 　Pool对象调用join（）方法会等待所有子进程执行完毕，调用join（）之前必须先调用close（），调用close（）之后就不能继续添加新的Process了。

4.进程间通信

假如创建了大量的进程，那进程间通信是必不可少的。Python提供了多种进程间通信的方式，例如Queue、Pipe、Value+Array等。本节主要讲解Queue和Pipe这两种方式。Queue和Pipe的区别在于Pipe常用来在两个进程间通信，Queue用来在多个进程间实现通信。

首先讲解一下Queue通信方式。Queue是多进程安全的队列，可以使用Queue实现多进程之间的数据传递。有两个方法：Put和Get可以进行Queue操作：

·Put方法用以插入数据到队列中，它还有两个可选参数：blocked和timeout。如果blocked为True（默认值），并且timeout为正值，该方法会阻塞timeout指定的时间，直到该队列有剩余的空间。如果超时，会抛出Queue.Full异常。如果blocked为False，但该Queue已满，会立即抛出Queue.Full异常。





·Get方法可以从队列读取并且删除一个元素。同样，Get方法有两个可选参数：blocked和timeout。如果blocked为True（默认值），并且timeout为正值，那么在等待时间内没有取到任何元素，会抛出Queue.Empty异常。如果blocked为False，分两种情况：如果Queue有一个值可用，则立即返回该值；否则，如果队列为空，则立即抛出Queue.Empty异常。


下面通过一个例子进行说明：在父进程中创建三个子进程，两个子进程往Queue中写入数据，一个子进程从Queue中读取数据。程序示例如下：



* * *



from multiprocessing import Process, Queue import os, time, random # 写数据进程执行的代码: def proc_write(q,urls): print('Process(%s) is writing...' % os.getpid()) for url in urls: q.put(url) print('Put %s to queue...' % url) time.sleep(random.random()) # 读数据进程执行的代码: def proc_read(q): print('Process(%s) is reading...' % os.getpid()) while True: url = q.get(True) print('Get %s from queue.' % url) if __name__=='__main__': # 父进程创建Queue，并传给各个子进程： q = Queue() proc_writer1 = Process(target=proc_write, args=(q,['url_1', 'url_2', 'url_3'])) proc_writer2 = Process(target=proc_write, args=(q,['url_4','url_5','url_6'])) proc_reader = Process(target=proc_read, args=(q,)) # 启动子进程proc_writer，写入: proc_writer1.start() proc_writer2.start() # 启动子进程proc_reader，读取: proc_reader.start() # 等待proc_writer结束: proc_writer1.join() proc_writer2.join() # proc_reader进程里是死循环，无法等待其结束，只能强行终止: proc_reader.terminate()



* * *



运行结果如下：



* * *



Process(9968) is writing... Process(9512) is writing... Put url_1 to queue... Put url_4 to queue... Process(1124) is reading... Get url_1 from queue. Get url_4 from queue. Put url_5 to queue... Get url_5 from queue. Put url_2 to queue... Get url_2 from queue. Put url_6 to queue... Get url_6 from queue. Put url_3 to queue... Get url_3 from queue.



* * *



最后介绍一下Pipe的通信机制，Pipe常用来在两个进程间进行通信，两个进程分别位于管道的两端。

Pipe方法返回（conn1，conn2）代表一个管道的两个端。Pipe方法有duplex参数，如果duplex参数为True（默认值），那么这个管道是全双工模式，也就是说conn1和conn2均可收发。若duplex为False，conn1只负责接收消息，conn2只负责发送消息。send和recv方法分别是发送和接收消息的方法。例如，在全双工模式下，可以调用conn1.send发送消息，conn1.recv接收消息。如果没有消息可接收，recv方法会一直阻塞。如果管道已经被关闭，那么recv方法会抛出EOFError。

下面通过一个例子进行说明：创建两个进程，一个子进程通过Pipe发送数据，一个子进程通过Pipe接收数据。程序示例如下：



* * *



import multiprocessing import random import time,os def proc_send(pipe,urls): for url in urls: print "Process(%s) send: %s" %(os.getpid(),url) pipe.send(url) time.sleep(random.random()) def proc_recv(pipe): while True: print "Process(%s) rev:%s" %(os.getpid(),pipe.recv()) time.sleep(random.random()) if __name__ == "__main__": pipe = multiprocessing.Pipe() p1 = multiprocessing.Process(target=proc_send, args=(pipe[0],['url_'+str(i) for i in range(10) ])) p2 = multiprocessing.Process(target=proc_recv, args=(pipe[1],)) p1.start() p2.start() p1.join() p2.join()



* * *



运行结果如下：



* * *



Process(10448) send: url_0 Process(5832) rev:url_0 Process(10448) send: url_1 Process(5832) rev:url_1 Process(10448) send: url_2 Process(5832) rev:url_2 Process(10448) send: url_3 Process(10448) send: url_4 Process(5832) rev:url_3 Process(10448) send: url_5 Process(10448) send: url_6 Process(5832) rev:url_4 Process(5832) rev:url_5 Process(10448) send: url_7 Process(10448) send: url_8 Process(5832) rev:url_6 Process(5832) rev:url_7 Process(10448) send: url_9 Process(5832) rev:url_8 Process(5832) rev:url_9



* * *



注意 　以上多进程程序运行结果的打印顺序在不同的系统和硬件条件下略有不同。





1.4.2　多线程


多线程类似于同时执行多个不同程序，多线程运行有如下优点：

·可以把运行时间长的任务放到后台去处理。

·用户界面可以更加吸引人，比如用户点击了一个按钮去触发某些事件的处理，可以弹出一个进度条来显示处理的进度。

·程序的运行速度可能加快。

·在一些需要等待的任务实现上，如用户输入、文件读写和网络收发数据等，线程就比较有用了。在这种情况下我们可以释放一些珍贵的资源，如内存占用等。

Python的标准库提供了两个模块：thread和threading，thread是低级模块，threading是高级模块，对thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。

1.用threading模块创建多线程

threading模块一般通过两种方式创建多线程：第一种方式是把一个函数传入并创建Thread实例，然后调用start方法开始执行；第二种方式是直接从threading.Thread继承并创建线程类，然后重写__init__方法和run方法。

首先介绍第一种方法，通过一个简单例子演示创建多线程的流程，程序如下：



* * *



import random import time, threading # 新线程执行的代码: def thread_run(urls): print 'Current %s is running...' % threading.current_thread().name for url in urls: print '%s ---->>> %s' % (threading.current_thread().name,url) time.sleep(random.random()) print '%s ended.' % threading.current_thread().name print '%s is running...' % threading.current_thread().name t1 = threading.Thread(target=thread_run, name='Thread_1',args=(['url_1','url_2',' url_3'],)) t2 = threading.Thread(target=thread_run, name='Thread_2',args=(['url_4','url_5',' url_6'],)) t1.start() t2.start() t1.join() t2.join() print '%s ended.' % threading.current_thread().name



* * *



运行结果如下：



* * *



MainThread is running... Current Thread_1 is running... Thread_1 ---->>> url_1 Current Thread_2 is running... Thread_2 ---->>> url_4 Thread_1 ---->>> url_2 Thread_2 ---->>> url_5 Thread_2 ---->>> url_6 Thread_1 ---->>> url_3 Thread_1 ended. Thread_2 ended. MainThread ended.



* * *





第二种方式从threading.Thread继承创建线程类，下面将方法一的程序进行重写，程序如下：




* * *



import random import threading import time class myThread(threading.Thread): def __init__(self,name,urls): threading.Thread.__init__(self,name=name) self.urls = urls def run(self): print 'Current %s is running...' % threading.current_thread().name for url in self.urls: print '%s ---->>> %s' % (threading.current_thread().name,url) time.sleep(random.random()) print '%s ended.' % threading.current_thread().name print '%s is running...' % threading.current_thread().name t1 = myThread(name='Thread_1',urls=['url_1','url_2','url_3']) t2 = myThread(name='Thread_2',urls=['url_4','url_5','url_6']) t1.start() t2.start() t1.join() t2.join() print '%s ended.' % threading.current_thread().name



* * *



运行结果如下：



* * *



MainThread is running... Current Thread_1 is running... Thread_1 ---->>> url_1 Current Thread_2 is running... Thread_2 ---->>> url_4 Thread_2 ---->>> url_5 Thread_1 ---->>> url_2 Thread_1 ---->>> url_3 Thread_2 ---->>> url_6 Thread_2 ended. Thread_1 ended.



* * *



2.线程同步

如果多个线程共同对某个数据修改，则可能出现不可预料的结果，为了保证数据的正确性，需要对多个线程进行同步。使用Thread对象的Lock和RLock可以实现简单的线程同步，这两个对象都有acquire方法和release方法，对于那些每次只允许一个线程操作的数据，可以将其操作放到acquire和release方法之间。

对于Lock对象而言，如果一个线程连续两次进行acquire操作，那么由于第一次acquire之后没有release，第二次acquire将挂起线程。这会导致Lock对象永远不会release，使得线程死锁。RLock对象允许一个线程多次对其进行acquire操作，因为在其内部通过一个counter变量维护着线程acquire的次数。而且每一次的acquire操作必须有一个release操作与之对应，在所有的release操作完成之后，别的线程才能申请该RLock对象。下面通过一个简单的例子演示线程同步的过程：



* * *



import threading mylock = threading.RLock() num=0 class myThread(threading.Thread): def __init__(self, name): threading.Thread.__init__(self,name=name) def run(self): global num while True: mylock.acquire() print '%s locked, Number: %d'%(threading.current_thread().name, num) if num>=4: mylock.release() print '%s released, Number: %d'%(threading.current_thread().name, num) break num+=1 print '%s released, Number: %d'%(threading.current_thread().name, num) mylock.release() if __name__== '__main__': thread1 = myThread('Thread_1') thread2 = myThread('Thread_2') thread1.start() thread2.start()



* * *



运行结果如下：



* * *



Thread_1 locked, Number: 0 Thread_1 released, Number: 1 Thread_1 locked, Number: 1 Thread_1 released, Number: 2 Thread_2 locked, Number: 2 Thread_2 released, Number: 3 Thread_1 locked, Number: 3 Thread_1 released, Number: 4 Thread_2 locked, Number: 4 Thread_2 released, Number: 4 Thread_1 locked, Number: 4 Thread_1 released, Number: 4



* * *



3.全局解释器锁（GIL）

在Python的原始解释器CPython中存在着GIL（Global Interpreter Lock，全局解释器锁），因此在解释执行Python代码时，会产生互斥锁来限制线程对共享资源的访问，直到解释器遇到I/O操作或者操作次数达到一定数目时才会释放GIL。由于全局解释器锁的存在，在进行多线程操作的时候，不能调用多个CPU内核，只能利用一个内核，所以在进行CPU密集型操作的时候，不推荐使用多线程，更加倾向于多进程。那么多线程适合什么样的应用场景呢？对于IO密集型操作，多线程可以明显提高效率，例如Python爬虫的开发，绝大多数时间爬虫是在等待socket返回数据，网络IO的操作延时比CPU大得多。





1.4.3　协程


协程 （coroutine），又称微线程，纤程，是一种用户级的轻量级线程。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此协程能保留上一次调用时的状态，每次过程重入时，就相当于进入上一次调用的状态。在并发编程中，协程与线程类似，每个协程表示一个执行单元，有自己的本地数据，与其他协程共享全局数据和其他资源。

协程需要用户自己来编写调度逻辑，对于CPU来说，协程其实是单线程，所以CPU不用去考虑怎么调度、切换上下文，这就省去了CPU的切换开销，所以协程在一定程度上又好于多线程。那么在Python中是如何实现协程的呢？

Python通过yield提供了对协程的基本支持，但是不完全，而使用第三方gevent库是更好的选择，gevent提供了比较完善的协程支持。gevent是一个基于协程的Python网络函数库，使用greenlet在libev事件循环顶部提供了一个有高级别并发性的API。主要特性有以下几点：

·基于libev的快速事件循环，Linux上是epoll机制。

·基于greenlet的轻量级执行单元。

·API复用了Python标准库里的内容。

·支持SSL的协作式sockets。

·可通过线程池或c-ares实现DNS查询。

·通过monkey patching功能使得第三方模块变成协作式。

gevent对协程的支持，本质上是greenlet在实现切换工作。greenlet工作流程如下：假如进行访问网络的IO操作时，出现阻塞，greenlet就显式切换到另一段没有被阻塞的代码段执行，直到原先的阻塞状况消失以后，再自动切换回原来的代码段继续处理。因此，greenlet是一种合理安排的串行方式。

由于IO操作非常耗时，经常使程序处于等待状态，有了gevent为我们自动切换协程，就保证总有greenlet在运行，而不是等待IO，这就是协程一般比多线程效率高的原因。由于切换是在IO操作时自动完成，所以gevent需要修改Python自带的一些标准库，将一些常见的阻塞，如socket、select等地方实现协程跳转，这一过程在启动时通过monkey patch完成。下面通过一个的例子来演示gevent的使用流程，代码如下：



* * *





from gevent import monkey; monkey.patch_all() import gevent import urllib2 def run_task(url): print 'Visit --> %s' % url try: response = urllib2.urlopen(url) data = response.read() print '%d bytes received from %s.' % (len(data), url) except Exception,e: print e if __name__=='__main__': urls = ['https:// github.com/','https:// www.python.org/','http://www.cnblogs.com/'] greenlets = [gevent.spawn(run_task, url) for url in urls ] gevent.joinall(greenlets)




* * *



运行结果如下：



* * *



Visit --> https:// github.com/ Visit --> https:// www.python.org/ Visit --> http://www.cnblogs.com/ 45740 bytes received from http://www.cnblogs.com/. 25482 bytes received from https:// github.com/. 47445 bytes received from https:// www.python.org/.



* * *



以上程序主要用了gevent中的spawn方法和joinall方法。spawn方法可以看做是用来形成协程，joinall方法就是添加这些协程任务，并且启动运行。从运行结果来看，3个网络操作是并发执行的，而且结束顺序不同，但其实只有一个线程。

gevent中还提供了对池的支持。当拥有动态数量的greenlet需要进行并发管理（限制并发数）时，就可以使用池，这在处理大量的网络和IO操作时是非常需要的。接下来使用gevent中pool对象，对上面的例子进行改写，程序如下：



* * *



from gevent import monkey monkey.patch_all() import urllib2 from gevent.pool import Pool def run_task(url): print 'Visit --> %s' % url try: response = urllib2.urlopen(url) data = response.read() print '%d bytes received from %s.' % (len(data), url) except Exception,e: print e return 'url:%s --->finish'% url if __name__=='__main__': pool = Pool(2) urls = ['https:// github.com/','https:// www.python.org/','http://www.cnblogs.com/'] results = pool.map(run_task,urls) print results



* * *



运行结果如下：



* * *



Visit --> https:// github.com/ Visit --> https:// www.python.org/ 25482 bytes received from https:// github.com/. Visit --> http://www.cnblogs.com/ 47445 bytes received from https:// www.python.org/. 45687 bytes received from http://www.cnblogs.com/. ['url:https:// github.com/ --->finish', 'url:https:// www.python.org/ --->finish', 'url:http://www.cnblogs.com/ --->finish']



* * *



通过运行结果可以看出，Pool对象确实对协程的并发数量进行了管理，先访问了前两个网址，当其中一个任务完成时，才会执行第三个。





1.4.4　分布式进程


分布式进程指的是将Process进程分布到多台机器上，充分利用多台机器的性能完成复杂的任务。我们可以将这一点应用到分布式爬虫的开发中。

分布式进程在Python中依然要用到multiprocessing模块。multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。可以写一个服务进程作为调度者，将任务分布到其他多个进程中，依靠网络通信进行管理。举个例子：在做爬虫程序时，常常会遇到这样的场景，我们想抓取某个网站的所有图片，如果使用多进程的话，一般是一个进程负责抓取图片的链接地址，将链接地址存放到Queue中，另外的进程负责从Queue中读取链接地址进行下载和存储到本地。现在把这个过程做成分布式，一台机器上的进程负责抓取链接，其他机器上的进程负责下载存储。那么遇到的主要问题是将Queue暴露到网络中，让其他机器进程都可以访问，分布式进程就是将这一个过程进行了封装，我们可以将这个过程称为本地队列的网络化。整体过程如图1-24所示。



图1-24　分布式进程

要实现上面例子的功能，创建分布式进程需要分为六个步骤：

1）建立队列Queue，用来进行进程间的通信。服务进程创建任务队列task_queue，用来作为传递任务给任务进程的通道；服务进程创建结果队列result_queue，作为任务进程完成任务后回复服务进程的通道。在分布式多进程环境下，必须通过由Queuemanager获得的Queue接口来添加任务。

2）把第一步中建立的队列在网络上注册，暴露给其他进程（主机），注册后获得网络队列，相当于本地队列的映像。

3）建立一个对象（Queuemanager（BaseManager））实例manager，绑定端口和验证口令。

4）启动第三步中建立的实例，即启动管理manager，监管信息通道。

5）通过管理实例的方法获得通过网络访问的Queue对象，即再把网络队列实体化成可以使用的本地队列。

6）创建任务到“本地”队列中，自动上传任务到网络队列中，分配给任务进程进行处理。

接下来通过程序实现上面的例子（Linux版），首先编写的是服务进程（taskManager.py），代码如下：



* * *



import random,time,Queue from multiprocessing.managers import BaseManager # 第一步：建立task_queue和result_queue，用来存放任务和结果 task_queue=Queue.Queue() result_queue=Queue.Queue() class Queuemanager(BaseManager): pass # 第二步：把创建的两个队列注册在网络上，利用register方法，callable参数关联了Queue对象， # 将Queue对象在网络中暴露 Queuemanager.register('get_task_queue',callable=lambda:task_queue) Queuemanager.register('get_result_queue',callable=lambda:result_queue) # 第三步：绑定端口8001，设置验证口令‘qiye’。这个相当于对象的初始化 manager=Queuemanager(address=('',8001),authkey='qiye') # 第四步：启动管理，监听信息通道 manager.start() # 第五步：通过管理实例的方法获得通过网络访问的Queue对象 task=manager.get_task_queue() result=manager.get_result_queue() # 第六步：添加任务 for url in ["ImageUrl_"+i for i in range(10)]: print 'put task %s ...' %url task.put(url) # 获取返回结果 print 'try get result...' for i in range(10): print 'result is %s' %result.get(timeout=10) # 关闭管理 manager.shutdown()



* * *



任务进程已经编写完成，接下来编写任务进程（taskWorker.py），创建任务进程的步骤相对较少，需要四个步骤：

1）使用QueueManager注册用于获取Queue的方法名称，任务进程只能通过名称来在网络上获取Queue。

2）连接服务器，端口和验证口令注意保持与服务进程中完全一致。

3）从网络上获取Queue，进行本地化。

4）从task队列获取任务，并把结果写入result队列。

程序taskWorker.py代码（win/linux版）如下：



* * *





# coding:utf-8 import time from multiprocessing.managers import BaseManager # 创建类似的QueueManager: class QueueManager(BaseManager): pass # 第一步：使用QueueManager注册用于获取Queue的方法名称 QueueManager.register('get_task_queue') QueueManager.register('get_result_queue') # 第二步：连接到服务器: server_addr = '127.0.0.1' print('Connect to server %s...' % server_addr) # 端口和验证口令注意保持与服务进程完全一致: m = QueueManager(address=(server_addr, 8001), authkey='qiye') # 从网络连接: m.connect() # 第三步：获取Queue的对象: task = m.get_task_queue() result = m.get_result_queue() # 第四步：从task队列获取任务,并把结果写入result队列: while(not task.empty()): image_url = task.get(True,timeout=5) print('run task download %s...' % image_url) time.sleep(1) result.put('%s--->success'%image_url) # 处理结束: print('worker exit.')




* * *



最后开始运行程序，先启动服务进程taskManager.py，运行结果如下：



* * *



put task ImageUrl_0 ... put task ImageUrl_1 ... put task ImageUrl_2 ... put task ImageUrl_3 ... put task ImageUrl_4 ... put task ImageUrl_5 ... put task ImageUrl_6 ... put task ImageUrl_7 ... put task ImageUrl_8 ... put task ImageUrl_9 ... try get result...



* * *



接着再启动任务进程taskWorker.py，运行结果如下：



* * *



Connect to server 127.0.0.1... run task download ImageUrl_0... run task download ImageUrl_1... run task download ImageUrl_2... run task download ImageUrl_3... run task download ImageUrl_4... run task download ImageUrl_5... run task download ImageUrl_6... run task download ImageUrl_7... run task download ImageUrl_8... run task download ImageUrl_9... worker exit.



* * *



当任务进程运行结束后，服务进程运行结果如下：



* * *



result is ImageUrl_0--->success result is ImageUrl_1--->success result is ImageUrl_2--->success result is ImageUrl_3--->success result is ImageUrl_4--->success result is ImageUrl_5--->success result is ImageUrl_6--->success result is ImageUrl_7--->success result is ImageUrl_8--->success result is ImageUrl_9--->success



* * *



其实这就是一个简单但真正的分布式计算，把代码稍加改造，启动多个worker，就可以把任务分布到几台甚至几十台机器上，实现大规模的分布式爬虫。

注意 　由于平台的特性，创建服务进程的代码在Linux和Windows上有一些不同，创建工作进程的代码是一致的。

taskManager.py程序在Windows版下的代码如下：



* * *



# coding:utf-8 # taskManager.py for windows import Queue from multiprocessing.managers import BaseManager from multiprocessing import freeze_support # 任务个数 task_number = 10 # 定义收发队列 task_queue = Queue.Queue(task_number); result_queue = Queue.Queue(task_number); def get_task(): return task_queue def get_result(): return result_queue # 创建类似的QueueManager: class QueueManager(BaseManager): pass def win_run(): # Windows下绑定调用接口不能使用lambda，所以只能先定义函数再绑定 QueueManager.register('get_task_queue',callable = get_task) QueueManager.register('get_result_queue',callable = get_result) # 绑定端口并设置验证口令，Windows下需要填写IP地址，Linux下不填默认为本地 manager = QueueManager(address = ('127.0.0.1',8001),authkey = 'qiye') # 启动 manager.start() try: # 通过网络获取任务队列和结果队列 task = manager.get_task_queue() result = manager.get_result_queue() # 添加任务 for url in ["ImageUrl_"+str(i) for i in range(10)]: print 'put task %s ...' %url task.put(url) print 'try get result...' for i in range(10): print 'result is %s' %result.get(timeout=10) except: print('Manager error') finally: # 一定要关闭，否则会报管道未关闭的错误 manager.shutdown() if __name__ == '__main__': # Windows下多进程可能会有问题，添加这句可以缓解 freeze_support() win_run()



* * *





1.5　网络编程


既然是做爬虫开发，必然需要了解Python网络编程方面的知识。计算机网络是把各个计算机连接到一起，让网络中的计算机可以互相通信。网络编程就是如何在程序中实现两台计算机的通信。例如当你使用浏览器访问谷歌网站时，你的计算机就和谷歌的某台服务器通过互联网建立起了连接，然后谷歌服务器会把把网页内容作为数据通过互联网传输到你的电脑上。

网络编程对所有开发语言都是一样的，Python也不例外。使用Python进行网络编程时，实际上是在Python程序本身这个进程内，连接到指定服务器进程的通信端口进行通信，所以网络通信也可以看做两个进程间的通信。

提到网络编程，必须提到的一个概念是Socket。Socket（套接字）是网络编程的一个抽象概念，通常我们用一个Socket表示“打开了一个网络链接”，而打开一个Socket需要知道目标计算机的IP地址和端口号，再指定协议类型即可。Python提供了两个基本的Socket模块：

·Socket，提供了标准的BSD Sockets API。

·SocketServer，提供了服务器中心类，可以简化网络服务器的开发。

下面讲一下Socket模块功能。

1.Socket类型

套接字格式为：socket（family，type[，protocal]），使用给定的地址族、套接字类型（如表1-2所示）、协议编号（默认为0）来创建套接字。

表1-2　Socket类型及说明



2.Socket函数

表1-3列举了Python网络编程常用的函数，其中包括了TCP和UDP。

表1-3　Socket函数及说明



本节接下来主要介绍Python中TCP和UDP两种网络类型的编程流程。





1.5.1　TCP编程


网络编程一般包括两部分：服务端和客户端。TCP是一种面向连接的通信方式，主动发起连接的叫客户端，被动响应连接的叫服务端。首先说一下服务端，创建和运行TCP服务端一般需要五个步骤：

1）创建Socket，绑定Socket到本地IP与端口。

2）开始监听连接。

3）进入循环，不断接收客户端的连接请求。

4）接收传来的数据，并发送给对方数据。

5）传输完毕后，关闭Socket。

下面通过一个例子演示创建TCP服务端的过程，程序如下：



* * *



# coding:utf-8 import socket import threading import time def dealClient(sock, addr): # 第四步：接收传来的数据，并发送给对方数据 print('Accept new connection from %s:%s...' % addr) sock.send(b'Hello,I am server!') while True: data = sock.recv(1024) time.sleep(1) if not data or data.decode('utf-8') == 'exit': break print '-->>%s!' % data.decode('utf-8') sock.send(('Loop_Msg: %s!' % data.decode('utf-8')).encode('utf-8')) # 第五步：关闭Socket sock.close() print('Connection from %s:%s closed.' % addr) if __name__=="__main__": # 第一步：创建一个基于IPv4和TCP协议的Socket # Socket绑定的IP(127.0.0.1为本机IP)与端口 s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.bind(('127.0.0.1', 9999)) # 第二步:监听连接 s.listen(5) print('Waiting for connection...') while True: # 第三步:接收一个新连接: sock, addr = s.accept() # 创建新线程来处理TCP连接: t = threading.Thread(target=dealClient, args=(sock, addr)) t.start()



* * *



接着编写客户端，与服务端进行交互，TCP客户端的创建和运行需要三个步骤：

1）创建Socket，连接远端地址。

2）连接后发送数据和接收数据。

3）传输完毕后，关闭Socket。

程序如下：



* * *



# coding:utf-8 import socket # 初始化Socket s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 连接目标的IP和端口 s.connect(('127.0.0.1', 9999)) # 接收消息 print('-->>'+s.recv(1024).decode('utf-8')) # 发送消息 s.send(b'Hello,I am a client') print('-->>'+s.recv(1024).decode('utf-8')) s.send(b'exit') # 关闭Socket s.close()



* * *



最后看一下运行结果，先启动服务端，再启动客户端。服务端打印的信息如下：



* * *



Waiting for connection... Accept new connection from 127.0.0.1:20164... -->>Hello,I am a client! Connection from 127.0.0.1:20164 closed.



* * *



客户端输出信息如下：



* * *



-->>Hello,I am server! -->>Loop_Msg: Hello,I am a client!



* * *



以上完成了TCP客户端与服务端的交互流程，用TCP协议进行Socket编程在Python中十分简单。对于客户端，要主动连接服务器的IP和指定端口；对于服务器，要首先监听指定端口，然后，对每一个新的连接，创建一个线程或进程来处理。通常，服务器程序会无限运行下去。





1.5.2　UDP编程


TCP通信需要一个建立可靠连接的过程，而且通信双方以流的形式发送数据。相对于TCP，UDP则是面向无连接的协议。使用UDP协议时，不需要建立连接，只需要知道对方的IP地址和端口号，就可以直接发数据包，但是不关心是否能到达目的端。虽然用UDP传输数据不可靠，但是由于它没有建立连接的过程，速度比TCP快得多，对于不要求可靠到达的数据，就可以使用UDP协议。

使用UDP协议，和TCP一样，也有服务端和客户端之分。UDP编程相对于TCP编程比较简单，服务端创建和运行只需要三个步骤：

1）创建Socket，绑定指定的IP和端口。

2）直接发送数据和接收数据。

3）关闭Socket。

示例程序如下：



* * *



# coding:utf-8 import socket # 创建Socket，绑定指定的IP和端口 # SOCK_DGRAM指定了这个Socket的类型是UDP，绑定端口和TCP示例一样。 s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) s.bind(('127.0.0.1', 9999)) print('Bind UDP on 9999...') while True: # 直接发送数据和接收数据 data, addr = s.recvfrom(1024) print('Received from %s:%s.' % addr) s.sendto(b'Hello, %s!' % data, addr)



* * *



客户端的创建和运行更加简单，创建Socket，直接可以与服务端进行数据交换，示例如下：



* * *





# coding:utf-8 import socket s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) for data in [b'Hello', b'World']: # 发送数据: s.sendto(data, ('127.0.0.1', 9999)) # 接收数据: print(s.recv(1024).decode('utf-8')) s.close()




* * *



以上就是UDP服务端和客户端数据交互的流程，UDP的使用与TCP类似，但是不需要建立连接。此外，服务器绑定UDP端口和TCP端口互不冲突，即UDP的9999端口与TCP的9999端口可以各自绑定。





1.6　小结


本章主要讲解了Python的编程基础，包括IO编程、进程和线程、网络编程等三个方面。这三个方面在Python爬虫开发中经常用到，熟悉这些知识点，对于之后的开发将起到事半功倍的效果。如果对于Python编程基础不是很熟练，希望能将本章讲的三个知识点着重复习，将书中的例子灵活运用并加以改进。





第2章　Web前端基础


爬虫主要是和网页打交道，了解Web前端的知识是非常重要的。Web前端的知识范围非常广泛，不可能面面俱到和深入讲解，本章主要是抽取Web前端中和爬虫相关的知识点进行讲解，帮助读者了解这些必备的知识，为之后的Python爬虫开发打下基础。





2.1　W3C标准


如果说你只知道Web前端的一个标准，估计肯定是W3C标准了。W3C，即万维网联盟，是Web技术领域最具权威和影响力的国际中立性技术标准机构。万维网联盟（W3C）标准不是某一个标准，而是一系列标准的集合。网页主要由三部分组成：结构 （Structure）、表现 （Presentation）和行为 （Behavior）。对应的标准也分三方面：结构化标准语言主要包括XHTML和XML，表现标准语言主要包括CSS，行为标准主要包括对象模型（如W3C DOM）、ECMAScript等。本节我们主要讲解HTML、CSS、JavaScript、Xpath和JSON等5个部分，基本上覆盖了爬虫开发中需要了解的Web前端基本知识。





2.1.1　HTML


什么是HTML标记语言？HTML不是编程语言，是一种表示网页信息的符号标记语言。标记语言是一套标记，HTML使用标记来描述网页。Web浏览器的作用是读取HTML文档，并以网页的形式显示出它们。浏览器不会显示HTML标记，而是使用标记来解释页面的内容。HTML语言的特点包括：

·可以设置文本的格式，比如标题、字号、文本颜色、段落，等等。

·可以创建列表。

·可以插入图像和媒体。

·可以建立表格。

·超链接，可以使用鼠标点击超链接来实现页面之间的跳转。

下面从HTML的基本结构、文档设置标记、图像标记、表格和超链接五个方面讲解。

1.HTML的基本结构

首先在浏览器上访问google网站（如图2-1所示），右键查看源代码，如图2-2所示。



图2-1　谷歌网站首页



图2-2　谷歌首页源代码

从谷歌首页的源代码中可以分析出HTML的基本结构：

·<html>内容</html>：HTML文档是由<html></html>包裹，这是HTML文档的文档标记，也称为HTML开始标记。这对标记分别位于网页的最前端和最后端，<html>在最前端表示网页的开始，</html>在最后端表示网页的结束。

·<head>内容</head>：HTML文件头标记，也称为HTML头信息开始标记。用来包含文件的基本信息，比如网页的标题、关键字，在<head></head>内可以放<title></title>、<meta></meta>、<style></style>等标记。注意：在<head></head>标记内的内容不会在浏览器中显示。

·<title>内容</title>：HTML文件标题标记。网页的“主题”，显示在浏览器的窗口的左上边。

·<body>内容</body>：<body>...</body>是网页的主体部分，在此标记之间可以包含如<p></p>、<h1></h1>、<br>、<hr>等等标记，正是由这些内容组成了我们所看见的网页。

·<meta>内容</meta>：页面的元信息（meta-information）。提供有关页面的元信息，比如针对搜索引擎和更新频度的描述和关键词。注意meta标记必须放在head元素里面。

2.文档设置标记

文档设置标记分为格式标记和文本标记。下面通过一个标准的HTML文档对格式标记进行讲解，文档如下所示：



* * *



<html> <head> <title>Python爬虫开发与项目实战</title> <meta charset="UTF-8"> </head> <body> 文档设置标记<br> <p>这是段落。</p> <p>这是段落。</p> <p>这是段落。</p> <hr> <center>居中标记1</center> <center>居中标记2</center> <hr> <pre> [00:00](music) [00:28]你我皆凡人，生在人世间； [00:35]终日奔波苦，一刻不得闲； [00:43]既然不是仙，难免有杂念； </pre> <hr> <p> [00:00](music) [00:28]你我皆凡人，生在人世间； [00:35]终日奔波苦，一刻不得闲； [00:43]既然不是仙，难免有杂念； </p> <hr> <br> <ul> <li>Coffee</li> <li>Milk</li> </ul> <ol type="A"> <li>Coffee</li> <li>Milk</li> </ol> <dl> <dt>计算机</dt> <dd>用来计算的仪器 ... ...</dd> <dt>显示器</dt> <dd>以视觉方式显示信息的装置 ... ...</dd> </dl> <div > <h3>这是标题</h3> <p>这是段落。</p> </div> </body> </html>



* * *



在浏览器中打开运行，效果如图2-3所示。



图2-3　运行效果图

格式标记包括：

·<br>：强制换行标记。让后面的文字、图片、表格等等，显示在下一行。

·<p>：换段落标记。换段落，由于多个空格和回车在HTML中会被等效为一个空格，所以HTML中要换段落就要用<p>，<p>段落中也可以包含<p>段落。例如：<p>This is a paragraph.</p>。

·<center>：居中对齐标记。让段落或者是文字相对于父标记居中显示。

·<pre>：预格式化标记。保留预先编排好的格式，常用来定义计算机源代码。和<p>进行一下对比，就可以理解。

·<li>：列表项目标记。每一个列表使用一个<li>标记，可用在有序列表（<ol>）和无序列表（<ul>）中。

·<ul>：无序列表标记。<ul>声明这个列表没有序号。

·<ol>：有序列表标记。可以显示特定的一些顺序。有序列表的type属性值“1”表示阿拉伯数字1.2.3等等；默认type属性值“A”表示大小字母A、B、C等等；上面的程序使用属性“a”，这表示小写字母a、b、c等等；“Ⅰ”表示大写罗马数字Ⅰ、Ⅱ、Ⅲ、Ⅳ等等；“ⅰ”表示小写罗马数字ⅰ、ⅱ、ⅲ、ⅳ等等。注意：列表可以进行嵌套。

·<dl><dt><dd>：定义型列表。对列表条目进行简短说明。

·<hr>：水平分割线标记。可以用作段落之间的分割线。

·<div>：分区显示标记，也称为层标记。常用来编排一大段的HTML段落，也可以用于将表格式化，和<p>很相似，可以多层嵌套使用。

接下来通过一个HTML文档对文本标记进行讲解，文档如下所示：



* * *





<html> <head> <title>Python爬虫开发与项目实战</title> <meta charset="UTF-8"> </head> <body> Hn标题标记---->> <br> <h1>Python爬虫</h1> <h2>Python爬虫</h2> <h3>Python爬虫</h3> <h4>Python爬虫</h4> <h5>Python爬虫</h5> <h6>Python爬虫</h6> font标记---->> <font size="1">Python爬虫</font> <font size="3">Python爬虫</font> <font size="7">Python爬虫</font> <font size="7" color="red" face="微软雅黑">Python爬虫</font> <font size="7" color="red" face="宋体">Python爬虫</font> <font size="7" color="red" face="新细明体">Python爬虫</font> <br> B标记加粗---->> <b>Python爬虫</b> <br> i标记斜体---->> <i>Python爬虫</i> <br> sub下标标记---->> 2<sub>2</sub> <br> sup上标标记---->> 2<sup>2</sup> <br> 引用标记---->> <cite>Python爬虫</cite> <br> em标记表示强调，显示为斜体---->> <em>Python爬虫</em> <br> strong标记表示强调，加粗显示---->> <strong>Python爬虫</strong> <br> small标记，可以显示小一号字体，可以嵌套使用---->> <small>Python爬虫</small> <small><small>Python爬虫</small></small> <small><small><small>Python爬虫</small></small></small> <br> big标记，显示大一号的字体---->> <big>Python爬虫</big> <big><big>Python爬虫</big></big> <br> u标记是显示下划线---->> <big><big><big><u>Python爬虫</u></big></big></big> <br> </body> </html>




* * *



在浏览器中打开运行，效果如图2-4所示。

其中文本标记包括：

·<hn>：标题标记。共有6个级别，n的范围为1～6，不同级别对应不同显示大小的标题，h1最大，h6最小。

·<font>：字体设置标记。用来设置字体的格式，一般有三个常用属性：size（字体大小），<font size=“14px”>；color（颜色），<font color=“red”>；face（字体），<font face=“微软雅黑”>。

·<b>：粗字体标记。

·<i>：斜字体标记。

·<sub>：文字下标字体标记。

·<sup>：文字上标字体标记。

·<tt>：打印机字体标记。

·<cite>：引用方式的字体，通常是斜体。

·<em>：表示强调，通常显示为斜体字。

·<strong>：表示强调，通常显示为粗体字。

·<small>：小型字体标记。

·<big>：大型字体标记。

·<u>：下划线字体标记。



图2-4　运行效果图

3.图像标记

<img>称为图像标记，用来在网页中显示图像。使用方法为：<img src=“路径/文件名.图片格式”width=“属性值”height=“属性值”border=“属性值”alt=“属性值”>。<img>标记主要包括以下属性：

·src属性用来指定我们要加载的图片的路径、图片的名称以及图片格式。

·width属性用来指定图片的宽度，单位为px、em、cm、mm。

·height属性用来指定图片的高度，单位为px、em、cm、mm。

·border属性用来指定图片的边框宽度，单位为px、em、cm、mm。

·alt属性有三个作用：1）当网页上的图片被加载完成后，鼠标移动到上面去，会显示这个图片指定的属性文字；2）如果图像没有下载或者加载失败，会用文字来代替图像显示；3）搜索引擎可以通过这个属性的文字来抓取图片。

我们可以在浏览器上访问博客园首页，对博客园首页的图片进行审查，就可以看到的img标记的使用方法，如图2-5所示。



图2-5　img标记

注意 　<img>为单标记，不需要使用</img>闭合。在加载图像文件的时候，文件的路径、文件名或者文件格式错误，将无法加载图片。

4.超链接的使用

爬虫开发中经常需要抽取链接，链接的引用使用的是<a>标记。

<a>标记的基本语法：<a href=“链接地址”target=“打开方式”name=“页面锚点名称”>链接文字或者图片</a>。<a>标记主要包括以下属性：

·href属性值是链接的地址，链接的地址可以是一个网页，也可以是一个视频、图片、音乐等。

·target属性用来定义超链接的打开方式。当属性值为_blank时，作用是在一个新的窗口中打开链接；当属性值为_self（默认值）时，作用是在当前窗口中打开链接；当属性值为_parent时，作用是在在父窗口中打开页面；当属性值为_top时，在顶层窗口中打开文件。

·name属性用来指定页面的锚点名称。

5.表格

表格的基本结构包括<table>、<caption>、<tr>、<td>和<th>等标记。

<table>标记的基本格式为<table属性1=“属性值1”属性2=“属性值2”......>表格内容</table>。table标记有以下常见属性：

·width属性：表示表格的宽度，它的值可以是像素（px）也可以是父级元素的百分比（%）。

·height属性：表示表格的高度，它的值可以是像素（px）也可以是父级元素的百分比（%）。

·border属性：表示表格外边框的宽度。

·align属性用来表示表格的显示位置。left居左显示，center居中显示，right居右显示。

·cellspacing属性：单元格之间的间距，默认是2px，单位为像素。

·cellpadding属性：单元格内容与单元格边框的显示距离，单位为像素。





·frame属性用来控制表格边框最外层的四条线框。void（默认值）表示无边框；above表示仅顶部有边框；below表示仅有底部边框；hsides表示仅有顶部边框和底部边框；lhs表示仅有左侧边框；rhs表示仅有右侧边框；vsides表示仅有左右侧边框；border表示包含全部4个边框。


·rules属性用来控制是否显示以及如何显示单元格之间的分割线。属性值none（默认值）表示无分割线；all表示包括所有分割线；rows表示仅有行分割线；clos表示仅有列分割线；groups表示仅在行组和列组之间有分割线。

<caption>标记用于在表格中使用标题。<caption>属性的插入位置，直接位于<table>属性之后，<tr>表格行之前。<caption>标记中align属性可以取四个值：top表示标题放在表格的上部；bottom表示标题放在表格的下部；left表示标题放在表格的左部；right表示标题放在表格的右部。

<tr>标记用来定义表格的行，对于每一个表格行，都是由一对<tr>...</tr>标记表示，每一行<tr>标记内可以嵌套多个<td>或者<th>标记。<tr>标记中的常见属性包括：

·bgcolor属性用来设置背景颜色，格式为bgcolor=“颜色值”。

·align属性用来设置垂直方向对齐方式，格式为align=“值”。值为bottom时，表示靠顶端对齐；值为top时，表示靠底部对齐；值为middle时，表示居中对齐。

·valign属性用来设置水平方向对齐方式，格式为valign=“值”。值为left时，表示靠左对齐；值为right时，表示靠右对齐；值为center时，表示居中对齐。

<td>和<th>都是单元格的标记，其必须嵌套在<tr>标记内，成对出现。<th>是表头标记，通常位于首行或者首列，<th>中的文字默认会被加粗，而<td>不会。<td>是数据标记，表示该单元格的具体数据。<td>和<th>两者的标记属性都是一样的，常用属性如下：

·bgcolor设置单元格背景。

·align设置单元格对齐方式。

·valign设置单元格垂直对齐方式。

·width设置单元格宽度。

·height设置单元格高度。

·rowspan设置单元格所占行数。

·colspan设置单元格所占列数。

下面通过一个HTML文档来演示表格的使用，文档如下：



* * *



<html> <head> <title>学生信息表</title> <meta charset="UTF-8"> </head> <body> <table width="960" align="center" border="1" rules="all" cellpadding="15"> <tr> <th>学号</th> <th>班级</th> <th>姓名</th> <th>年龄</th> <th>籍贯</th> </tr> <tr align="center"> <td>1500001</td> <td>(1)班</td> <td>张三</td> <td>16</td> <td>上海</td> </tr> <tr align="center"> <td>1500011</td> <td>(2)班</td> <td>李四</td> <td>15</td> <td bgcolor="# ccc">浙江</td> </tr> </table> <br/> <table width="960" align="center" border="1" rules="all" cellpadding= "15"> <tr bgcolor="# ccc"> <th>学号</th> <th>班级</th> <th>姓名</th> <th>年龄</th> <th>籍贯</th> </tr> <tr align="center"> <td>1500001</td> <td>(1)班</td> <td>张三</td> <td>16</td> <td bgcolor="red"><font color="white">上海</font></td> </tr> <tr align="center"> <td>1500011</td> <td>(2)班</td> <td>李四</td> <td>15</td> <td>浙江</td> </tr> </table> </body> </html>



* * *



在浏览器中打开运行，效果如图2-6所示。



图2-6　表格运行效果图





2.1.2　CSS


CSS指层叠样式表 （Cascading Style Sheets），用来定义如何显示HTML元素，一般和HTML配合使用。CSS样式表的目的是为了解决内容与表现分离的问题，即使同一个HTML文档也能表现出外观的多样化。在HTML中使用CSS样式的方式，一般有三种做法：

·内联样式表：CSS代码直接写在现有的HTML标记中，直接使用style属性改变样式。例如，<body style="background-color:green; margin:0; padding:0;"></body>。

·嵌入式样式表：CSS样式代码写在<style type="text/css"></style>标记之间，一般情况下嵌入式CSS样式写在<head></head>之间。

·外部样式表：CSS代码写一个单独的外部文件中，这个CSS样式文件以“.css”为扩展名，在<head>内（不是在<style>标记内）使用<link>标记将CSS样式文件链接到HTML文件内。例如，<link rel="StyleSheet" type="text/css" href="style.css">。

CSS规则由两个主要的部分构成：选择器，以及一条或多条声明。选择器通常是需要改变样式的HTML元素。每条声明由一个属性和一个值组成。属性 （property）是希望设置的样式属性（style attribute）。每个属性有一个值。属性和值由冒号分开。例如：h1{color：blue；font-size：12px}。其中h1为选择器，color和font-size是属性，blue和12px是属性值，这句话的意思是将h1标记中的颜色设置为蓝色，字体大小为12px。根据选择器的定义方式，可以将样式表的定义分成三种方式：

·HTML标记定义：上面举的例子就是使用的这种方式。假如想修改<p>...</p>的样式，可以定义CSS：p{属性：属性值；属性1：属性值1}。p可以叫做选择器，定义了标记中内容所执行的样式。一个选择器可以控制若干个样式属性，他们之间需要用英语的“；”隔开，最后一个可以不加“；”。





·ID选择器定义：ID选择器可以为标有特定ID的HTML元素指定特定的样式。HTML元素以ID属性来设置ID选择器，CSS中ID选择器以“#”来定义。假如定义为#word{text-align：center；color：red；}，就将HTML中ID为word的元素设置为居中，颜色为红色。


·class选择器定义：class选择器用于描述一组元素的样式，class选择器有别于ID选择器，它可以在多个元素中使用。class选择器在HTML中以class属性表示，在CSS中，class选择器以一个点“.”号显示。例如，.center{text-align：center；}将所有拥有center类的HTML元素设为居中。当然也可以指定特定的HTML元素使用class，例如，p.center{text-align：center；}是对所有的p元素使用class=“center”，让该元素的文本居中。

介绍完选择器，接着说一下CSS中一些常见的属性。常见属性主要说明一下颜色属性、字体属性、背景属性、文本属性和列表属性。

1.颜色属性

颜色属性color用来定义文本的颜色，可以使用以下方式定义颜色：

·颜色名称，如color：green。

·十六进制，如color：#ff6600。

·简写方式，如color：#f60。

·RGB方式，如rgb（255，255，255），红（R）、绿（G）、蓝（B）的取值范围均为0～255

·RGBA方式，如color：rgba（255，255，255，1），RGBA表示Red（红色）、Green（绿色）、Blue（蓝色）和Alpha的（色彩空间）透明度。

2.字体属性

可以使用字体属性定义文本形式，有如下方法：

·font-size定义字体大小，如font-size：14px。

·font-family定义字体，如font-family：微软雅黑，serif。字体之间可以使用“，”隔开，以确保当字体不存在的时候直接使用下一个字体。

·font-weight定义字体加粗，取值有两种方式。一种是使用名称，如normal（默认值）、bold（粗）、bolder（更粗）、lighter（更细）；一种是使用数字，如100、200、300～900，400=normal，而700=bold。

3.背景属性

可以用背景属性定义背景颜色、背景图片、背景重复和背景的位置，内容如下：

·background-color用来定义背景的颜色，用法参考颜色属性。

·background-image用来定义背景图片，如background-image：url（图片路径），也可以设置为background-image：none，表示不使用图片。

·background-repeat用来定义背景重复方式。取值为repeat，表示整体重复平铺；取值为repeat-x，表示只在水平方向平铺；取值为repeat-y，表示只在垂直方向平铺；取值为no-repeat，表示不重复。

·background-position用来定义背景位置。在横向上，可以取left、center、right；在纵向上可以取top、center、bottom。

·简写方式可以简化背景属性的书写，同时定义多个属性，格式为background：背景颜色url（图像）重复位置。如background：#f60url（images/bg、jpg）no-repeat top center。

4.文本属性

可以用文本属性设置行高、缩进和字符间距，具体如下：

·text-align设置文本对齐方式，属性值可以取left、center、right。

·line-height设置文本行高，属性值可以取具体数值，来设置固定的行高值。也可以取百分比，是基于字体大小的百分比行高。

·text-indent代表首行缩进，如text-indent：50px，意思是首行缩进50个像素。

·letter-spacing用来设置字符间距。属性值默认是normal，规定字符间没有额外的空间；可以设置具体的数值（可以是负值），如letter-spacing：3px；可以取inherit，从父元素继承letter-spacing属性的值。

5.列表

在HTML中，有两种类型的列表：无序和有序。其实使用CSS，可以列出进一步的样式，并可用图像作列表项标记。接下来主要讲解以下几种属性：

·list-style-type用来指明列表项标记的类型。常用的属性值有：none（无标记）、disc（默认，标记是实心圆）、circle（标记是空心圆）、square（标记是实心方块）、decimal（标记是数字）、decimal-leading-zero（0开头的数字标记）、lower-roman（小写罗马数字i、ii、iii、iv、v等）、upper-roman（大写罗马数字I、II、III、IV、V等）、lower-alpha（小写英文字母a、b、c、d、e等）、upper-alpha（大写英文字母A、B、C、D、E等）。例如，ul.a{list-style-type：circle；}是将class选择器的值为a的ul标记设置为空心圆标记。

·list-style-position用来指明列表项中标记的位置。属性值可以取inside、outside和inherit。inside指的是列表项标记放置在文本以内，且环绕文本根据标记对齐。outside为默认值，保持标记位于文本的左侧，列表项标记放置在文本以外，且环绕文本不根据标记对齐。inherit规定应该从父元素继承list-style-position属性的值。

·list-style-image用来设置设置图像列表标记。属性值可以为URL（图像的路径）、none（默认无图形被显示）、inherit（从父元素继承list-style-image属性的值）。例如，ul{list-style-image：url（‘image.gif’）；}，意思是给ul标记前面的标记设置为image.gif图片。

以上就将关于CSS的基本知识讲解完成了，接下来通过一个综合的例子将所有知识点进行融合，采用嵌入式样式表的方式，HTML文档如下：



* * *





<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <title>Python爬虫开发</title> <style> h1 { background-color:# 6495ed;/*--背景颜色--*/ color:red;/* 字体颜色 */ text-align:center;/* 文本居中 */ font-size:40px;/* 字体大小*/ } p { background-color:# e0ffff; text-indent:50px;/* 首行缩进 */ font-family:"Times New Roman", Times, serif;/* 设置字体 */ } p.ex {color:rgb(0,0,255);} div { background-color:# b0c4de; } ul.a {list-style-type:square;} ol.b {list-style-type:upper-roman;} ul.c{list-style-image:url('http://www.cnblogs.com/images/logo_small.gif');} </style> </head> <body> <h1>CSS background-color演示</h1> <div> 该文本插入在 div 元素中。 <p>该段落有自己的背景颜色。</p> <p class="ex">这是一个类为"ex"的段落。这个文本是蓝色的。</p> 我们仍然在同一个 div 中。 </div> <p>无序列表实例:</p> <ul class="a"> <li>Coffee</li> <li>Tea</li> <li>Coca Cola</li> </ul> <p>有序列表实例:</p> <ol class="b"> <li>Coffee</li> <li>Tea</li> <li>Coca Cola</li> </ol> <p>图片列表示例</p> <ul class="c"> <li>Coffee</li> <li>Tea</li> <li>Coca Cola</li> </ul> </body> </html>




* * *



在浏览器中打开文档，通过运行效果和之前的知识点进行对比，将更加容易理解。效果如图2-7所示。



图2-7　运行效果图





2.1.3　JavaScript


JavaScript是一种轻量级的脚本语言，和Python语言是一样的，只不过JavaScript是由浏览器进行解释执行。JavaScript可以插入HTML页面中，可由所有的现代浏览器执行。由于JavaScript是一门新的编程语言，知识点很多，本节不进行深入讲解，主要介绍一下JavaScript的用法和基本语法。大家如果想深入学习，需要额外看一些教程。

如何使用JavaScript呢？主要有直接插入代码和外部引用js文件两种做法：

1）直接插入代码。在<script></script>标记中编写代码。JavaScript代码可以直接嵌在网页的任何地方，不过通常我们都把JavaScript代码放到<head>中，示例如下：



* * *



<html> <head> <script type='text/javascript'> alert('Hello, world'); </script> </head> <body> python爬虫 </body> </html>



* * *



<script>标记中包含的就是JavaScript代码，可以直接被浏览器执行，弹出一个警告框。

2）外部引用js文件。把JavaScript代码放到一个单独的.js文件，然后在HTML中通过<script src=‘目标文档的URL’></script>的方式来引入js文件，其中目标文档的URL即是链接外部的js文件。示例如下：



* * *



<html> <head> <script src="/static/js/jquery.js"></script> </head> <body> python爬虫 </body> </html>



* * *



这样/static/js/jquery.js就会被浏览器执行。把JavaScript代码放入一个单独的.js文件中更利于维护代码，并且多个页面可以各自引用同一份.js文件，减少程序员编码量。在页面中多次编写JavaScript代码，浏览器按照顺序依次执行。

一般在正常的开发中都是采用上述两种做法结合的方式，之后在做Python爬虫开发时会经常见到。

为了能让零基础的读者在读完本节后能看懂简单的JavaScript代码，下面将从基本语法、数据类型和变量、运算符和操作符、条件判断、循环和函数等六个方面介绍JavaScript基础。

1.JavaScript基本语法

JavaScript严格区分大小写，JavaScript会忽略关键字、变量名、数字、函数名或其他各种元素之间的空格、制表符或换行符。我们可以使用缩进、换行来使代码整齐，提高可读性。一条完整的语句如下：



* * *



var x = 1;



* * *



这条语句定义了一个x的变量。从这条语句中可以看到以分号“；”作为结束。一行可以定义多条语句，但不推荐这么做。最后一个语句的分号可以省略，但尽量不要省略。示例语句如下：



* * *



var x = 1; var y = 2;



* * *



语句块是一组语句的集合，使用{...}形成一个块block。例如，下面的代码先做了一个判断，如果判断成立，将执行{...}中的所有语句：



* * *



var x = 2; var y = 1; if (x > y) { x = 3; y = 2; }



* * *



{...}还可以嵌套，形成层级结构。将以上的代码进行改造，程序如下：



* * *



var x = 2; var y = 1; if (x > y) { x = 3; y = 4; if(x<y){ x = 2; y = 1; } }



* * *



注释主要分为单行注释和多行注释。单行注释使用//作为注释符，多行注释使用/*/来注释内容。示例如下：



* * *



// var x = 2; var y = 1; /* var x = 2; var y = 1;*/



* * *



2.数据类型和变量

和Python一样，JavaScript也有自己的数据类型。在JavaScript中定义了以下几种数据类型：

·Number类型：JavaScript中不区分整数和浮点数，统一使用Number表示。示例如下：100（整数）、0.45（浮点数）、1.234e3（科学计数法表示）、-10（负数）、NaN（无法计算时候使用）、Infinity（无限大）、0xff（十六进制）。

·字符串类型：字符串是以单引号或双引号括起来的任意文本，比如‘abc’，“xyz”等。

·布尔值类型：一个布尔值只有true、false两种值。

·数组类型：数组是一组按顺序排列的集合，集合的每个值称为元素。JavaScript的数组可以包括任意数据类型，示例如下：var array=[1，2，3.14，‘Hello’，null，true]。上述数组包含6个元素。数组用[]表示，元素之间用“，”分隔。另一种创建数组的方法是通过Array（）函数实现，示例如下：var array=new Array（1，2，3）。数组的元素可以通过索引来访问，索引的起始值为0。





·对象类型：javaScript的对象是一组由键-值组成的无序集合，类似Python中的字典。示例如下：var person={name：‘qiye’，age：24，tags：[‘python’，‘web’，‘hacker’]，city：‘Beijing’，man：true}。JavaScript对象的键都是字符串类型，值可以是任意数据类型。要获取一个对象的属性，我们用“对象变量.属性名”的方式，如person.name。


JavaScript是弱类型的编程语言，声明变量的时候都是使用关键字var，没有int、char之说，为变量赋值时会自动判断类型并进行转换。变量名是大小写英文、数字、“$”和“_”的组合，且不能用数字开头。变量名也不能是JavaScript的关键字，如if、while等。申明一个变量用var语句，比如：var s_007=‘007’。

3.运算符和操作符

JavaScript中的运算符和操作符，与Python中的用法非常相似，表2-1总结了javaScript常用的运算符和操作符。

表2-1　运算符和操作符



4.条件判断

JavaScript使用if（）{...}else{...}来进行条件判断，和C语言的使用方法一样。例如，根据年龄显示不同内容，可以用if语句实现如下：



* * *



var role = 20; if (age >= 18) { alert('adult'); } else { alert('teenager'); }



* * *



5.循环

JavaScript的循环有两种：一种是for循环，一种是while循环。

首先说一下for循环。举个例子，计算1到100相加之和，程序如下：



* * *



var x = 0; var i; for (i=1; i<=100; i++) { x = x + i; }



* * *



for循环常用来遍历数组。另外for循环还有一个变体是for...in循环，它可以把一个对象的所有属性依次循环出来，示例如下：



* * *



var person = { name: 'qiye', age: 20, city: 'Beijing' }; for (var key in person ) { alert(key); // 'name', 'age', 'city' }



* * *



最后说一下while循环。使用方法和C语言一样，分为while（）{...}循环和do{...}while（），具体使用不再细说。

6.函数

在JavaScript中，定义函数使用function关键字，使用方式如下：



* * *



function add(x,y) { return x+y; }



* * *



上述add（）函数的定义如下：

·function指出这是一个函数定义；

·add是函数的名称。

·（x，y）括号内列出函数的参数，多个参数以“，”分隔。

·{...}之间的代码是函数体，可以包含若干语句，甚至可以没有任何语句。

调用函数时，按顺序传入参数即可：add（10，9）；//返回19。

由于JavaScript允许传入任意个参数而不影响调用，因此传入的参数比定义的参数多也没有问题，虽然函数内部并不需要这些参数：add（10，9，‘blablabla’）；//返回19。

传入的参数比定义的少也没有问题：add（）；//返回NaN。此时add（x，y）函数的参数x和y收到的值为undefined，计算结果为NaN。





2.1.4　XPath


XPath是一门在XML文档中查找信息的语言，被用于在XML文档中通过元素和属性进行导航。XPath虽然是被设计用来搜寻XML文档，不过它也能很好地在HTML文档中工作，并且大部分浏览器也支持通过XPath来查询节点。在Python爬虫开发中，经常使用XPath查找提取网页中的信息，因此XPath非常重要。

XPath既然叫Path，就是以路径表达式的形式来指定元素，这些路径表达式和我们在常规的电脑文件系统中看到的表达式非常相似。由于XPath一开始是被用来搜寻XML文档的，所以接下来就以XML文档为例子来讲解XPath。接下来从节点、语法、轴和运算符等四个方面讲解XPath的使用。

1.XPath节点

在XPath中，XML文档是被作为节点树来对待的，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。树的根被称为文档节点或者根节点。以下面的XML文档为例进行说明，文档如下：



* * *



<xml version="1.0" encoding="ISO-8859-1"> <classroom> <student> <id>1001</id> <name lang="en">marry</name> <age>20</age> <country>China</country> </student> </classroom>



* * *



上面的XML文档中的节点例子包括：<classroom>（文档节点）、<id>1001</id>（元素节点）、lang=“en”（属性节点）、marry（文本）。

接着说一下节点关系，包括父（Parent）、子（Children）、同胞（Sibling）、先辈（Ancestor）、后代（Descendant）。在上面的文档中：

·student元素是id、name、age以及country元素的父。

·id、name、age以及country元素都是student元素的子。

·id、name、age以及country元素都是同胞节点，拥有相同的父节点。

·name元素的先辈是student元素和classroom元素，也就是此节点的父、父的父等。

·classroom的后代是id、name、age以及country元素，也就是此节点的子，子的子等。

2.XPath语法

XPath使用路径表达式来选取XML文档中的节点或节点集。节点是沿着路径（path）或者步（steps）来选取的。接下来的重点是如何选取节点，下面给出一个XML文档进行分析：



* * *



<xml version="1.0" encoding="ISO-8859-1"> <classroom> <student> <id>1001</id> <name lang="en">marry</name> <age>20</age> <country>China</country> </student> <student> <id>1002</id> <name lang="en">jack</name> <age>25</age> <country>USA</country> </student> </classroom>



* * *



首先列举出一些常用的路径表达式进行节点的选取，如表2-2所示。

表2-2　路径表达式



通过表2-2中的路径表达式，我们尝试着对上面的文档进行节点选取。以表格的形式进行说明，如表2-3所示。

表2-3　节点选取示例





上面选取的例子最后实现的效果都是选取了所有符合条件的节点，是否能选取某个特定的节点或者包含某一个指定的值的节点呢？这就需要用到谓语，谓语被嵌在方括号中，接下来通过表格2-4来解释谓语的用法。


表2-4　谓语示例



XPath在进行节点选取的时候可以使用通配符“*”匹配未知的元素，同时使用操作符“|”一次选取多条路径，还是通过一个表格进行演示，如表2-5所示。

表2-5　通配符“*”与“1”操作符



3.XPath轴

轴定义了所选节点与当前节点之间的树关系。在Python爬虫开发中，提取网页中的信息会遇到这种情况：首先提取到一个节点的信息，然后想在在这个节点的基础上提取它的子节点或者父节点，这时候就会用到轴的概念。轴的存在会使提取变得更加灵活和准确。

在说轴的用法之前，需要了解位置路径表达式中的相对位置路径、绝对位置路径和步的概念。位置路径可以是绝对的，也可以是相对的。绝对路径起始于正斜杠（/），而相对路径不会这样。在两种情况中，位置路径均包括一个或多个步，每个步均被斜杠分割：/step/step/...（绝对位置路径），step/step/...（相对位置路径）。

步（step）包括：轴（axis）、节点测试（node-test）、零个或者更多谓语（predicate），用来更深入地提炼所选的节点集。步的语法为：轴名称：：节点测试[谓语]，大家可能觉比较抽象，通过之后的示例分析，会明白如何使用它。

表2-6列举了XPath轴中使用的节点集。

表2-6　XPath轴



首先给出一个XML文档，实例分析就按照这个文档来进行，文档如下：



* * *



<xml version="1.0" encoding="ISO-8859-1"> <classroom> <student> <id>1001</id> <name lang="en">marry</name> <age>20</age> <country>China</country> </student> <student> <id>1002</id> <name lang="en">jack</name> <age>25</age> <country>USA</country> </student> <teacher> <classid>1</classid> <name lang="en">tom</name> <age>50</age> <country>USA</country> </teacher> </classroom>



* * *



针对上面的文档进行示例演示，如表2-7所示。

表2-7　XPath轴示例分析



4.XPath运算符

XPath表达式可返回节点集、字符串、逻辑值以及数字。表2-8列举了可用在XPath表达式中的运算符。

表2-8　XPath运算符示例分析





2.1.5　JSON


JSON是JavaScript对象表示法（JavaScript Object Notation），用于存储和交换文本信息。JSON比XML更小、更快、更易解析，因此JSON在网络传输中，尤其是Web前端中运用非常广泛。JSON使用JavaScript语法来描述数据对象，但是JSON仍然独立于语言和平台。JSON解析器和JSON库支持许多不同的编程语言，其中就包括Python。

下面主要讲解一下JSON的语法，具体的存储解析放到第5章中进行讲解。JSON语法非常简单，主要包括以下几个方面：

·JSON名称/值对。JSON数据的书写格式是：名称/值对。名称/值对包括字段名称（在双引号中），紧接着是一个冒号，最后是值。例如，“name”：“qiye”，非常像Python中字典。

·JSON值。JSON值可以是：数字（整数或浮点数）、字符串（在双引号中）、逻辑值（true或false）、数组（在方括号中）、对象（在花括号中）、null。

·JSON对象。JSON对象在花括号中书写，对象可以包含多个名称/值对。例如：{“name”：“qiye”，“age”：“20”}，其实就是Python中的字典。

·JSON数组。JSON数组在方括号中书写，数组可包含多个对象。例如：{“reader”：[{“name”：“qiye”，“age”：“20”}，{“name”：“marry”，“age”：“21”}]}，这里对象“reader”是包含两个对象的数组。





2.2　HTTP标准


HTTP协议（HyperText Transfer Protocol，超文本传输协议）是用于从WWW服务器传输超文本到本地浏览器的传送协议。它可以使浏览器更加高效，减少网络传输。它不仅保证计算机正确快速地传输超文本文档，还确定传输文档中的哪一部分，以及哪部分内容首先显示（如文本先于图形）等。之后的Python爬虫开发，主要就是和HTTP协议打交道。





2.2.1　HTTP请求过程


HTTP协议采取的是请求响应模型，HTTP协议永远都是客户端发起请求，服务器回送响应。模型如图2-8所示。



图2-8　请求响应模型

HTTP协议是一个无状态的协议，同一个客户端的这次请求和上次请求没有对应关系。一次HTTP操作称为一个事务，其执行过程可分为四步：

·首先客户端与服务器需要建立连接，例如单击某个超链接，HTTP的工作就开始了。

·建立连接后，客户端发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是MIME信息，包括请求修饰符、客户机信息和可能的内容。

·服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息，包括服务器信息、实体信息和可能的内容。

·客户端接收服务器所返回的信息，通过浏览器将信息显示在用户的显示屏上，然后客户端与服务器断开连接。

如果以上过程中的某一步出现错误，那么产生错误的信息将返回到客户端，在显示屏输出，这些过程是由HTTP协议自己完成的。





2.2.2　HTTP状态码含义


当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。在浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。HTTP状态码主要是是为了标识此次HTTP请求的运行状态。下面是常见的HTTP状态码：

·200——请求成功。

·301——资源（网页等）被永久转移到其他URL。

·404——请求的资源（网页等）不存在。

·500——内部服务器错误。

HTTP状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型。HTTP状态码共分为5种类型，如表2-9所示。

表2-9　HTTP状态码



全部的HTTP状态码的信息，请大家查询HTTP协议标准手册。





2.2.3　HTTP头部信息


HTTP头部信息由众多的头域组成，每个头域由一个域名、冒号（：）和域值三部分组成。域名是大小写无关的，域值前可以添加任何数量的空格符，头域可以被扩展为多行，在每行开始处，使用至少一个空格或制表符。

通过浏览器访问博客园首页时，使用F12打开开发者工具，里面可以监控整个HTTP访问的过程。下面就以访问博客园的HTTP请求进行分析，首先是浏览器发出请求，请求头的数据如下：



* * *



GET / HTTP/1.1 Host: www.cnblogs.com User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3 Accept-Encoding: gzip, deflate Connection: keep-alive If-Modified-Since: Sun, 30 Oct 2016 10:13:18 GMT



* * *



在请求头中包含以下内容：

·GET代表的是请求方式，HTTP/1.1表示使用HTTP 1.1协议标准。

·Host头域，用于指定请求资源的Intenet主机和端口号，必须表示请求URL的原始服务器或网关的位置。HTTP/1.1请求必须包含主机头域，否则系统会以400状态码返回。

·User-Agent头域，里面包含发出请求的用户信息，其中有使用的浏览器型号、版本和操作系统的信息。这个头域经常用来作为反爬虫的措施。

·Accept请求报头域，用于指定客户端接受哪些类型的信息。例如：Accept：image/gif，表明客户端希望接受GIF图象格式的资源；Accept：text/html，表明客户端希望接受html文本。

·Accept-Language请求报头域，类似于Accept，但是它用于指定一种自然语言。例如：Accept-Language：zh-cn.如果请求消息中没有设置这个报头域，服务器假定客户端对各种语言都可以接受。

·Accept-Encoding请求报头域，类似于Accept，但是它用于指定可接受的内容编码。例如：Accept-Encoding：gzip.deflate。如果请求消息中没有设置这个域服务器假定客户端对各种内容编码都可以接受。

·Connection报头域允许发送用于指定连接的选项。例如指定连接的状态是连续，或者指定“close”选项，通知服务器，在响应完成后，关闭连接。

·If-Modified-Since头域用于在发送HTTP请求时，把浏览器端缓存页面的最后修改时间一起发到服务器去，服务器会把这个时间与服务器上实际文件的最后修改时间进行比较。如果时间一致，那么返回HTTP状态码304（不返回文件内容），客户端收到之后，就直接把本地缓存文件显示到浏览器中。如果时间不一致，就返回HTTP状态码200和新的文件内容，客户端收到之后，会丢弃旧文件，把新文件缓存起来，并显示到浏览器中。

请求发送成功后，服务器进行响应，接下来看一下响应的头信息，数据如下：



* * *



HTTP/1.1 200 OK Date: Sun, 30 Oct 2016 10:13:50 GMT Content-Type: text/html; charset=utf-8 Transfer-Encoding: chunked Connection: keep-alive Vary: Accept-Encoding Cache-Control: public, max-age=3 Expires: Sun, 30 Oct 2016 10:13:54 GMT Last-Modified: Sun, 30 Oct 2016 10:13:24 GMT Content-Encoding: gzip



* * *



响应头中包含以下内容：

·HTTP/1.1表示使用HTTP 1.1协议标准，200OK说明请求成功。

·Date表示消息产生的日期和时间。

·Content-Type实体报头域用于指明发送给接收者的实体正文的媒体类型。text/html；charset=utf-8代表HTML文本文档，UTF-8编码。

·Transfer-Encoding：chunked表示输出的内容长度不能确定。

·Connection报头域允许发送用于指定连接的选项。例如指定连接的状态是连续，或者指定“close”选项，通知服务器，在响应完成后，关闭连接。

·Vary头域指定了一些请求头域，这些请求头域用来决定当缓存中存在一个响应，并且该缓存没有过期失效时，是否被允许利用此响应去回复后续请求而不需要重复验证。

·Cache-Control用于指定缓存指令，缓存指令是单向的，且是独立的。请求时的缓存指令包括：no-cache（用于指示请求或响应消息不能缓存）、no-store、max-age、max-stale、min-fresh、only-if-cached；响应时的缓存指令包括：public、private、no-cache、no-store、no-transform、must-revalidate、proxy-revalidate、max-age、s-maxage。

·Expires实体报头域给出响应过期的日期和时间。为了让代理服务器或浏览器在一段时间以后更新缓存中（再次访问曾访问过的页面时，直接从缓存中加载，缩短响应时间和降低服务器负载）的页面，我们可以使用Expires实体报头域指定页面过期的时间。

·Last-Modified实体报头域用于指示资源的最后修改日期和时间。

·Content-Encoding实体报头域被用作媒体类型的修饰符，它的值指示了已经被应用到实体正文的附加内容的编码，因而要获得Content-Type报头域中所引用的媒体类型，必须采用相应的解码机制。

从上面分析的过程中，大家基本上了解了请求和响应的头信息，最后进行一下总结：

HTTP消息报头主要包括普通报头、请求报头、响应报头、实体报头。具体如下：

1）在普通报头中，有少数报头域用于所有的请求和响应消息，但并不用于被传输的实体，只用于传输的消息。

2）请求报头允许客户端向服务器端传递请求的附加信息以及客户端自身的信息。

3）响应报头允许服务器传递不能放在状态行中的附加响应信息，以及关于服务器的信息和对Request-URI所标识的资源进行下一步访问的信息。

4）请求和响应消息都可以传送一个实体。一个实体由实体报头域和实体正文组成，但并不是说实体报头域和实体正文要在一起发送，可以只发送实体报头域。实体报头定义了关于实体正文和请求所标识的资源的元信息。

通过表2-10～表2-13对报文头进行分类列举说明。

表2-10　常见普通报头



表2-11　常见请求报头



表2-12　常见响应报头



表2-13　常见实体报头





2.2.4　Cookie状态管理


Cookie和Session都用来保存状态信息，都是保存客户端状态的机制，它们都是为了解决HTTP无状态的问题所做的努力。对于爬虫开发来说，我们更加关注的是Cookie，因为Cookie将状态保存在客户端，Session将状态保存在服务器端。

Cookie是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。网络服务器用HTTP头向客户端发送Cookie，浏览器则会解析这些Cookie并将它们保存为一个本地文件，它会自动将同一服务器的任何请求绑定上这些Cookie。

Cookie的工作方式：服务器给每个Session分配一个唯一的JSESSIONID，并通过Cookie发送给客户端。当客户端发起新的请求的时候，将在Cookie头中携带这个JSESSIONID。这样服务器能够找到这个客户端对应的Session，流程如图2-9所示。



图2-9　Cookie工作流程





2.2.5　HTTP请求方式


HTTP的请求方法包括如下几种：

·GET

·POST

·HEAD

·PUT

·DELETE

·OPTIONS

·TRACE

·CONNECT

其中常用的请求方式是GET和POST：

·GET方式：是以实体的方式得到由请求URL所指定资源的信息，如果请求URL只是一个数据产生过程，那么最终要在响应实体中返回的是处理过程的结果所指向的资源，而不是处理过程的描述。

·POST方式：用来向目的服务器发出请求，要求它接受被附在请求后的实体，并把它当作请求队列中请求URL所指定资源的附加新子项。

GET与POST方法有以下区别：

·在客户端，Get方式通过URL提交数据，数据在URL中可以看到：POST方式，数据放置在实体区内提交。

·GET方式提交的数据最多只能有1024字节，而POST则没有此限制。

·安全性问题。使用Get的时候，参数会显示在地址栏上，而Post不会。所以，如果这些数据是非敏感数据，那么使用Get；如果用户输入的数据包含敏感数据，那么还是使用Post为好。

在爬虫开发中基本处理的也是GET和POST请求。GET请求在访问网页时很常见，POST请求则是常用在登录框、提交框的位置。下面展示一个完整的POST请求，这是登录知乎社区时捕获的请求，上面一部分是请求头，下面全部加粗的数据是请求实体。请求内容如下：



* * *



POST /login/phone_num HTTP/1.1 Host: www.zhihu.com User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0 Accept: */* Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3 Accept-Encoding: gzip, deflate, br X-Xsrftoken: ade0896dc13cc3b2204a8f7742ad7f48 Content-Type: application/x-www-form-urlencoded; charset=UTF-8 X-Requested-With: XMLHttpRequest Referer: https:// www.zhihu.com/ Content-Length: 117 Cookie:q_c1=7bc53a12dd7942d3b64776441ab69983|1477975324000|1465870098000; d_c0="ACAAa1M-EwqPTgdv2RIP3IIzHO2R7zKBGpw=|1465870098";__utma=51854390.1118849962.1465870098.1466355392.1477975328.3;__utmz=51854390.1465870098.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none);__utmv=51854390.000--|3=entry_date=20160614=1;_zap=7514ab27-5b42-4c95-a4cc-e31ce5757e14;_za=4ab6eb3b-c34f-4772-aac2-7182f21894cb;_xsrf=ade0896dc13cc3b2204a8f7742ad7f48;l_cap_id="ZjBkODkyYjdiZWZkNDQ2NWE4YzI1ZTk3Njc0MDZlMWM=|1477975324|95c5032340720551391178c9ee67cd8a3e2849d5";cap_id="ZjAxNjBmNzU5NzZkNDI2ZTlkYTk3ZDVlNDNhNzgyZTA=|1477975324|0616dfa45cd15d66fe792484c6ae0af71557cb3c";n_c=1;__utmb=51854390.2.10.1477975328;__utmc=51854390;__utmt=1; login="ZWU1NTFlM2EzYzg4NDNjNzlhODY wN2ZhYzgyZmExOTE=|1477975348|735a805117328df9e557f0126eb348e7712e310c" Connection: keep-alive _xsrf=ade0896dc13cc3b2204a8f7742ad7f48&password=xxxxxxxx&captcha_type=cn&remember_me=true&phone_num=xxxxxxxxx



* * *





2.3　小结


本章主要讲解了Web前端中标记语言、脚本语言和HTTP的基本概念，在这些知识中，重点掌握HTML标记语言、XPath路径表达式的书写和HTTP请求流程，这对接下来的Python爬虫开发有着非常直接的作用，有助于爬虫开发的快速入门。本章讲解的只是Web前端的基础知识，希望大家有时间系统地学习Web前端的知识，这样对之后涉及协议分析和反爬虫措施的应对方面有很大帮助。





第3章　初识网络爬虫


从本章开始，将正式涉及Python爬虫的开发。本章主要分为两个部分：一部分是网络爬虫的概述，帮助大家详细了解网络爬虫；另一部分是HTTP请求的Python实现，帮助大家了解Python中实现HTTP请求的各种方式，以便具备编写HTTP网络程序的能力。





3.1　网络爬虫概述


本节正式进入Python爬虫开发的专题，接下来从网络爬虫的概念、用处与价值和结构等三个方面，让大家对网络爬虫有一个基本的了解。





3.1.1　网络爬虫及其应用


随着网络的迅速发展，万维网成为大量信息的载体，如何有效地提取并利用这些信息成为一个巨大的挑战，网络爬虫应运而生。网络爬虫 （又被称为网页蜘蛛、网络机器人），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。下面通过图3-1展示一下网络爬虫在互联网中起到的作用：

网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型：通用网络爬虫、聚焦网络爬虫、增量式网络爬虫、深层网络爬虫。实际的网络爬虫系统通常是几种爬虫技术相结合实现的。

搜索引擎 （Search Engine），例如传统的通用搜索引擎baidu、Yahoo和Google等，是一种大型复杂的网络爬虫，属于通用性网络爬虫的范畴。但是通用性搜索引擎存在着一定的局限性：

1）不同领域、不同背景的用户往往具有不同的检索目的和需求，通用搜索引擎所返回的结果包含大量用户不关心的网页。



图3-1　网络爬虫

2）通用搜索引擎的目标是尽可能大的网络覆盖率，有限的搜索引擎服务器资源与无限的网络数据资源之间的矛盾将进一步加深。

3）万维网数据形式的丰富和网络技术的不断发展，图片、数据库、音频、视频多媒体等不同数据大量出现，通用搜索引擎往往对这些信息含量密集且具有一定结构的数据无能为力，不能很好地发现和获取。

4）通用搜索引擎大多提供基于关键字的检索，难以支持根据语义信息提出的查询。

为了解决上述问题，定向抓取相关网页资源的聚焦爬虫应运而生。

聚焦爬虫是一个自动下载网页的程序，它根据既定的抓取目标，有选择地访问万维网上的网页与相关的链接，获取所需要的信息。与通用爬虫不同，聚焦爬虫并不追求大的覆盖，而将目标定为抓取与某一特定主题内容相关的网页，为面向主题的用户查询准备数据资源。

说完了聚焦爬虫，接下来再说一下增量式网络爬虫。增量式网络爬虫是指对已下载网页采取增量式更新和只爬行新产生的或者已经发生变化网页的爬虫，它能够在一定程度上保证所爬行的页面是尽可能新的页面。和周期性爬行和刷新页面的网络爬虫相比，增量式爬虫只会在需要的时候爬行新产生或发生更新的页面，并不重新下载没有发生变化的页面，可有效减少数据下载量，及时更新已爬行的网页，减小时间和空间上的耗费，但是增加了爬行算法的复杂度和实现难度。例如：想获取赶集网的招聘信息，以前爬取过的数据没有必要重复爬取，只需要获取更新的招聘数据，这时候就要用到增量式爬虫。

最后说一下深层网络爬虫。Web页面按存在方式可以分为表层网页和深层网页。表层网页是指传统搜索引擎可以索引的页面，以超链接可以到达的静态网页为主构成的Web页面。深层网络是那些大部分内容不能通过静态链接获取的、隐藏在搜索表单后的，只有用户提交一些关键词才能获得的Web页面。例如用户登录或者注册才能访问的页面。可以想象这样一个场景：爬取贴吧或者论坛中的数据，必须在用户登录后，有权限的情况下才能获取完整的数据。

本书除了通用性爬虫不会涉及之外，聚焦爬虫、增量式爬虫和深层网络爬虫的具体运用都会进行讲解。下面展示一下网络爬虫实际运用的一些场景：

1）常见的BT网站，通过爬取互联网的DHT网络中分享的BT种子信息，提供对外搜索服务。例如http://www.cilisou.cn/ ，如图3-2所示。



图3-2　磁力搜网站首页

2）一些云盘搜索网站，通过爬取用户共享出来的云盘文件数据，对文件数据进行分类划分，从而提供对外搜索服务。例如http://www.pansou.com/ ，如图3-3所示。



图3-3　盘搜网站首页





3.1.2　网络爬虫结构


下面用一个通用的网络爬虫结构来说明网络爬虫的基本工作流程，如图3-4所示。



图3-4　网络爬虫结构

网络爬虫的基本工作流程如下：

1）首先选取一部分精心挑选的种子URL。

2）将这些URL放入待抓取URL队列。

3）从待抓取URL队列中读取待抓取队列的URL，解析DNS，并且得到主机的IP，并将URL对应的网页下载下来，存储进已下载网页库中。此外，将这些URL放进已抓取URL队列。

4）分析已抓取URL队列中的URL，从已下载的网页数据中分析出其他URL，并和已抓取的URL进行比较去重，最后将去重过的URL放入待抓取URL队列，从而进入下一个循环。

这便是一个基本的通用网络爬虫框架及其工作流程，在之后的章节我们会用Python实现这种网络爬虫结构。





3.2　HTTP请求的Python实现


通过上面的网络爬虫结构，我们可以看到读取URL、下载网页是每一个爬虫必备而且关键的功能，这就需要和HTTP请求打交道。接下来讲解Python中实现HTTP请求的三种方式：urllib2/urllib、httplib/urllib以及Requests。





3.2.1　urllib2/urllib实现


urllib2和urllib是Python中的两个内置模块，要实现HTTP功能，实现方式是以urllib2为主，urllib为辅。

1.首先实现一个完整的请求与响应模型

urllib2提供一个基础函数urlopen，通过向指定的URL发出请求来获取数据。最简单的形式是：



* * *



import urllib2 response=urllib2.urlopen('http://www.zhihu.com') html=response.read() print html



* * *



其实可以将上面对http://www.zhihu.com 的请求响应分为两步，一步是请求，一步是响应，形式如下：



* * *



import urllib2 # 请求 request=urllib2.Request('http://www.zhihu.com') # 响应 response = urllib2.urlopen(request) html=response.read() print html



* * *



上面这两种形式都是GET请求，接下来演示一下POST请求，其实大同小异，只是增加了请求数据，这时候用到了urllib。示例如下：



* * *



import urllib import urllib2 url = 'http://www.xxxxxx.com/login' postdata = {'username' : 'qiye', 'password' : 'qiye_pass'} # info 需要被编码为urllib2能理解的格式，这里用到的是urllib data = urllib.urlencode(postdata) req = urllib2.Request(url, data) response = urllib2.urlopen(req) html = response.read()



* * *



但是有时会出现这种情况：即使POST请求的数据是对的，但是服务器拒绝你的访问。这是为什么呢问题出在请求中的头信息，服务器会检验请求头，来判断是否是来自浏览器的访问，这也是反爬虫的常用手段。

2.请求头headers处理

将上面的例子改写一下，加上请求头信息，设置一下请求头中的User-Agent域和Referer域信息。



* * *



import urllib import urllib2 url = 'http://www.xxxxxx.com/login' user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' referer='http://www.xxxxxx.com/' postdata = {'username' : 'qiye', 'password' : 'qiye_pass'} # 将user_agent,referer写入头信息 headers={'User-Agent':user_agent,'Referer':referer} data = urllib.urlencode(postdata) req = urllib2.Request(url, data,headers) response = urllib2.urlopen(req) html = response.read()



* * *



也可以这样写，使用add_header来添加请求头信息，修改如下：



* * *



import urllib import urllib2 url = 'http://www.xxxxxx.com/login' user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' referer='http://www.xxxxxx.com/' postdata = {'username' : 'qiye', 'password' : 'qiye_pass'} data = urllib.urlencode(postdata) req = urllib2.Request(url) # 将user_agent,referer写入头信息 req.add_header('User-Agent',user_agent) req.add_header('Referer',referer) req.add_data(data) response = urllib2.urlopen(req) html = response.read()



* * *



对有些header要特别留意，服务器会针对这些header做检查，例如：

·User-Agent：有些服务器或Proxy会通过该值来判断是否是浏览器发出的请求。

·Content-Type：在使用REST接口时，服务器会检查该值，用来确定HTTP Body中的内容该怎样解析。在使用服务器提供的RESTful或SOAP服务时，Content-Type设置错误会导致服务器拒绝服务。常见的取值有：application/xml（在XML RPC，如RESTful/SOAP调用时使用）、application/json（在JSON RPC调用时使用）、application/x-www-form-urlencoded（浏览器提交Web表单时使用）。

·Referer：服务器有时候会检查防盗链。

3.Cookie处理

urllib2对Cookie的处理也是自动的，使用CookieJar函数进行Cookie的管理。如果需要得到某个Cookie项的值，可以这么做：



* * *



import urllib2 import cookielib cookie = cookielib.CookieJar() opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie)) response = opener.open('http://www.zhihu.com') for item in cookie: print item.name+':'+item.value



* * *



但是有时候会遇到这种情况，我们不想让urllib2自动处理，我们想自己添加Cookie的内容，可以通过设置请求头中的Cookie域来做：



* * *



import urllib2 opener = urllib2.build_opener() opener.addheaders.append( ( 'Cookie', 'email=' + "xxxxxxx@163.com" ) ) req = urllib2.Request( "http://www.zhihu.com/" ) response = opener.open(req) print response.headers retdata = response.read()



* * *



4.Timeout设置超时

在Python2.6之前的版本，urllib2的API并没有暴露Timeout的设置，要设置Timeout值，只能更改Socket的全局Timeout值。示例如下：



* * *



import urllib2 import socket socket.setdefaulttimeout(10) # 10 秒钟后超时 urllib2.socket.setdefaulttimeout(10) # 另一种方式



* * *



在Python2.6及新的版本中，urlopen函数提供了对Timeout的设置，示例如下：



* * *



import urllib2 request=urllib2.Request('http://www.zhihu.com') response = urllib2.urlopen(request,timeout=2) html=response.read() print html



* * *



5.获取HTTP响应码

对于200OK来说，只要使用urlopen返回的response对象的getcode（）方法就可以得到HTTP的返回码。但对其他返回码来说，urlopen会抛出异常。这时候，就要检查异常对象的code属性了，示例如下：



* * *



import urllib2 try: response = urllib2.urlopen('http://www.google.com') print response except urllib2.HTTPError as e: if hasattr(e, 'code'): print 'Error code:',e.code



* * *



6.重定向

urllib2默认情况下会针对HTTP 3XX返回码自动进行重定向动作。要检测是否发生了重定向动作，只要检查一下Response的URL和Request的URL是否一致就可以了，示例如下：



* * *



import urllib2 response = urllib2.urlopen('http://www.zhihu.cn') isRedirected = response.geturl() == 'http://www.zhihu.cn'



* * *



如果不想自动重定向，可以自定义HTTPRedirectHandler类，示例如下：



* * *





import urllib2 class RedirectHandler(urllib2.HTTPRedirectHandler): def http_error_301(self, req, fp, code, msg, headers): pass def http_error_302(self, req, fp, code, msg, headers): result = urllib2.HTTPRedirectHandler.http_error_301(self, req, fp, code, msg, headers) result.status = code result.newurl = result.geturl() return result opener = urllib2.build_opener(RedirectHandler) opener.open('http://www.zhihu.cn')




* * *



7.Proxy的设置

在做爬虫开发中，必不可少地会用到代理。urllib2默认会使用环境变量http_proxy来设置HTTP Proxy。但是我们一般不采用这种方式，而是使用ProxyHandler在程序中动态设置代理，示例代码如下：



* * *



import urllib2 proxy = urllib2.ProxyHandler({'http': '127.0.0.1:8087'}) opener = urllib2.build_opener([proxy,]) urllib2.install_opener(opener) response = urllib2.urlopen('http://www.zhihu.com/') print response.read()



* * *



这里要注意的一个细节，使用urllib2.install_opener（）会设置urllib2的全局opener，之后所有的HTTP访问都会使用这个代理。这样使用会很方便，但不能做更细粒度的控制，比如想在程序中使用两个不同的Proxy设置，这种场景在爬虫中很常见。比较好的做法是不使用install_opener去更改全局的设置，而只是直接调用opener的open方法代替全局的urlopen方法，修改如下：



* * *



import urllib2 proxy = urllib2.ProxyHandler({'http': '127.0.0.1:8087'}) opener = urllib2.build_opener(proxy,) response = opener.open("http://www.zhihu.com/") print response.read()



* * *





3.2.2　httplib/urllib实现


httplib模块是一个底层基础模块，可以看到建立HTTP请求的每一步，但是实现的功能比较少，正常情况下比较少用到。在Python爬虫开发中基本上用不到，所以在此只是进行一下知识普及。下面介绍一下常用的对象和函数：

·创建HTTPConnection对象：class httplib.HTTPConnection（host[，port[，strict[，timeout[，source_address]]]]）。

·发送请求：HTTPConnection.request（method，url[，body[，headers]]）。

·获得响应：HTTPConnection.getresponse（）。

·读取响应信息：HTTPResponse.read（[amt]）。

·获得指定头信息：HTTPResponse.getheader（name[，default]）。

·获得响应头（header，value）元组的列表：HTTPResponse.getheaders（）。

·获得底层socket文件描述符：HTTPResponse.fileno（）。

·获得头内容：HTTPResponse.msg。

·获得头http版本：HTTPResponse.version。

·获得返回状态码：HTTPResponse.status。

·获得返回说明：HTTPResponse.reason。

接下来演示一下GET请求和POST请求的发送，首先是GET请求的示例，如下所示：



* * *



import httplib conn =None try: conn = httplib.HTTPConnection("www.zhihu.com") conn.request("GET", "/") response = conn.getresponse() print response.status, response.reason print '-' * 40 headers = response.getheaders() for h in headers: print h print '-' * 40 print response.msg except Exception,e: print e finally: if conn: conn.close()



* * *



POST请求的示例如下：



* * *



import httplib, urllib conn = None try: params = urllib.urlencode({'name': 'qiye', 'age': 22}) headers = {"Content-type": "application/x-www-form-urlencoded" , "Accept": "text/plain"} conn = httplib.HTTPConnection("www.zhihu.com", 80, timeout=3) conn.request("POST", "/login", params, headers) response = conn.getresponse() print response.getheaders() # 获取头信息 print response.status print response.read() except Exception, e: print e finally: if conn: conn.close()



* * *





3.2.3　更人性化的Requests


Python中Requests实现HTTP请求的方式，是本人极力推荐的，也是在Python爬虫开发中最为常用的方式。Requests实现HTTP请求非常简单，操作更加人性化。

Requests库是第三方模块，需要额外进行安装。Requests是一个开源库，源码位于GitHub：https://github.com/kennethreitz/requests ，希望大家多多支持作者。使用Requests库需要先进行安装，一般有两种安装方式：

·使用pip进行安装，安装命令为：pip install requests，不过可能不是最新版。

·直接到GitHub上下载Requests的源代码，下载链接为：https://github.com/kennethreitz/requests/releases 。将源代码压缩包进行解压，然后进入解压后的文件夹，运行setup.py文件即可。

如何验证Requests模块安装是否成功呢？在Python的shell中输入import requests，如果不报错，则是安装成功。如图3-5所示。



图3-5　验证Requests安装

1.首先还是实现一个完整的请求与响应模型

以GET请求为例，最简单的形式如下：



* * *



import requests r = requests.get('http://www.baidu.com') print r.content



* * *



大家可以看到比urllib2实现方式的代码量少。接下来演示一下POST请求，同样是非常简短，更加具有Python风格。示例如下：



* * *



import requests postdata={'key':'value'} r = requests.post('http://www.xxxxxx.com/login',data=postdata) print r.content



* * *



HTTP中的其他请求方式也可以用Requests来实现，示例如下：

·r=requests.put（'http://www.xxxxxx.com/put'，data={'key':'value'}）

·r=requests.delete（'http://www.xxxxxx.com/delete'）

·r=requests.head（'http://www.xxxxxx.com/get'）

·r=requests.options（'http://www.xxxxxx.com/get'）

接着讲解一下稍微复杂的方式，大家肯定见过类似这样的URL：http://zzk.cnblogs.com/s/　blogpost ?Keywords=blog:qiyeboy&pageindex=1 ，就是在网址后面紧跟着“?”，“?”后面还有参数。那么这样的GET请求该如何发送呢？肯定有人会说，直接将完整的URL带入即可，不过Requests还提供了其他方式，示例如下：



* * *





import requests payload = {'Keywords': 'blog:qiyeboy','pageindex':1} r = requests.get('http://zzk.cnblogs.com/s/blogpost', params=payload) print r.url




* * *



通过打印结果，我们看到最终的URL变成了：



* * *



http://zzk.cnblogs.com/s/blogpostKeywords=blog:qiyeboy&pageindex=1。



* * *



2.响应与编码

还是从代码入手，示例如下：



* * *



import requests r = requests.get('http://www.baidu.com') print 'content-->'+r.content print 'text-->'+r.text print 'encoding-->'+r.encoding r.encoding='utf-8' print 'new text-->'+r.text



* * *



其中r.content返回的是字节形式，r.text返回的是文本形式，r.encoding返回的是根据HTTP头猜测的网页编码格式。

输出结果中：“text-->”之后的内容在控制台看到的是乱码，“encoding-->”之后的内容是ISO-8859-1（实际上的编码格式是UTF-8），由于Requests猜测编码错误，导致解析文本出现了乱码。Requests提供了解决方案，可以自行设置编码格式，r.encoding=‘utf-8’设置成UTF-8之后，“new text-->”的内容就不会出现乱码。但是这种手动的方式略显笨拙，下面提供一种更加简便的方式：chardet，这是一个非常优秀的字符串/文件编码检测模块。安装方式如下：



* * *



pip install chardet



* * *



安装完成后，使用chardet.detect（）返回字典，其中confidence是检测精确度，encoding是编码形式。示例如下：



* * *



import requests r = requests.get('http://www.baidu.com') print chardet.detect(r.content) r.encoding = chardet.detect(r.content)['encoding'] print r.text



* * *



直接将chardet探测到的编码，赋给r.encoding实现解码，r.text输出就不会有乱码了。

除了上面那种直接获取全部响应的方式，还有一种流模式，示例如下：



* * *



import requests r = requests.get('http://www.baidu.com',stream=True) print r.raw.read(10)



* * *



设置stream=True标志位，使响应以字节流方式进行读取，r.raw.read函数指定读取的字节数。

3.请求头headers处理

Requests对headers的处理和urllib2非常相似，在Requests的get函数中添加headers参数即可。示例如下：



* * *



import requests user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers={'User-Agent':user_agent} r = requests.get('http://www.baidu.com',headers=headers) print r.content



* * *



4.响应码code和响应头headers处理

获取响应码是使用Requests中的status_code字段，获取响应头使用Requests中的headers字段。示例如下：



* * *



import requests r = requests.get('http://www.baidu.com') if r.status_code == requests.codes.ok: print r.status_code# 响应码 print r.headers# 响应头 print r.headers.get('content-type')# 推荐使用这种获取方式，获取其中的某个字段 print r.headers['content-type']# 不推荐使用这种获取方式 else: r.raise_for_status()



* * *



上述程序中，r.headers包含所有的响应头信息，可以通过get函数获取其中的某一个字段，也可以通过字典引用的方式获取字典值，但是不推荐，因为如果字段中没有这个字段，第二种方式会抛出异常，第一种方式会返回None。r.raise_for_status（）是用来主动地产生一个异常，当响应码是4XX或5XX时，raise_for_status（）函数会抛出异常，而响应码为200时，raise_for_status（）函数返回None。

5.Cookie处理

如果响应中包含Cookie的值，可以如下方式获取Cookie字段的值，示例如下：



* * *



import requests user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers={'User-Agent':user_agent} r = requests.get('http://www.baidu.com',headers=headers) # 遍历出所有的cookie字段的值 for cookie in r.cookies.keys(): print cookie+':'+r.cookies.get(cookie)



* * *



如果想自定义Cookie值发送出去，可以使用以下方式，示例如下：



* * *



import requests user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers={'User-Agent':user_agent} cookies = dict(name='qiye',age='10') r = requests.get('http://www.baidu.com',headers=headers,cookies=cookies) print r.text



* * *



还有一种更加高级，且能自动处理Cookie的方式，有时候我们不需要关心Cookie值是多少，只是希望每次访问的时候，程序自动把Cookie的值带上，像浏览器一样。Requests提供了一个session的概念，在连续访问网页，处理登录跳转时特别方便，不需要关注具体细节。使用方法示例如下：



* * *



import Requests loginUrl = 'http://www.xxxxxxx.com/login' s = requests.Session() #首先访问登录界面，作为游客，服务器会先分配一个cookie r = s.get(loginUrl,allow_redirects=True) datas={'name':'qiye','passwd':'qiye'} #向登录链接发送post请求，验证成功，游客权限转为会员权限 r = s.post(loginUrl, data=datas,allow_redirects= True) print r.text



* * *



上面的这段程序，其实是正式做Python开发中遇到的问题，如果没有第一步访问登录的页面，而是直接向登录链接发送Post请求，系统会把你当做非法用户，因为访问登录界面时会分配一个Cookie，需要将这个Cookie在发送Post请求时带上，这种使用Session函数处理Cookie的方式之后会很常用。

6.重定向与历史信息

处理重定向只是需要设置一下allow_redirects字段即可，例如r=requests.get（‘http://www.baidu.com ’，allow_redirects=True）。将allow_redirects设置为True，则是允许重定向；设置为False，则是禁止重定向。如果是允许重定向，可以通过r.history字段查看历史信息，即访问成功之前的所有请求跳转信息。示例如下：



* * *



import requests r = requests.get('http://github.com') print r.url print r.status_code print r.history



* * *



打印结果如下：



* * *



https:// github.com/ 200 (<Response [301]>,)



* * *



上面的示例代码显示的效果是访问GitHub网址时，会将所有的HTTP请求全部重定向为HTTPS。

7.超时设置

超时选项是通过参数timeout来进行设置的，示例如下：



* * *



requests.get('http://github.com', timeout=2)



* * *



8.代理设置

使用代理Proxy，你可以为任意请求方法通过设置proxies参数来配置单个请求：



* * *





import requests proxies = { "http": "http://0.10.1.10:3128", "https": "http://10.10.1.10:1080", } requests.get("http://example.org", proxies=proxies)




* * *



也可以通过环境变量HTTP_PROXY和HTTPS_PROXY来配置代理，但是在爬虫开发中不常用。你的代理需要使用HTTP Basic Auth，可以使用http://user：password@host/语法：



* * *



proxies = { "http": "http://user:pass@10.10.1.10:3128/", }



* * *





3.3　小结


本章主要讲解了网络爬虫的结构和应用，以及Python实现HTTP请求的几种方法。希望大家对本章中的网络爬虫工作流程和Requests实现HTTP请求的方式重点吸收消化。





第4章　HTML解析大法


HTML网页数据解析提取是Python爬虫开发中非常关键的一步。HTML网页的解析提取有很多种方式，本章主要从三个方面进行讲解，分别为Firebug工具的使用、正则表达式和Beautiful soup，基本上涵盖了HTML网页数据解析提取的方方面面。





4.1　初识Firebug


Firebug是一个用于Web前端开发的工具，它是FireFox浏览器的一个扩展插件。它可以用于调试JavaScript、查看DOM、分析CSS、监控网络流量以及进行Ajax交互等。它几乎提供了前端开发需要的全部功能，因此在Python爬虫开发中非常有用，尤其是在分析协议和分析动态网站的时候，本节我们所有的分析场景都是基于这个工具，基于FireFox浏览器。Firebug面板如图4-1所示。



图4-1　Firebug面板

大家如果之前用过Firebug，会发现在面板上多了一个FirePath的选项。FirePath是Firebug上的一个扩展插件，它的功能主要是帮助我们精确定位网页中的元素，生成XPath或者是CSS查找路径表达式，这在Python爬虫开发中抽取网页元素非常便利，省去了手写XPath和CSS路径表达式的麻烦。FirePath选项面板内容如图4-2所示。



图4-2　FirePath面板





4.1.1　安装Firebug


由于Firebug是FireFox浏览器的一个扩展插件，所以首先需要下载FireFox（火狐）浏览器。读者可以访问www.mozilla.com下载并安装FireFox浏览器。安装完成后用FireFox访问https://addons.mozilla.org/zh-CN/firefox/collections/mozilla/webdeveloper/ ，进入如图4-3所示页面。点击“添加到Firefox”，然后点击“立即安装”，最后重新启动FireFox浏览器即可完成安装。



图4-3　Firebug下载页面

Firebug安装完成后，为了扩展Firebug在路径选择上的功能，还需要安装Firebug的插件FirePath。打开火狐浏览器，进入“设置→附件组件→搜索”，输入firepath，如图4-4所示。



图4-4　FirePath安装





4.1.2　强大的功能


下面按照主面板、子面板的顺序说明Firebug的强大功能，相信大家会被Firebug所吸引。

1.主面板

安装完成之后，在Firefox浏览器的地址后方就会有一个小虫子的图标，这是Firebug启动开关，如图4-5所示。



图4-5　Firebug启动开关

单击该图标后即可展开Firebug的控制台，也可以通过快捷键<F12>来打开控制台，默认位于Firefox浏览器底部。使用Ctrl+F12快捷键可以使Firebug独立打开一个窗口而不占用Firefox页面底部的空间，如图4-6所示。



图4-6　Firebug主面板

从上图中可以看出，Firebug包括8个子面板：

·控制台面板：用于记录日志、概览、错误提示和执行命令行，同时也用于Ajax的调试。

·HTML面板：用于查看HTML元素，可以实时地编辑HTML和改变CSS样式，它包括3个子面板，分别是样式、计算出的样式、布局、DOM和事件面板。

·CSS面板：用于查看所有页面上的CSS文件，可以动态地修改CSS样式，由于HTML面板中已经包含了样式面板，因此该面板将很少用到。

·脚本面板：用于显示JavaScript文件及其所在的页面，也可以用来显示Javascript的Debug调试信息，包含3个子面板，分别是监控、堆栈和断点。

·DOM面板：用于显示页面上的所有对象。

·网络面板：用于监视网络活动，可以帮助查看一个页面的载入情况，包括文件下载所占用的时间和文件下载出错等信息，也可以用于监视Ajax行为。在分析网络请求和动态网站加载时非常有用。

·Cookies面板：用于查看和调整cookie。

·FirePath面板：用于精确定位网页中的元素，生成XPath或者是CSS查找路径表达式。

2.控制台面板

控制台面板可以用于记录日志，也可以用于输入脚本的命令行。Firebug提供如下几个常用的记录日志的函数：

·console.log：简单的记录日志。

·console.debug：记录调试信息，并且附上行号的超链接。

·console.error：在消息前显示错误图标，并且附上行号的超链接。

·console.info：在消息前显示消息图标，并且附上行号的超链接。

·console.warn：在消息前显示警告图标，并且附行号的超链接。

例如新建一个html页面中，向<body>标记中加入<script>标记，代码如下：



* * *



<!DOCTYPE HTML PUBLIC "-// W3C// DTD HTML 4.01// EN" "http://www.w3.org/TR/html4/strict.dtd"> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <title>Firefox测试</title> </head> <body> <script type="text/javascript"> var a = "Python"; var b = "爬虫开发"; document.write(a,b);// 网页上输出内容 console.log(a + b); console.debug(a + b); console.error(a + b); console.info(a + b); console.warn(a + b); </script> </body> </html>



* * *



在Firefox浏览器中开启Firebug并运行此HTML文档，效果如图4-7所示。



图4-7　Firebug控制台输出结果

也可以直接在右侧输入JavaScript代码执行，同时可以对输入的源代码格式进行美化，示例如图4-8所示。



图4-8　Firebug js脚本执行

控制台面板内有一排子菜单，分别是清除、保持、概况、全部等。

·“清除”用于清除控制台中的内容。

·“保持”则是把控制台中的内容保存，即使刷新了依然还存在。

·“全部”则是显示全部的信息。

·“概况”菜单用于查看函数的性能。

·后面的“错误”、“警告”、“消息”、“调试信息”、“Cookies”菜单则是对所有的信息进行了分类。

控制台面板还可以进行Ajax调试。例如打开一个页面，可以在Firebug控制台查看到本次Ajax的HTTP请求头信息和服务器响应头信息。首先在Firefox浏览器中开启Firebug，并访问百度的首页，可以看到图4-9的效果。





图4-9　Ajax请求


如果没有上图的效果，可以在控制台的下拉菜单中，选中显示XMLHttpRequests，如图4-10所示。



图4-10　显示XMLHttpRequests

3.HTML面板

HTML面板的强大之处就是能查看和修改HTML代码，而且这些代码都是经过格式化的。以百度首页为例，在HTML控制台的左侧可以看到整个页面当前的文档结构，可以通过单击“+”来展开。当单击相应的元素时，右侧面板中就会显示出当前元素的样式、布局以及DOM信息，效果如图4-11所示。



图4-11　百度首页HTML结构

而当光标移动到HTML树中相应元素上时，页面中相应的元素将会被高亮显示，高亮部分我用框圈起来了，如图4-12所示。



图4-12　“百度一下”高亮显示

还有一种更快更常用的查找HTML元素的方法。利用查看（Inspect）功能，可以快速地寻找到某个元素的HTML结构，如图4-13所示，线框圈起来的就是Inspect按钮。



图4-13　Inspect按钮查看元素

当单击Inspect按钮后，用鼠标在网页上选中一个元素时，元素会被一个蓝色的框框住，同时下面的HTML面板中相应的HTML树也会展开并且高亮显示，再次单击后即可退出该模式。通过这个功能，可以快速寻找页面内的元素，调试和查找相应代码非常方便。

之前讲的都是查看HTML，还可以修改HTML内容和样式。例如，将百度首页的“百度一下”按钮文字修改为“搜索一下”，只需将input标记中的value值改为搜索一下，如图4-14所示。



图4-14　修改HTML元素值

在这个基础上，修改一下样式，将background值改为red，“搜索一下”的背景立即变成了红色，效果如图4-15所示。



图4-15　修改HTML样式

4.网络面板

在Python爬虫开发中，网络面板比较常用，能够监听网络访问请求与响应，在分析异步加载请求时非常有用。例如访问百度首页，效果如图4-16所示。



图4-16　网络请求

线框中可以看到，网络访问的头信息、响应码、响应内容和Cookies都能得到有效记录。

在网络面板的子菜单中又分为HTML、CSS、JavaScript、XHR、图片等选项，其实只是将所有的网络访问进行了分类划分。

5.脚本面板

脚本面板不仅可以查看页面内的脚本，而且还有强大的调试功能。在脚本面板的右侧有“监控”、“堆栈”和“断点”三个面板，利用Firebug提供的设置断点的功能，可以很方便地调试程序，还可以将JavaScript脚本格式化，方便阅读源码进行分析。Firebug脚本面板如图4-17所示。

接下来测试一下脚本面板的断点调试功能，以jsTest.html文件为例，代码如下：



* * *



<!DOCTYPE html PUBLIC "-// W3C// DTD XHTML 1.0 Transitional// EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"> <html> <head> <meta charset="utf-8"> <script type="text/javascript"> function doLogin(){ var msg = document.getElementById('message'); var username = document.getElementById('username'); var password = document.getElementById('password'); arrs=[1,2,3,4,5,6,7,8,9]; for(var arr in arrs){ msg.innerHTML+=arr+"<br />" msg.innerHTML+="username->"+username.value +"password->"+password.value+"<br />" } } </script> </head> <body> <div> <input id="username" type="text" placeholder="用户名" value=""/> <br/> <input id="password" type="text" placeholder="密码" value=""/> <br/> <input type="button" value="login" onClick="doLogin();"/> <br/> <div id="message"></div> </div> </body> </html>



* * *





图4-17　脚本面板

运行代码后可以看到如图4-18所示的效果。图中加粗并有绿色的行号的代码表示此处为JavaScript代码，可以在此处设置断点。比如在第8行这句代码前面单击一下，它前面就会出现一个红褐色的圆点，表示此处已经被设置了断点。此时，在右侧断点面板的断点列表中就出现了刚才设置的断点。如果想暂时禁用某个断点，可以在断点列表中去掉某个断点的前面的复选框中的勾，那么此时左侧面板中相应的断点就从红褐色变成了红灰褐色了。



图4-18　断点设置

设置完断点之后，我们就可以调试程序了。单击页面中的“login”按钮，可以看到脚本停止在用淡黄色底色标出的那一行上。此时用鼠标移动到某个变量上即可显示此时这个变量的值。显示效果如图4-19所示。



图4-19　断点调试

此时JavaScript内容上方的 四个按钮已经变得可用了。它们分别代表“继续执行”、“单步进入”、“单步跳过”和“单步退出”。可以使用快捷键进行操作：

·继续执行<F8>：当通过断点来停止执行脚本时，单击<F8>就会恢复执行脚本。

·单步进入<F11>：允许跳到页面中的其他函数内部。

·单步跳过<F10>：直接跳过函数的调用，即跳到return之后。

·单步退出<shift+F11>：允许恢复脚本的执行，直到下一个断点为止。

单击“单步进入”按钮，代码会跳到下一行，当鼠标移动到“msg”变量上时，就可以显示出它的内容是一个DOM元素，即“div#message”。将右侧面板切换到“监控”面板，这里列出了几个变量，包括“this”指针的指向以及“msg”变量。单击“+”可以看到详细的信息，如图4-20所示。





图4-20　单步调试

以上设置的都是静态断点，脚本面板还提供了条件断点的高级功能。在要调试的代码前面的序号上单击鼠标右键，就可以出现设置条件断点的输入框。在该框内输入“arr==5”，然后回车确认，显示效果如图4-21所示。

最后单击页面的“login”按钮。可以发现，脚本在“arr==5”这个表达式为真时停下了。

6.FirePath面板

切换到FirePath面板，通过查看（Inspect）按钮，点击“百度一下”按钮，XPath后面的输出框中出现XPath路径表达式，如图4-22所示，这在Python爬虫开发中非常有用。



图4-21　条件调试



图4-22　FirePath面板





4.2　正则表达式


在编写处理网页文本的程序时，经常会有查找符合某些复杂规则的字符串的需要。正则表达式就是用于描述这些规则的工具。正则表达式是由普通字符（例如字符a到z）以及特殊字符（称为“元字符”）组成的文字模式。模式用于描述在搜索文本时要匹配的一个或多个字符串。正则表达式作为一个模板，将某个字符模式与所搜索的字符串进行匹配。





4.2.1　基本语法与使用


正则表达式功能非常强大，但是学好并不是很困难。一些初学者总是感觉到正则表达式很抽象，看到稍微长的表达式直接选择放弃。接下来从一个新手的角度，由浅及深，配合各种示例来讲解正则表达式的用法。

1.入门小例子

学习正则表达式最好的办法就是通过例子。在不断解决问题的过程中，就会不断理解正则表达式构造方法的灵活多变。

例如我们想找到一篇英文文献中所有的we单词，你可以使用正则表达式：we，这是最简单的正则表达式，可以精确匹配英文文献中的we单词。正则表达式工具一般可以设置为忽略大小写，那we这个正则表达式可以将文献中的We、wE、we和WE都匹配出来。如果仅仅使用we来匹配，会发现得出来的结果和预想的不一样，类似于well、welcome这样的单词也会被匹配出来，因为这些单词中也包含we。如何仅仅将we单词匹配出来呢？我们需要使用这样的正则表达式：\bwe\b。

“\b”是正则表达式规定的一个特殊代码，被称为元字符，代表着单词的开头或结尾，也就是单词的分界处，它不代表英语中空格、标点符号、换行等单词分隔符，只是用来匹配一个位置，这种理解方式很关键。

假如我们看到we单词不远处有一个work单词，想把we、work和它们之间的所有内容都匹配出来，那么我们需要了解另外两个元字符“.”和“*”，正则表达式可以写为\bwe\b.*\bwork\b。“.”这个元字符的含义是匹配除了换行符的任意字符，“*”元字符不是代表字符，而是代表数量，含义是“*”前面的内容可以连续重复任意次使得整个表达式被匹配。“.*”整体的意思就非常明显了，表示可以匹配任意数量不换行的字符，那么\bwe\b.*\bwork\b作用就是先匹配出we单词，接着再匹配任意的字符（非换行），直到匹配到work单词结束。通过上面的例子，我们看到元字符在正则表达式中非常关键，元字符的组合能构造出强大的功能。

接下来咱们开始讲解常用的元字符，在讲解之前，需要介绍一个正则表达式的测试工具Match Tracer，这个工具可以将写的正则表达式生成树状结构，描述并高亮每一部分的语法，同时可以检验正则表达式写的是否正确，如图4-23所示。

2.常用元字符

元字符主要有四种作用：有的用来匹配字符，有的用来匹配位置，有的用来匹配数量，有的用来匹配模式。在上面的例子中，我们讲到了“.”“*”这两个元字符，还有其他元字符，如表4-1所示。



图4-23　Match Tracer

表4-1　常见元字符



上面的元字符是用来匹配字符和位置的，接下来讲解其他功能时，会依次列出匹配数量和模式的元字符。下面对上面列出的元字符使用一些小例子来进行一下练习。

假如一行文本为：we are still studying and so busy，我们想匹配出所有以s开头的单词，那么正则表达式可以写为：\bs\w*\b。\bs\w*\b的匹配顺序：先是某个单词开始处（\b），然后是字母s，然后是任意数量的字母或数字（\w*），最后是单词结束处（\b）。同理，如果匹配s100这样的字符串（不是单词），需要用到“^”和“$”，一个匹配开头，一个匹配结束，可以写为^s\d*$。

3.字符转义

如果你想查找元字符本身的话，比如你查找“.”或者“*”就会出现问题，因为它们具有特定功能，没办法把它们指定为普通字符。这个时候就需要用到转义，使用“\”来取消这些字符的特殊意义。因此如果查找“.”、“\”或者“*”时，必须写成“\.”、“\\”和“\*”。例如匹配www.google.com这个网址时，可以表达式可以写为www\.google\.com。

4.重复

首先列举出匹配重复的限定符（指定数量的代码），如表4-2所示。

表4-2　常用限定符



下面是一些重复的例子：

·hello\d+：匹配hello后面跟1个或更多数字，例如可以匹配hello1、hello10等情况。

·^\d{5，12}$：匹配5到12个数字的字符串，例如QQ号符合要求。

·we\d：匹配we后面跟0个或者一个数字，例如we、we0符合情况。

5.字符集合

通过上面介绍的元字符，可以看到查找数字、字母或数字、空格是很简单的，因为已经有了对应这些字符的集合，但是如果想匹配没有预定义元字符的字符集合，例如匹配a、b、c、d和e中任意一个字符，这时候就需要自定义字符集合。正则表达式是通过[]来实现自定义字符集合，[abcde]就是匹配abcde中的任意一个字符，[.！]匹配标点符号（“.”、“”或“！”）。

除了将需要自定义的字符都写入[]中，还可以指定一个字符范围。[0-9]代表的含义与“\d”是完全一致的，代表一位数字；[a-z0-9A-Z_]也完全等同于“\w”（只考虑英文），代表着26个字母中的大小写、0～9的数字和下划线中的任一个字符。

6.分支条件

正则表达式里的分支条件指的是有几种匹配规则，如果满足其中任意一种规则都应该当成匹配，具体方法是用“|”把不同的规则分隔开。例如匹配电话号码，电话号码中一种是3位区号，8位本地号，形如010-11223344，另一种是4位区号，7位本地号，形如0321-1234567。如果想把电话号码匹配出来，就需要用到分支条件：0\d{2}-\d{8}|0\d{3}-\d{7}。在分支条件中有一点需要注意，匹配分支条件时，将会从左到右地测试每个条件，如果满足了某个分支的话，就不会去再管其他条件了，条件之间是一种或的关系，例如从1234567890匹配出连续的4个数字或者连续8个数字，如果写成\d{4}|\d{8}，其实\d{8}是失效的，既然能匹配出来8位数字，肯定就能匹配出4位数字。

7.分组





先以简单的IP地址匹配为例子，想匹配类似192.168.1.1这样的IP地址，可以这样写正则表达式（（\d{1，3}）\.）{3}\d{1，3}。下面分析一下这个正则表达式：\d{1，3}代表着1～3位的数字，（（\d{1，3}）\.）{3}代表着将1～3位数字加上一个“.”重复3次，匹配出类似192.168.1.这部分，之后再加上\d{1，3}，表示1～3位的数字。但是上述的正则表达式会匹配出类似333.444.555.666这些不可能存在的IP地址，因为IP地址中每个数字都不能大于255，所以要写出一个完整的IP地址匹配表达式，还需要关注一下细节，下面给出一个使用分组的完整IP表达式：（（25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]\d）\.）{3}（（25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]\d））。其中的关键是（25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]\d）部分，大家应该有能力分析出来。


8.反义

有时需要查找除某一类字符集合之外的字符。比如想查找除了数字以外，包含其他任意字符的情况，这时就需要用到反义，如表4-3所示。

表4-3　常用的反义



例如“\D+”匹配非数字的一个或者多个字符。

9.后向引用

前面我们讲到了分组，使用小括号指定一个表达式就可以看做是一个分组。默认情况下，每个分组会自动拥有一个组号，规则是：从左向右，以分组的左括号为标志，第一个出现的分组的组号为1，第二个为2，以此类推。还是以简单的IP匹配表达式（（\d{1，3}）\.）{3}\d{1，3}为例，这里面有两个分组1和2，使用Match Tracer这个工具可以很明显地看出来，如图4-24所示。



图4-24　捕获组

所以上面的表达式可以改写成（（\d{1，3}）\.）{3}\2。

你也可以自己指定子表达式的组名。要指定一个子表达式的组名，使用这样的语法：（<Digit>\d+）或者（‘Digit’\d+）），这样就把“\d+”的组名指定为Digit了。要反向引用这个分组捕获的内容，你可以使用\k<Digit>，所以上面的IP匹配表达式写成（（<Digit>\d{1，3}）\.）{3}\k<Digit>。使用小括号的地方很多，主要是用来分组，表4-4中列出了一些常用的形式。

表4-4　常用分组形式



在捕获这个表项里，我们讲解了前两种用法，还有（？：exp）没有进行讲解。（？：exp）不会改变正则表达式的处理方式，只是这样的组所匹配的内容不会像前两种那样被捕获到某个组里面，也不会拥有组号，这样做有什么意义？一般来说是为了节省资源，提高效率。比如说验证输入是否为整数，可以这样写^（[1-9][0-9]*|0）$。这时候我们需要用到“（）”来限制“|”表示“或”关系的范围，但我们只是要判断规则，没必要把exp匹配的内容保存到组里，这时就可以用非捕获组了^（：[1-9][0-9]*|0）$。

10.零宽断言

在表4-4中，零宽断言总共有四种形式。前两种是正向零宽断言，后两种是负向零宽断言。什么是零宽断言呢？我们知道元字符“\b”、“^”匹配的是一个位置，而且这个位置需要满足一定的条件，我们把这个条件称为断言或零宽度断言。断言用来声明一个应该为真的事实，正则表达式中只有当断言为真时才会继续进行匹配。可能大家感到有些抽象，下面通过一些例子进行讲解。

首先说一下正向零宽断言的两种形式：

·（？=exp）叫零宽度正预测先行断言，它断言此位置的后面能匹配表达式exp。比如[a-z]*（？=exp）匹配以ing结尾的单词的前面部分（除了ing以外的部分），查找I love cooking and singing时会匹配出中的cook与sing。先行断言的执行步骤应该是从要匹配字符的最右端找到第一个“ing”，再匹配前面的表达式，如无法匹配则查找第二个“ing”。

·（？=exp）叫零宽度正回顾后发断言，它断言此位置的前面能匹配表达式exp。比如（？<=abc）.*匹配以abc开头的字符串的后面部分，可以匹配abcdefgabc中的defgabc而不是abcdefg。通过比较很容易看出后发断言和先行断言正好相反：它先从要匹配的字符串的最左端开始查找断言表达式，之后再匹配后面的字符串，如果无法匹配则继续查找第二个断言表达式，如此反复。

再说一下负向零宽断言的两种形式：

·（？！exp）叫零宽度负预测先行断言，断言此位置的后面不能匹配表达式exp。比如\b（（？！abc）\w）+\b匹配不包含连续字符串abc的单词，查找“abc123，ade123”这个字符串，可以匹配出ade123，可以使用Match Tracer进行查看分析。

·（？！exp）叫零宽度负回顾后发断言，断言此位置的前面不能匹配表达式exp。比如（？<！[a-z]）\d{7}匹配前面不是小写字母的七位数字。还有一个复杂的例子：（？<=<（\w+）>）.*（？=<\/\1>），用于匹配不包含属性的简单HTML标记内的内容。该表达式可以从

python爬虫

中提取出“python爬虫”，这在Python爬虫开发中常用到。大家可以思考一下是如何提取出包含属性的HTML标记内的内容。11.注释

正则表达式可以包含注释进行解释说明，通过语法（#comment）来实现，例如\b\w+（#字符串）\b。要包含注释的话，最好是启用“忽略模式里的空白符”选项，这样在编写表达式时能任意地添加空格、Tab、换行，而实际使用时这些都将被忽略。

12.贪婪与懒惰

当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能多的字符，这就是贪婪模式。以表达式a\w+b为例，如果搜索a12b34b，会尽可能匹配更多的个数，最后就会匹配整个a12b34b，而不是a12b。但是如果想匹配出a12b怎么办呢？这时候就需要使用懒惰模式，尽可能匹配个数较少的情况，因此需要将上面的a\w+b表达式改为a\w+b，使用“”来启用懒惰模式。表4-5列举了懒惰限定符的使用方式。

表4-5　懒惰限定符的使用方式



13.处理选项

一般正则表达式的实现库都提供了用来改变正则表达式处理选项的方式，表4-6提供了常用的处理选项。

表4-6　常用的处理选项



正则表达式中还有平衡组/递归匹配的概念，对于初学者来说，一般用不了这么复杂，此处不进行讲解。





4.2.2　Python与正则


上一节讲解了正则表达式的语法和应用，对于不同的编程语言来说，对正则表达式的语法绝大部分语言都是支持的，但是还是略有不同，每种编程语言都有一些独特的匹配规则，Python也不例外。下面通过表4-7列出一些Python的匹配规则。

表4-7　Python的匹配规则



在讲Python对正则表达式的实现之前，首先让说一下反斜杠问题。正则表达式里使用“\”作为转义字符，这就可能造成反斜杠困扰。假如你需要匹配文本中的字符“\”，那么使用编程语言表示的正则表达式里将需要4个反斜杠“\\\\”：前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。但是Python提供了对原生字符串的支持，从而解决了这个问题。匹配一个‘\’的正则表达式可以写为r‘\\’，同样，匹配一个数字的‘\\d’可以写成r‘\d’，

Python通过re模块提供对正则表达式的支持。使用re的一般步骤是先将正则表达式的字符串形式编译为Pattern实例，然后使用Pattern实例处理文本并获得匹配结果，最后使用Match实例获得信息，进行其他操作。主要用到的方法列举如下：

·re.compile（string[，flag]）

·re.match（pattern，string[，flags]）

·re.search（pattern，string[，flags]）

·re.split（pattern，string[，maxsplit]）

·re.findall（pattern，string[，flags]）

·re.finditer（pattern，string[，flags]）

·re.sub（pattern，repl，string[，count]）

·re.subn（pattern，repl，string[，count]）

首先说一下re中compile函数，它将一个正则表达式的字符串转化为Pattern匹配对象。示例如下：



* * *



pattern = re.compile(r'\d+')



* * *



这会生成一个匹配数字的pattern对象，用来给接下来的函数作为参数，进行进一步的搜索操作。

大家发现其他几个函数中，还有一个flag参数。参数flag是匹配模式，取值可以使用按位或运算符“|”表示同时生效，比如re.I|re.M。flag的可选值如下：

·re.I：忽略大小写。

·re.M：多行模式，改变“^”和“$”的行为。

·re.S：点任意匹配模式，改变“.”的行为。

·re.L：使预定字符类\w\W\b\B\s\S取决于当前区域设定。

·re.U：使预定字符类\w\W\b\B\s\S\d\D取决于unicode定义的字符属性。

·re.X：详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。

1.re.match（pattern，string[，flags]）

这个函数是从输入参数string（匹配的字符串）的开头开始，尝试匹配pattern，一直向后匹配，如果遇到无法匹配的字符或者已经到达string的末尾，立即返回None，反之获取匹配的结果。示例如下：



* * *



# coding:utf-8 import re # 将正则表达式编译成pattern对象 pattern = re.compile(r'\d+') # 使用re.match匹配文本，获得匹配结果，无法匹配时将返回None result1 = re.match(pattern,'192abc') if result1: print result1.group() else: print '匹配失败1' result2 = re.match(pattern,'abc192') if result2: print result2.group() else: print '匹配失败2'



* * *



运行结果如下：



* * *



192 匹配失败2



* * *



匹配192abc字符串时，match函数是从字符串开头进行匹配，匹配到192立即返回值，通过group（）可以获取捕获的值。同样，匹配abc192字符串时，字符串开头不符合正则表达式，立即返回None。

2.re.search（pattern，string[，flags]）

search方法与match方法极其类似，区别在于match（）函数只从string的开始位置匹配，search（）会扫描整个string查找匹配，match（）只有在string起始位置匹配成功的时候才有返回，如果不是开始位置匹配成功的话，match（）就返回None。search方法的返回对象和match（）返回对象在方法和属性上是一致的。示例如下：



* * *



import re # 将正则表达式编译成pattern对象 pattern = re.compile(r'\d+') # 使用re.match匹配文本获得匹配结果；无法匹配时将返回None result1 = re.search(pattern,'abc192edf') if result1: print result1.group() else: print '匹配失败1'



* * *



输出结果为：



* * *



192



* * *



3. re.split(pattern, string[, maxsplit])



按照能够匹配的子串将string分割后返回列表。maxsplit用于指定最大分割次数，不指定，则将全部分割。示例如下：



* * *



import re pattern = re.compile(r'\d+') print re.split(pattern,'A1B2C3D4')



* * *



输出结果为：



* * *



['A', 'B', 'C', 'D', '']



* * *



4.re.findall（pattern，string[，flags]）

搜索整个string，以列表形式返回能匹配的全部子串。示例如下：



* * *



import re pattern = re.compile(r'\d+') print re.findall(pattern,'A1B2C3D4')



* * *



输出结果为：



* * *



['1', '2', '3', '4']



* * *



5.re.finditer（pattern，string[，flags]）

搜索整个string，以迭代器形式返回能匹配的全部Match对象。示例如下：



* * *



import re pattern = re.compile(r'\d+') matchiter = re.finditer(pattern,'A1B2C3D4') for match in matchiter: print match.group()



* * *



输出结果为：



* * *



1 2 3 4



* * *



6.re.sub（pattern，repl，string[，count]）

使用repl替换string中每一个匹配的子串后返回替换后的字符串。当repl是一个字符串时，可以使用\id或\g<id>、\g<name>引用分组，但不能使用编号0。当repl是一个方法时，这个方法应当只接受一个参数（Match对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。count用于指定最多替换次数，不指定时全部替换。示例如下：



* * *



import re p = re.compile(r'(P<word1>\w+) (P<word2>\w+)')# 使用名称引用 s = 'i say, hello world!' print p.sub(r'\g<word2> \g<word1>', s) p = re.compile(r'(\w+) (\w+)')# 使用编号 print p.sub(r'\2 \1', s) def func(m): return m.group(1).title() + ' ' + m.group(2).title() print p.sub(func, s)



* * *





输出结果为：




* * *



say i, world hello! say i, world hello! I Say, Hello World!



* * *



7.re.subn（pattern，repl，string[，count]）

返回（sub（repl，string[，count]），替换次数）。示例如下：



* * *



import re s = 'i say, hello world!' p = re.compile(r'(\w+) (\w+)') print p.subn(r'\2 \1', s) def func(m): return m.group(1).title() + ' ' + m.group(2).title() print p.subn(func, s)



* * *



输出结果为：



* * *



('say i, world hello!', 2) ('I Say, Hello World!', 2)



* * *



以上7个函数在re模块中进行搜索匹配，如何将捕获到的值提取出来呢？这就需要用到Match对象，之前已经使用了Match中的groups方法，现在介绍一下Match对象的属性和方法。

Match对象的属性：

·string：匹配时使用的文本。

·re：匹配时使用的Pattern对象。

·pos：文本中正则表达式开始搜索的索引。值与Pattern.match（）和Pattern.search（）方法的同名参数相同。

·endpos：文本中正则表达式结束搜索的索引。值与Pattern.match（）和Pattern.search（）方法的同名参数相同。

·lastindex：最后一个被捕获的分组在文本中的索引。如果没有被捕获的分组，将为None。

·lastgroup：最后一个被捕获的分组的别名。如果这个分组没有别名或者没有被捕获的分组，将为None。

·Match对象的方法：

·group（[group1，…]）：获得一个或多个分组截获的字符串，指定多个参数时将以元组形式返回。group1可以使用编号也可以使用别名，编号0代表整个匹配的子串，不填写参数时，返回group（0）。没有截获字符串的组返回None，截获了多次的组返回最后一次截获的子串。

·groups（[default]）：以元组形式返回全部分组截获的字符串。相当于调用group（1，2，…last）。default表示没有截获字符串的组以这个值替代，默认为None。

·groupdict（[default]）：返回以有别名的组的别名为键、以该组截获的子串为值的字典，没有别名的组不包含在内。default含义同上。

·start（[group]）：返回指定的组截获的子串在string中的起始索引（子串第一个字符的索引）。group默认值为0。

·end（[group]）：返回指定的组截获的子串在string中的结束索引（子串最后一个字符的索引+1）。group默认值为0。

·span（[group]）：返回（start（group），end（group））。

·expand（template）：将匹配到的分组代入template中然后返回。template中可以使用\id或\g<id>、\g<name>引用分组，但不能使用编号0。\id与\g<id>是等价的，但\10将被认为是第10个分组，如果你想表达\1之后是字符‘0’，只能使用\g<1>0。

示例如下：



* * *



import re pattern = re.compile(r'(\w+) (\w+) (P<word>.*)') match = pattern.match( 'I love you!') print "match.string:", match.string print "match.re:", match.re print "match.pos:", match.pos print "match.endpos:", match.endpos print "match.lastindex:", match.lastindex print "match.lastgroup:", match.lastgroup print "match.group(1,2):", match.group(1, 2) print "match.groups():", match.groups() print "match.groupdict():", match.groupdict() print "match.start(2):", match.start(2) print "match.end(2):", match.end(2) print "match.span(2):", match.span(2) print r"match.expand(r'\2 \1 \3'):", match.expand(r'\2 \1 \3')



* * *



输出结果：



* * *



match.string: I love you! match.re: <_sre.SRE_Pattern object at 0x003F47A0> match.pos: 0 match.endpos: 11 match.lastindex: 3 match.lastgroup: word match.group(1,2): ('I', 'love') match.groups(): ('I', 'love', 'you!') match.groupdict(): {'word': 'you!'} match.start(2): 2 match.end(2): 6 match.span(2): (2, 6) match.expand(r'\2 \1 \3'): love I you!



* * *



前文介绍的7种方法的调用方式大都是re.match、re.search之类，其实还可以使用由re.compile方法产生的Pattern对象直接调用这些函数，类似pattern.match，pattern.search，只不过不用将Pattern作为第一个参数传入。函数对比如表4-8所示。

表4-8　函数调用方式





4.3　强大的BeautifulSoup


Beautiful Soup是一个可以从HTML或XML文件中提取数据的Python库。它能够通过你喜欢的转换器实现惯用的文档导航、查找、修改文档的方式。在Python爬虫开发中，我们主要用到的是Beautiful Soup的查找提取功能，修改文档的方式很少用到。接下来由浅及深介绍Beautiful Soup在Python爬虫开发中的使用。





4.3.1　安装BeautifulSoup


对于Beautiful Soup，我们推荐使用的是Beautiful Soup 4，已经移植到BS4中，Beautiful Soup 3已经停止开发。安装Beautiful Soup 4有三种方式：

·如果你用的是新版的Debain或ubuntu，那么可以通过系统的软件包管理来安装：apt-get install Python-bs4。

·Beautiful Soup 4通过PyPi发布，可以通过easy_install或pip来安装。包的名字是beautifulsoup4，这个包兼容Python2和Python3。安装命令：easy_installbeautifulsoup4或者pipinstallbeautifulsoup4。

·也可以通过下载源码的方式进行安装，当前最新的版本是4.5.1，源码下载地址为https://pypi.python.org/pypi/beautifulsoup4/ 。运行下面的命令即可完成安装：python setup.py install。

Beautiful Soup支持Python标准库中的HTML解析器，还支持一些第三方的解析器，其中一个是lxml。由于lxml解析速度比标准库中的HTML解析器的速度快得多，我们选择安装lxml作为新的解析器。根据操作系统不同，可以选择下列方法来安装lxml：

·apt-get install Python-lxml

·easy_install lxml

·pip install lxml

另一个可供选择的解析器是纯Python实现的html5lib，html5lib的解析方式与浏览器相同，可以选择下列方法来安装html5lib：

·apt-get install Python-html5lib

·easy_install html5lib

·pip install html5lib

表4-9列出了主要的解析器，以及它们的优缺点。

表4-9　解析器比较



从表4-9中可以看出推荐使用lxml作为解析器的原因，因为它效率更高。





4.3.2　BeautifulSoup的使用


安装完BeautifulSoup，接下来开始讲解BeautifulSoup的使用。

1.快速开始

首先导入bs4库：from bs4import BeautifulSoup。接着创建包含HTML代码的字符串，用来进行解析。字符串如下：



* * *



html_str = """ <html><head><title>The Dormouse's story</title></head> <body> <p class="title"><b>The Dormouse's story</b></p> <p class="story">Once upon a time there were three little sisters; and their names were <a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>, <a href="http://example.com/lacie" class="sister" id="link2"><!-- Lacie --></a> and <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>; and they lived at the bottom of a well.</p> <p class="story">...</p> """



* * *



接下来的数据解析和提取都是以这个字符串为例子。

然后创建BeautifulSoup对象，创建BeautifulSoup对象有两种方式。一种直接通过字符串创建：



* * *



soup = BeautifulSoup(html_str,'lxml', from_encoding='utf-8')



* * *



另一种通过文件来创建，假如将html_str字符串保存为index.html文件，创建方式如下：



* * *



soup = BeautifulSoup(open('index.html'))



* * *



文档被转换成Unicode，并且HTML的实例都被转换成Unicode编码。打印soup对象的内容，格式化输出：



* * *



print soup.prettify()



* * *



输入结果如下：



* * *



<html> <head> <title> The Dormouse's story </title> </head> <body> <p class="title"> <b> The Dormouse's story </b> </p> <p class="story"> Once upon a time there were three little sisters; and their names were <a class="sister" href="http://example.com/elsie" id="link1"> <!--Elsie --> </a> , <a class="sister" href="http://example.com/lacie" id="link2"> <!--Lacie--> </a> and <a class="sister" href="http://example.com/tillie" id="link3"> Tillie </a> ; and they lived at the bottom of a well. </p> <p class="story"> ... </p> </body> </html>



* * *



Beautiful Soup选择最合适的解析器来解析这段文档，如果手动指定解析器那么Beautiful Soup会选择指定的解析器来解析文档，使用方法如表4-9所示。

2.对象种类

Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构，每个节点都是Python对象，所有对象可以归纳为4种：

·Tag

·NavigableString

·BeautifulSoup

·Comment

1）Tag

首先说一下Tag对象，Tag对象与XML或HTML原生文档中的Tag相同，通俗点说就是标记。比如<title>The Dormouse‘s story</title>或者<a href=“http://example.com/elsie”class=“sister”id=“link1”>Elsie</a>，title和a标记及其里面的内容称为Tag对象。怎样从html_str中抽取Tag呢？示例如下：

·抽取title：print soup.title

·抽取a：print soup.a

·抽取p：print soup.a

从例子中可以看到利用soup加标记名就可以获取这些标记的内容，比之前讲的正则表达式简单多了。不过利用这种方式，查找的是在所有内容中第一个符合要求的标记，如果要查询所有的标记，后面的内容进行讲解。

Tag中有两个最重要的属性：name和attributes。每个Tag都有自己的名字，通过.name来获取。示例如下：



* * *



print soup.name print soup.title.name



* * *



输出结果：



* * *



[document] title



* * *



soup对象本身比较特殊，它的name为[document]，对于其他内部标记，输出的值便为标记本身的名称。





Tag不仅可以获取name，还可以修改name，改变之后将影响所有通过当前Beautiful Soup对象生成的HTML文档。示例如下：




* * *



soup.title.name = 'mytitle' print soup.title print soup.mytitle



* * *



输出结果：



* * *



None <mytitle>The Dormouse's story</mytitle>



* * *



这里已经将title标记成功修改为mytitle。

再说一下Tag中的属性，<p class=“title”><b>The Dormouse’s story</b></p>有一个“class”属性，值为“title”。Tag的属性的操作方法与字典相同：



* * *



print soup.p['class'] print soup.p.get('class')



* * *



输出结果：



* * *



['title'] ['title']



* * *



也可以直接“点”取属性，比如：.attrs，用于获取Tag中所有属性：



* * *



print soup.p.attrs



* * *



输出结果：



* * *



{'class': ['title']}



* * *



和name一样，我们可以对标记中的这些属性和内容等进行修改，示例如下：



* * *



soup.p['class']="myClass" print soup.p



* * *



输出结果：



* * *



<p class="myClass"><b>The Dormouse's story</b></p>



* * *



2）NavigableString

我们已经得到了标记的内容，要想获取标记内部的文字怎么办呢？需要用到.string。

示例如下：



* * *



print soup.p.string print type(soup.p.string)



* * *



输出结果：



* * *



The Dormouse's story <class 'bs4.element.NavigableString'>



* * *



Beautiful Soup用NavigableString类来包装Tag中的字符串，一个NavigableString字符串与Python中的Unicode字符串相同，通过unicode（）方法可以直接将NavigableString对象转换成Unicode字符串：



* * *



unicode_string = unicode(soup.p.string)



* * *



3）BeautifulSoup

BeautifulSoup对象表示的是一个文档的全部内容。大部分时候，可以把它当作Tag对象，是一个特殊的Tag，因为BeautifulSoup对象并不是真正的HTML或XML的标记，所以它没有name和attribute属性。但为了将BeautifulSoup对象标准化为Tag对象，实现接口的统一，我们依然可以分别获取它的name和attribute属性。示例如下：



* * *



print type(soup.name) print soup.name print soup.attrs



* * *



输出结果：



* * *



<type 'unicode'> [document] {}



* * *



4）Comment

Tag、NavigableString、BeautifulSoup几乎覆盖了HTML和XML中的所有内容，但是还有一些特殊对象。容易让人担心的内容是文档的注释部分：



* * *



print soup.a.string print type(soup.a.string)



* * *



输出结果：



* * *



Elsie <class 'bs4.element.Comment'>



* * *



a标记里的内容实际上是注释，但是如果我们利用.string来输出它的内容，会发现它已经把注释符号去掉了。另外如果打印输出它的类型，会发现它是一个Comment类型。如果在我们不清楚这个标记.string的情况下，可能造成数据提取混乱。因此在提取字符串时，可以判断一下类型：



* * *



if type(soup.a.string)==bs4.element.Comment: print soup.a.string



* * *



3.遍历文档树

BeautifulSoup会将HTML转化为文档树进行搜索，既然是树形结构，节点的概念必不可少。

1）子节点

首先说一下直接子节点，Tag中的.contents和.children是非常重要的。Tag的.content属性可以将Tag子节点以列表的方式输出：



* * *



print soup.head.contents



* * *



输出结果：



* * *



[<title>The Dormouse's story</title>]



* * *



既然输出方式是列表，我们就可以获取列表的大小，并通过列表索引获取里面的值：



* * *



print len(soup.head.contents) print soup.head.contents[0].string



* * *



输出结果：



* * *



1 The Dormouse's story



* * *



有一点需要注意：字符串没有.contents属性，因为字符串没有子节点。

.children属性返回的是一个生成器，可以对Tag的子节点进行循环：



* * *



for child in soup.head.children: print(child)



* * *



输出结果：



* * *



<title>The Dormouse's story</title>



* * *



.contents和.children属性仅包含Tag的直接子节点。例如，<head>标记只有一个直接子节点<title>。但是<title>标记也包含一个子节点：字符串“The Dormouse’s story”，这种情况下字符串“The Dormouse’s story”也属于<head>标记的子孙节点。.descendants属性可以对所有tag的子孙节点进行递归循环：



* * *



for child in soup.head.descendants: print(child)



* * *



输出结果：



* * *



<title>The Dormouse's story</title> The Dormouse's story



* * *



以上都是关于如何获取子节点，接下来说一下如何获取节点的内容，这就涉及.string、.strings、stripped_strings三个属性。

.string这个属性很有特点：如果一个标记里面没有标记了，那么.string就会返回标记里面的内容。如果标记里面只有唯一的一个标记了，那么.string也会返回最里面的内容。如果tag包含了多个子节点，tag就无法确定，string方法应该调用哪个子节点的内容，.string的输出结果是None。示例如下：



* * *



print soup.head.string print soup.title.string print soup.html.string



* * *



输出结果：



* * *



The Dormouse's story The Dormouse's story None



* * *



.strings属性主要应用于tag中包含多个字符串的情况，可以进行循环遍历，示例如下：



* * *



for string in soup.strings: print(repr(string))



* * *



输出结果：



* * *





u"The Dormouse's story" u'\n' u'\n' u"The Dormouse's story" u'\n' u'Once upon a time there were three little sisters; and their names were\n' u',\n' u' and\n' u'Tillie' u';\nand they lived at the bottom of a well.' u'\n' u'...' u'\n'




* * *



.stripped_strings属性可以去掉输出字符串中包含的空格或空行，示例如下：



* * *



for string in soup.stripped_strings: print(repr(string))



* * *



输出结果：



* * *



u"The Dormouse's story" u"The Dormouse's story" u'Once upon a time there were three little sisters; and their names were' u',' u'and' u'Tillie' u';\nand they lived at the bottom of a well.' u'...'



* * *



2）父节点

继续分析文档树，每个Tag或字符串都有父节点：被包含在某个Tag中。

通过.parent属性来获取某个元素的父节点。在html_str中，<head>标记是<title>标记的父节点：



* * *



print soup.title print soup.title.parent



* * *



输出结果：



* * *



<title>The Dormouse's story</title> <head><title>The Dormouse's story</title></head>



* * *



通过元素的.parents属性可以递归得到元素的所有父辈节点，下面的例子使用了.parents方法遍历了<a>标记到根节点的所有节点：



* * *



print soup.a for parent in soup.a.parents: if parent is None: print(parent) else: print(parent.name)



* * *



输出结果：



* * *



<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a> p body html [document]



* * *



3）兄弟节点

从soup.prettify（）的输出结果中，我们可以看到<a>有很多兄弟节点。兄弟节点可以理解为和本节点处在同一级的节点，.next_sibling属性可以获取该节点的下一个兄弟节点，.previous_sibling则与之相反，如果节点不存在，则返回None。示例如下：



* * *



print soup.p.next_sibling print soup.p.prev_sibling print soup.p.next_sibling.next_sibling



* * *



输出结果：



* * *



None <p class="story">Once upon a time there were three little sisters; and their names were <a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>, <a class="sister" href="http://example.com/lacie" id="link2"><!-- Lacie --></a> and <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>; and they lived at the bottom of a well.</p>



* * *



第一个输出结果为空白，因为空白或者换行也可以被视作一个节点，所以得到的结果可能是空白或者换行。

通过.next_siblings和.previous_siblings属性可以对当前节点的兄弟节点迭代输出：



* * *



for sibling in soup.a.next_siblings: print(repr(sibling))



* * *



输出结果：



* * *



u',\n' <a class="sister" href="http://example.com/lacie" id="link2"><!-- Lacie --></a> u' and\n' <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a> u';\nand they lived at the bottom of a well.'



* * *



4）前后节点

前后节点需要使用.next_element、.previous_element这两个属性，与.next_sibling.previous_sibling不同，它并不是针对于兄弟节点，而是针对所有节点，不分层次，例如<head><title>The Dormouse‘s story</title></head>中的下一个节点就是title：



* * *



print soup.head print soup.head.next_element



* * *



输出结果：



* * *



<head><title>The Dormouse's story</title></head> <title>The Dormouse's story</title>



* * *



如果想遍历所有的前节点或者后节点，通过.next_elements和.previous_elements的迭代器就可以向前或向后访问文档的解析内容，就好像文档正在被解析一样：



* * *



for element in soup.a.next_elements: print(repr(element))



* * *



输出结果：



* * *



u' Elsie ' u',\n' <a class="sister" href="http://example.com/lacie" id="link2"><!-- Lacie --></a> u' Lacie ' u' and\n' <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a> u'Tillie' u';\nand they lived at the bottom of a well.' u'\n' <p class="story">...</p> u'...' u'\n'



* * *



以上就是遍历文档树的用法，接下来开始讲解比较核心的内容：搜索文档树。

4.搜索文档树

Beautiful Soup定义了很多搜索方法，这里着重介绍find_all（）方法，其他方法的参数和用法类似，请大家举一反三。

首先看一下find_all方法，用于搜索当前Tag的所有Tag子节点，并判断是否符合过滤器的条件，函数原型如下：



* * *



find_all( name , attrs , recursive , text , **kwargs )



* * *



接下来分析函数中各个参数，不过需要打乱函数参数顺序，这样方便例子的讲解。

1）name参数

name参数可以查找所有名字为name的标记，字符串对象会被自动忽略掉。name参数取值可以是字符串、正则表达式、列表、True和方法。

最简单的过滤器是字符串。在搜索方法中传入一个字符串参数，Beautiful Soup会查找与字符串完整匹配的内容，下面的例子用于查找文档中所有的<b>标记，返回值为列表：



* * *



print soup.find_all('b')



* * *



输出结果：



* * *



[<b>The Dormouse's story</b>]



* * *



如果传入正则表达式作为参数，Beautiful Soup会通过正则表达式的match（）来匹配内容。下面的例子中找出所有以b开头的标记，这表示<body>和<b>标记都应该被找到：



* * *



import re for tag in soup.find_all(re.compile("^b")): print(tag.name)



* * *



输出结果：



* * *



body b



* * *



如果传入列表参数，Beautiful Soup会将与列表中任一元素匹配的内容返回。下面的代码找到文档中所有<a>标记和<b>标记：



* * *



print soup.find_all(["a", "b"])



* * *



输出结果：



* * *





[<b>The Dormouse's story</b>, <a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>, <a class="sister" href="http://example.com/lacie" id="link2"><!-- Lacie --></a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]




* * *



如果传入的参数是True，True可以匹配任何值，下面代码查找到所有的tag，但是不会返回字符串节点：



* * *



for tag in soup.find_all(True): print(tag.name)



* * *



输出结果：



* * *



html head title body p b p a a a p



* * *



如果没有合适过滤器，那么还可以定义一个方法，方法只接受一个元素参数Tag节点，如果这个方法返回True表示当前元素匹配并且被找到，如果不是则返回False。比如过滤包含class属性，也包含id属性的元素，程序如下：



* * *



def hasClass_Id(tag): return tag.has_attr('class') and tag.has_attr('id') print soup.find_all(hasClass_Id)



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>, <a class="sister" href="http://example.com/lacie" id="link2"><!-- Lacie --></a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]



* * *



2）kwargs参数

kwargs参数在Python中表示为keyword参数。如果一个指定名字的参数不是搜索内置的参数名，搜索时会把该参数当作指定名字Tag的属性来搜索。搜索指定名字的属性时可以使用的参数值包括字符串、正则表达式、列表、True。

如果包含id参数，Beautiful Soup会搜索每个tag的“id”属性。示例如下：



* * *



print soup.find_all(id='link2')



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/lacie" id="link2"><!-- Lacie --></a>]



* * *



如果传入href参数，Beautiful Soup会搜索每个Tag的“href”属性。比如查找href属性中含有“elsie”的tag：



* * *



import re print soup.find_all(href=re.compile("elsie"))



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>]



* * *



下面的代码在文档树中查找所有包含id属性的Tag，无论id的值是什么：



* * *



print soup.find_all(id=True)



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>, <a class="sister" href="http://example.com/lacie" id="link2"><!-- Lacie --></a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]



* * *



如果我们想用class过滤，但是class是python的关键字，需要在class后面加个下划线：



* * *



print soup.find_all("a", class_="sister")



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>, <a class="sister" href="http://example.com/lacie" id="link2"><!-- Lacie --></a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]



* * *



使用多个指定名字的参数可以同时过滤tag的多个属性：



* * *



print soup.find_all(href=re.compile("elsie"), id='link1')



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>]



* * *



有些tag属性在搜索不能使用，比如HTML5中的data-*属性：



* * *



data_soup = BeautifulSoup('<div data-foo="value">foo!</div>') data_soup.find_all(data-foo="value")



* * *



这样的代码在Python中是不合法的，但是可以通过find_all（）方法的attrs参数定义一个字典参数来搜索包含特殊属性的tag，示例代码如下：



* * *



data_soup = BeautifulSoup('<div data-foo="value">foo!</div>') data_soup.find_all(attrs={"data-foo": "value"})



* * *



输出结果：



* * *



[<div data-foo="value">foo!</div>]



* * *



3）text参数

通过text参数可以搜索文档中的字符串内容。与name参数的可选值一样，text参数接受字符串、正则表达式、列表、True。示例如下：



* * *



print soup.find_all(text="Elsie") print soup.find_all(text=["Tillie", "Elsie", "Lacie"]) print soup.find_all(text=re.compile("Dormouse"))



* * *



输出结果：



* * *



[u'Elsie'] [u'Elsie', u'Lacie', u'Tillie'] [u"The Dormouse's story", u"The Dormouse's story"]



* * *



虽然text参数用于搜索字符串，还可以与其他参数混合使用来过滤tag。Beautiful Soup会找到.string方法与text参数值相符的tag。下面的代码用来搜索内容里面包含“Elsie”的<a>标记：



* * *



print soup.find_all("a", text="Elsie")



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/elsie" id="link1"><!--Elsie--></a>]



* * *



4）limit参数

find_all（）方法返回全部的搜索结构，如果文档树很大那么搜索会很慢。如果我们不需要全部结果，可以使用limit参数限制返回结果的数量。效果与SQL中的limit关键字类似，当搜索到的结果数量达到limit的限制时，就停止搜索返回结果。下面的例子中，文档树中有3个tag符合搜索条件，但结果只返回了2个，因为我们限制了返回数量。



* * *



print soup.find_all("a", limit=2)



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/elsie" id="link1"><!--Elsie--></a>, <a class="sister" href="http://example.com/lacie" id="link2"><!--Lacie--></a>]



* * *



5）recursive参数

调用tag的find_all（）方法时，Beautiful Soup会检索当前tag的所有子孙节点，如果只想搜索tag的直接子节点，可以使用参数recursive=False。示例如下：



* * *



print soup.find_all("title") print soup.find_all("title", recursive=False)



* * *



输出结果：



* * *



[<title>The Dormouse's story</title>] []



* * *





以上将find_all函数的各个参数基本上讲解完毕，其他函数的使用方法和这个类似，表4-10列举了其他函数。


表4-10　搜索函数





5.CSS选择器

在之前Web前端的章节中，我们讲到了CSS的语法，通过CSS也可以定位元素的位置。在写CSS时，标记名不加任何修饰，类名前加点“.”，id名前加“#”，在这里我们也可以利用类似的方法来筛选元素，用到的方法是soup.select（），返回类型是list。

1）通过标记名称进行查找

通过标记名称可以直接查找、逐层查找，也可以找到某个标记下的直接子标记和兄弟节点标记。示例如下：



* * *



# 直接查找title标记 print soup.select("title") # 逐层查找title标记 print soup.select("html head title") # 查找直接子节点 # 查找head下的title标记 print soup.select("head > title") # 查找p下的id="link1"的标记 print soup.select("p > # link1") # 查找兄弟节点 # 查找id="link1"之后class=sisiter的所有兄弟标记 print soup.select("# link1 ~ .sister") # 查找紧跟着id="link1"之后class=sisiter的子标记 print soup.select("# link1 + .sister")



* * *



输出结果：



* * *



[<title>The Dormouse's story</title>] [<title>The Dormouse's story</title>] [<title>The Dormouse's story</title>] [<a class="sister" href="http://example.com/elsie" id="link1"><!--Elsie--></a>] [<a class="sister" href="http://example.com/lacie" id="link2"><!--Lacie--></a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>] [<a class="sister" href="http://example.com/lacie" id="link2"><!--Lacie--></a>]



* * *



2）通过CSS的类名查找

示例如下：



* * *



print soup.select(".sister") print soup.select("[class~=sister]")



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/elsie" id="link1"><!--Elsie--></a>, <a class="sister" href="http://example.com/lacie" id="link2"><!--Lacie--></a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>] [<a class="sister" href="http://example.com/elsie" id="link1"><!--Elsie--></a>, <a class="sister" href="http://example.com/lacie" id="link2"><!--Lacie--></a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]



* * *



3）通过tag的id查找

示例如下：



* * *



print soup.select("# link1") print soup.select("a# link2")



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/elsie" id="link1"><!--Elsie--></a>] [<a class="sister" href="http://example.com/lacie" id="link2"><!--Lacie--></a>]



* * *



4）通过是否存在某个属性来查找

示例如下：



* * *



print soup.select('a[href]')



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/elsie" id="link1"><!--Elsie--></a>, <a class="sister" href="http://example.com/lacie" id="link2"><!--Lacie--></a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]



* * *



5）通过属性值来查找

示例如下：



* * *



print soup.select('a[href="http://example.com/elsie"]') print soup.select('a[href^="http://example.com/"]') print soup.select('a[href$="tillie"]') print soup.select('a[href*=".com/el"]')



* * *



输出结果：



* * *



[<a class="sister" href="http://example.com/elsie" id="link1"><!--Elsie--></a>] [<a class="sister" href="http://example.com/elsie" id="link1"><!--Elsie--></a>, <a class="sister" href="http://example.com/lacie" id="link2"><!--Lacie--></a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>] [<a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>] [<a class="sister" href="http://example.com/elsie" id="link1"><!--Elsie--></a>]



* * *



以上就是CSS选择器的查找方式，如果大家对CSS选择器的写法不是很熟悉，可以搜索一下W3CSchool的CSS选择器参考手册进行学习。除此之外，还可以使用Firebug中的FirePath功能自动获取网页元素的CSS选择器表达式，如图4-25所示。



图4-25　FirePath CSS选择器





4.3.3　lxml的XPath解析


BeautifulSoup可以将lxml作为默认的解析器使用，同样lxml可以单独使用。下面比较一下这两者之间的优缺点：

·BeautifulSoup和lxml的原理不一样，BeautifulSoup是基于DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多。而lxml是使用XPath技术查询和处理HTML/XML文档的库，只会局部遍历，所以速度会快一些。幸好现在BeautifulSoup可以使用lxml作为默认解析库。

·BeautifulSoup用起来比较简单，API非常人性化，支持CSS选择器，适合新手。lxml的XPath写起来麻烦，开发效率不如BeautifulSoup，当然这也是因人而异，如果你能熟练使用XPath，那么使用lxml是更好的选择，况且现在又有了FirePath这样的自动生成XPath表达式的利器。

第2章已经讲过了XPath的用法，所以现在直接介绍如何使用lxml库来解析网页。示例如下：



* * *



from lxml import etree html_str = """ <html><head><title>The Dormouse's story</title></head> <body> <p class="title"><b>The Dormouse's story</b></p> <p class="story">Once upon a time there were three little sisters; and their names were <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>, <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>; and they lived at the bottom of a well.</p> <p class="story">...</p> """ html = etree.HTML(html_str) result = etree.tostring(html) print(result)



* * *



输出结果：



* * *





<html><head><title>The Dormouse's story</title></head> <body> <p class="title"><b>The Dormouse's story</b></p> <p class="story">Once upon a time there were three little sisters; and their names were <a href="http://example.com/elsie" class="sister" id="link1"><!--Elsie--></a>, <a href="http://example.com/lacie" class="sister" id="link2"><!--Lacie--></a> and <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>; and they lived at the bottom of a well.</p> <p class="story">...</p> </body></html>




* * *



大家看到html_str最后是没有</html>和</body>标签的，没有进行闭合，但是通过输出结果我们可以看到lxml的一个非常实用的功能就是自动修正html代码。

除了读取字符串之外，lxml还可以直接读取html文件。假如将html_str存储为index.html文件，利用parse方法进行解析，示例如下：



* * *



from lxml import etree html = etree.parse('index.html') result = etree.tostring(html, pretty_print=True) print(result)



* * *



接下来使用XPath语法抽取出其中所有的URL，示例如下：



* * *



html = etree.HTML(html_str) urls = html.xpath(".// *[@class='sister']/@href") print urls



* * *



输出结果：



* * *



['http://example.com/elsie', 'http://example.com/lacie', 'http://example.com/tillie']



* * *



使用lxml的关键是构造XPath表达式，如果大家对XPath不熟悉，可以复习一下第2章中XPath内容。





4.4　小结


本章主要讲解了HTML解析的各种方式，这也是提取网页数据非常关键的环节。希望大家把正则表达式、Beautiful Soup和XPath的知识做到灵活运用。同时还要注意Firebug、FirePath和Match Tracer的配合使用，将会使开发达到事半功倍的效果。





第5章　数据存储（无数据库版）


本章主要讲解数据存储中非数据库版的部分，大体分为两块内容，一块是将取出的HTML文本内容进行存储，一块是多媒体文件的存储。本章同时也讲解了Email的发送，这在Python爬虫出现异常时有很好的作用。





5.1　HTML正文抽取


本小节讲解的是对HTML正文的抽取存储，主要是将HTML正文存储为两种格式：JSON和CSV。以一个盗墓笔记的小说阅读网（http://seputu.com/ ）为例，抽取出盗墓笔记的标题、章节、章节名称和链接，如图5-1所示。

首先有一点需要说明，这是一个静态网站，标题、章节、章节名称都不是由JavaScript动态加载的，这是下面所进行的工作的前提。

这个例子使用第4章介绍的Beautiful Soup和lxml两种方式进行解析抽取，力求将之前的知识进行灵活运用。5.1.1小节，使用Beautiful Soup解析，5.1.2小节使用lxml方式解析。





5.1.1　存储为JSON


首先使用Requests访问http://seputu.com/ ，获取HTML文档内容，并打印文档内容。代码如下：



* * *



import requests user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers={'User-Agent':user_agent} r = requests.get('http://seputu.com/',headers=headers) print r.text



* * *





图5-1　盗墓笔记小说网

接着分析http://seputu.com/ 首页的HTML结构，确定要抽取标记的位置，分析如下：

标题和章节都被包含在<div class=“mulu”>标记下，标题位于其中的<div class=“mulu-title”>下的<h2>中，章节位于其中的<div class=“box”>下的<a>中，如图5-2所示。



图5-2　HTML结构分析

分析完成就可以进行编码了，代码如下：



* * *



soup = BeautifulSoup(r.text,'html.parser',from_encoding='utf-8')# html.parser for mulu in soup.find_all(class_="mulu"): h2 = mulu.find('h2') if h2!=None: h2_title = h2.string# 获取标题 for a in mulu.find(class_='box').find_all('a'):# 获取所有的a标记中url和章节内容 href = a.get('href') box_title = a.get('title') print href,box_title



* * *



这时已经成功获取标题、章节，接下来将数据存储为JSON。在第2章中，我们已经讲解了JSON文件的基本格式，下面讲解Python如何操作JSON文件。

Python对JSON文件的操作分为编码和解码，通过JSON模块来实现。编码过程是把Python对象转换成JSON对象的一个过程，常用的两个函数是dumps和dump函数。两个函数的唯一区别就是dump把Python对象转换成JSON对象，并将JSON对象通过fp文件流写入文件中，而dumps则是生成了一个字符串。下面看一下dumps和dump的函数原型：



* * *



dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding='utf-8', default=None, sort_keys=False, **kw) dump(obj, fp, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding='utf-8', default=None, sort_keys=False, **kw):



* * *



常用参数分析：

·Skipkeys：默认值是False。如果dict的keys内的数据不是python的基本类型（str、unicode、int、long、float、bool、None），设置为False时，就会报TypeError错误。此时设置成True，则会跳过这类key。

·ensure_ascii：默认值True。如果dict内含有非ASCII的字符，则会以类似“\uXXXX”的格式显示数据，设置成False后，就能正常显示。

·indent：应该是一个非负的整型，如果是0，或者为空，则一行显示数据，否则会换行且按照indent的数量显示前面的空白，将JSON内容进行格式化显示。

·separators：分隔符，实际上是（item_separator，dict_separator）的一个元组，默认的就是（’，‘，’：‘），这表示dictionary内keys之间用“，”隔开，而key和value之间用“：”隔开。

·encoding：默认是UTF-8。设置JSON数据的编码方式，在处理中文时一定要注意。

·sort_keys：将数据根据keys的值进行排序。

示例如下：



* * *



import json str =[{"username":"七夜","age":24},(2,3),1] json_str= json.dumps(str,ensure_ascii=False) print json_str with open('qiye.txt','w') as fp: json.dump(str,fp=fp,ensure_ascii=False)



* * *



输出结果：



* * *



[{"username": "七夜", "age": 24}, [2, 3], 1]



* * *



解码过程是把json对象转换成python对象的一个过程，常用的两个函数是load和loads函数，区别跟dump和dumps是一样的。函数原型如下：



* * *



loads(s, encoding=None, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw) load(fp, encoding=None, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw)



* * *



常用参数分析：

·encoding：指定编码格式。

·parse_float：如果指定，将把每一个JSON字符串按照float解码调用。默认情况下，这相当于float（num_str）。

·parse_int：如果指定，将把每一个JSON字符串按照int解码调用。默认情况下，这相当于int（num_str）。

示例如下：



* * *



new_str=json.loads(json_str) print new_str with open('qiye.txt','r') as fp: print json.load(fp)



* * *



输出结果：



* * *



[{u'username': u'\u4e03\u591c', u'age': 24}, [2, 3], 1] [{u'username': u'\u4e03\u591c', u'age': 24}, [2, 3], 1]



* * *



通过上面的例子可以看到，Python的一些基本类型通过编码之后，tuple类型就转成了list类型了，再将其转回为python对象时，list类型也并没有转回成tuple类型，而且编码格式也发生了变化，变成了Unicode编码。具体转化时，类型变化规则如表5-1和表5-2所示。

表5-1　Python→JSON



表5-2　JSON→Python



以上就是Python操作JSON的全部内容，接下来将提取到的标题、章节和链接进行JSON存储。完整代码如下：



* * *





# coding:utf-8 import json from bs4 import BeautifulSoup import requests user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers={'User-Agent':user_agent} r = requests.get('http://seputu.com/',headers=headers) soup = BeautifulSoup(r.text,'html.parser',from_encoding='utf-8')# html.parser content=[] for mulu in soup.find_all(class_="mulu"): h2 = mulu.find('h2') if h2!=None: h2_title = h2.string# 获取标题 list=[] for a in mulu.find(class_='box').find_all('a'):# 获取所有的a标记中url和章节内容 href = a.get('href') box_title = a.get('title') list.append({'href':href,'box_title':box_title}) content.append({'title':h2_title,'content':list}) with open('qiye.json','wb') as fp: json.dump(content,fp=fp,indent=4)




* * *



打开qiye.json文件，效果如图5-3所示。



图5-3　qiye.json





5.1.2　存储为CSV


CSV（Comma-Separated Values，逗号分隔值，有时也称为字符分隔值，因为分隔字符也可以不是逗号），其文件以纯文本形式存储表格数据（数字和文本）。纯文本意味着该文件是一个字符序列，不含必须像二进制数字那样被解读的数据。

CSV文件由任意数目的记录组成，记录间以某种换行符分隔；每条记录由字段组成，字段间的分隔符是其他字符或字符串，最常见的是逗号或制表符。通常，所有记录都有完全相同的字段序列。CSV文件示例如下：



* * *



ID,UserName,Password,age,country 1001,"qiye","qiye_pass",24,"China" 1002,"Mary","Mary_pass",20,"USA" 1003,"Jack","Jack_pass",20,"USA"



* * *



Python使用csv库来读写CSV文件。要将上面CSV文件的示例内容写成qiye.csv文件，需要用到Writer对象，代码如下：



* * *



import csv headers = ['ID','UserName','Password','Age','Country'] rows = [(1001,"qiye","qiye_pass",24,"China"), (1002,"Mary","Mary_pass",20,"USA"), (1003,"Jack","Jack_pass",20,"USA"), ] with open('qiye.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows)



* * *



里面的rows列表中的数据元组，也可以是字典数据。示例如下：



* * *



import csv headers = ['ID','UserName','Password','Age','Country'] rows = [{'ID':1001,'UserName':"qiye",'Password':"qiye_pass",'Age':24,'Country':" China"}, {'ID':1002,'UserName':"Mary",'Password':"Mary_pass",'Age':20,'Country':"USA"}, {'ID':1003,'UserName':"Jack",'Password':"Jack_pass",'Age':20,'Country':"USA"}, ] with open('qiye.csv','w') as f: f_csv = csv.DictWriter(f,headers) f_csv.writeheader() f_csv.writerows(rows)



* * *



接下来讲解CSV文件的读取。将之前写好的qiye.csv文件读取出来，需要创建reader对象，示例如下：



* * *



import csv with open('qiye.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) print headers for row in f_csv: print row



* * *



运行结果：



* * *



['ID', 'UserName', 'Password', 'Age', 'Country'] ['1001', 'qiye', 'qiye_pass', '24', 'China'] ['1002', 'Mary', 'Mary_pass', '20', 'USA'] ['1003', 'Jack', 'Jack_pass', '20', 'USA']



* * *



在上面的代码中，row会是一个列表。因此，为了访问某个字段，你需要使用索引，如row[0]访问ID，row[3]访问Age。由于这种索引访问通常会引起混淆，因此可以考虑使用命名元组。示例如下：



* * *



from collections import namedtuple import csv with open('qiye.csv') as f: f_csv = csv.reader(f) headings = next(f_csv) Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) print row.UserName,row.Password print row



* * *



运行结果：



* * *



qiye qiye_pass Row(ID='1001', UserName='qiye', Password='qiye_pass', Age='24', Country='China') Mary Mary_pass Row(ID='1002', UserName='Mary', Password='Mary_pass', Age='20', Country='USA') Jack Jack_pass Row(ID='1003', UserName='Jack', Password='Jack_pass', Age='20', Country='USA')



* * *



它允许使用列名如row.UserName和row.Password代替下标访问。需要注意的是这个只有在列名是合法的Python标识符的时候才生效。

除了使用命名分组之外，另外一个解决办法就是读取到一个字典序列中，示例如下：



* * *



import csv with open('qiye.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: print row.get('UserName'),row.get('Password')



* * *



运行结果：



* * *



qiye qiye_pass Mary Mary_pass Jack Jack_pass



* * *



这样就可以使用列名去访问每一行的数据了。比如，row[’UserName‘]或者row.get（’UserName‘）。

以上就是CSV文件读写的全部内容。接下来使用lxml解析http://seputu.com/ 首页的标题、章节和链接等数据，代码如下：



* * *



from lxml import etree import requests user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers={'User-Agent':user_agent} r = requests.get('http://seputu.com/',headers=headers) # 使用lxml解析网页 html = etree.HTML(r.text) div_mulus = html.xpath('.// *[@class="mulu"]')# 先找到所有的div class=mulu标记 for div_mulu in div_mulus: # 找到所有的div_h2标记 div_h2 = div_mulu.xpath('./div[@class="mulu-title"]/center/h2/text()') if len(div_h2)> 0: h2_title = div_h2[0] a_s = div_mulu.xpath('./div[@class="box"]/ul/li/a') for a in a_s: # 找到href属性 href=a.xpath('./@href')[0] # 找到title属性 box_title = a.xpath('./@title')[0].encode('utf-8')



* * *



将box_title数据抽取出来之后，里面的内容类似“[2014-10-2416：59：14]巫山妖棺第七十七章交心”这种形式。这里相较于5.1.1小节添加一步数据清洗，将内容里的时间和章节标题进行分离，这就要使用正则表达式，代码如下。



* * *



pattern = re.compile(r'\s*\[(.*)\]\s+(.*)') match = pattern.search(box_title) if match!=None: date =match.group(1) real_title= match.group(2)



* * *



最后将获取的数据按照title、real_title、href、date的格式写入到CSV文件中，解析存储的完整代码如下：



* * *





html = etree.HTML(r.text) div_mulus = html.xpath('.// *[@class="mulu"]')# 先找到所有的div class=mulu标记 pattern = re.compile(r'\s*\[(.*)\]\s+(.*)') rows=[] for div_mulu in div_mulus: # 找到所有的div_h2标记 div_h2 = div_mulu.xpath('./div[@class="mulu-title"]/center/h2/text()') if len(div_h2)> 0: h2_title = div_h2[0].encode('utf-8') a_s = div_mulu.xpath('./div[@class="box"]/ul/li/a') for a in a_s: # 找到href属性 href=a.xpath('./@href')[0].encode('utf-8') # 找到title属性 box_title = a.xpath('./@title')[0] pattern = re.compile(r'\s*\[(.*)\]\s+(.*)') match = pattern.search(box_title) if match!=None: date =match.group(1).encode('utf-8') real_title= match.group(2).encode('utf-8') # print real_title content=(h2_title,real_title,href,date) print content rows.append(content) headers = ['title','real_title','href','date'] with open('qiye.csv','w') as f: f_csv = csv.writer(f,) f_csv.writerow(headers) f_csv.writerows(rows)




* * *



运行效果如图5-4所示。



图5-4　qiye.csv

注意 　1）在存储CSV文件时，需要统一存储数据的类型。代码中使用encode（’utf-8‘）作用就是将title、real_title、href、date变量类型统一为str。

2）5.1.1节BeautifulSoup如果使用lxml作为解析库，会发现解析出来的HTML内容缺失，这是由于BeautifulSoup为不同的解析器提供了相同的接口，但解析器本身是有区别的，同一篇文档被不同的解析器解析后可能会生成不同结构的树型文档。因此如果遇到缺失的情况，BeautifulSoup可以使用html.parser作为解析器，或者单独使用lxml进行解析即可。





5.2　多媒体文件抽取


存储媒体文件主要有两种方式：只获取文件的URL链接，或者直接将媒体文件下载到本地。如果你采取的是第一种方式，只需看5.1节。本节主要讲解第二种方式，即将媒体文件下载下来。

本节主要介绍urllib模块提供的urlretrieve（）函数。urlretrieve（）方法直接将远程数据下载到本地，函数原型如下：



* * *



urlretrieve(url, filename=None, reporthook=None, data=None)



* * *



参数说明：

·参数filename指定了存储的本地路径（如果参数未指定，urllib会生成一个临时文件保存数据。）

·参数reporthook是一个回调函数。当连接上服务器以及相应的数据块传输完毕时会触发该回调函数，我们可以利用这个回调函数来显示当前的下载进度。

·参数data指post到服务器的数据，该方法返回一个包含两个元素的（filename，headers）元组，filename表示保存到本地的路径，header表示服务器的响应头。

以天堂图片网为例（http://www.ivsky.com/tupian/ziranfengguang/ ），提取当前网址中的图片链接，并将图片下载到当前目录下。代码如下：



* * *



import urllib from lxml import etree import requests def Schedule(blocknum,blocksize,totalsize): ''''' blocknum:已经下载的数据块 blocksize:数据块的大小 totalsize:远程文件的大小 ''' per = 100.0 * blocknum * blocksize / totalsize if per > 100 : per = 100 print '当前下载进度：%d'%per user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers={'User-Agent':user_agent} r = requests.get('http://www.ivsky.com/tupian/ziranfengguang/',headers=headers) # 使用lxml解析网页 html = etree.HTML(r.text) img_urls = html.xpath('.// img/@src')# 先找到所有的img i=0 for img_url in img_urls: urllib.urlretrieve(img_url,'img'+str(i)+'.jpg',Schedule) i+=1



* * *



本程序中先从当前网址将img标记中的src属性提取出来，交给urllib.urlretrieve函数去下载，自动回调Schedule函数，显示当前下载的进度。Schedule函数主要包括3个参数：blocknum（已经下载的数据块）、blocksize（数据块的大小）和totalsize（远程文件的大小）。





5.3　Email提醒


大家可能会奇怪Email在Python爬虫开发中有什么用呢？Email主要起到提醒作用，当爬虫在运行过程中遇到异常或者服务器遇到问题，可以通过Email及时向自己报告。

发送邮件的协议是STMP，Python内置对SMTP的支持，可以发送纯文本邮件、HTML邮件以及带附件的邮件。Python对SMTP支持有smtplib和email两个模块，email负责构造邮件，smtplib负责发送邮件。

在讲解发送Email之前，首先申请一个163邮箱，开启SMTP功能，采用的是网易的电子邮件服务器smtp.163.com，如图5-5所示。



图5-5　163邮箱开启SMTP

将SMTP开启之后，我们来构造一个纯文本邮件：



* * *



from email.mime.text import MIMEText msg = MIMEText('Python爬虫运行异常，异常信息为遇到HTTP 403', 'plain', 'utf-8')



* * *



构造MIMEText对象时需要3个参数：

·邮件正文。

·MIME的subtype，传入“plain”表示纯文本，最终的MIME就是“text/plain”。

·设置编码格式，UTF-8编码保证多语言兼容性。

接着设置邮件的发件人、收件人和邮件主题等信息，并通过STMP发送出去。代码如下：



* * *



from email.header import Header from email.mime.text import MIMEText from email.utils import parseaddr, formataddr import smtplib def _format_addr(s): name, addr = parseaddr(s) return formataddr((Header(name, 'utf-8').encode(), addr)) # 发件人地址 from_addr = 'xxxxxxxx@163.com' # 邮箱密码 password = 'pass' # 收件人地址 to_addr = 'xxxxxxxx@qq.com' # 163网易邮箱服务器地址 smtp_server = 'smtp.163.com ' # 设置邮件信息 msg = MIMEText('Python爬虫运行异常，异常信息为遇到HTTP 403', 'plain', 'utf-8') msg['From'] = _format_addr('一号爬虫 <%s>' % from_addr) msg['To'] = _format_addr('管理员 <%s>' % to_addr) msg['Subject'] = Header('一号爬虫运行状态', 'utf-8').encode() # 发送邮件 server = smtplib.SMTP(smtp_server, 25) server.login(from_addr, password) server.sendmail(from_addr, [to_addr], msg.as_string()) server.quit()



* * *



有时候我们发送的可能不是纯文本，需要发送HTML邮件，将异常网页信息发送回去。在构造MIMEText对象时，把HTML字符串传进去，再把第二个参数由“plain”变为“html”就可以了。示例如下：



* * *



msg = MIMEText('<html><body><h1>Hello</h1>' + '<p>异常网页<a href="http://www.cnblogs.com">cnblogs</a>...</p>' + '</body></html>', 'html', 'utf-8')



* * *





5.4　小结


本章主要讲解了Python爬虫开发中文件存储的各种方式，在存储过程中尤其要注意网页编码和文件编码的问题。多媒体文件存储使用urlretrieve函数非常方便，无需关心存储细节，并能及时了解进度。





第6章　实战项目：基础爬虫


本章讲解第一个实战项目：基础爬虫。为什么叫基础爬虫呢？首先这个爬虫项目功能简单，仅仅考虑功能实现，未涉及优化和稳健性的考虑。再者爬虫虽小，五脏俱全，大型爬虫有的基础模块，这个爬虫都有，只不过实现方式、优化方式，大型爬虫做得更加全面、多样。本次实战项目的需求是爬取100个百度百科网络爬虫词条以及相关词条的标题、摘要和链接等信息，如图6-1所示。



图6-1　网络爬虫词条





6.1　基础爬虫架构及运行流程


首先讲解一下基础爬虫的架构，如图6-2所示。介绍基础爬虫包含哪些模块，各个模块之间的关系是什么。



图6-2　基础爬虫框架

基础爬虫框架主要包括五大模块，分别为爬虫调度器、URL管理器、HTML下载器、HTML解析器、数据存储器。功能分析如下：

·爬虫调度器主要负责统筹其他四个模块的协调工作。

·URL管理器负责管理URL链接，维护已经爬取的URL集合和未爬取的URL集合，提供获取新URL链接的接口。

·HTML下载器用于从URL管理器中获取未爬取的URL链接并下载HTML网页。

·HTML解析器用于从HTML下载器中获取已经下载的HTML网页，并从中解析出新的URL链接交给URL管理器，解析出有效数据交给数据存储器。

·数据存储器用于将HTML解析器解析出来的数据通过文件或者数据库的形式存储起来。

下面通过图6-3展示一下爬虫框架的动态运行流程，方便大家理解。



图6-3　运行流程





6.2　URL管理器


URL管理器主要包括两个变量，一个是已爬取URL的集合，另一个是未爬取URL的集合。采用Python中的set类型，主要是使用set的去重复功能，防止链接重复爬取，因为爬取链接重复时容易造成死循环。链接去重复在Python爬虫开发中是必备的功能，解决方案主要有三种：1）内存去重2）关系数据库去重3）缓存数据库去重。大型成熟的爬虫基本上采用缓存数据库的去重方案，尽可能避免内存大小的限制，又比关系型数据库去重性能高很多。由于基础爬虫的爬取数量较小，因此我们使用Python中set这个内存去重方式。

URL管理器除了具有两个URL集合，还需要提供以下接口，用于配合其他模块使用，接口如下：

·判断是否有待取的URL，方法定义为has_new_url（）。

·添加新的URL到未爬取集合中，方法定义为add_new_url（url），add_new_urls（urls）。

·获取一个未爬取的URL，方法定义为get_new_url（）。

·获取未爬取URL集合的大小，方法定义为new_url_size（）。

·获取已经爬取的URL集合的大小，方法定义为old_url_size（）。

程序URLManager.py的完整代码如下：



* * *



# coding:utf-8 class UrlManager(object): def __init__(self): self.new_urls = set()# 未爬取URL集合 self.old_urls = set()# 已爬取URL集合 def has_new_url(self): ''' 判断是否有未爬取的URL :return: ''' return self.new_url_size()!=0 def get_new_url(self): ''' 获取一个未爬取的URL :return: ''' new_url = self.new_urls.pop() self.old_urls.add(new_url) return new_url def add_new_url(self,url): ''' 将新的URL添加到未爬取的URL集合中 :param url:单个URL :return: ''' if url is None: return if url not in self.new_urls and url not in self.old_urls: self.new_urls.add(url) def add_new_urls(self,urls): ''' 将新的URL添加到未爬取的URL集合中 :param urls:url集合 :return: ''' if urls is None or len(urls)==0: return for url in urls: self.add_new_url(url) def new_url_size(self): ''' 获取未爬取URL集合的大小 :return: ''' return len(self.new_urls) def old_url_size(self): ''' 获取已经爬取URL集合的大小 :return: ''' return len(self.old_urls)



* * *





6.3　HTML下载器


HTML下载器用来下载网页，这时候需要注意网页的编码，以保证下载的网页没有乱码。下载器需要用到Requests模块，里面只需要实现一个接口即可：download（url）。程序HtmlDownloader.py代码如下：



* * *



# coding:utf-8 import requests class HtmlDownloader(object): def download(self,url): if url is None: return None user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers={'User-Agent':user_agent} r = requests.get(url,headers=headers) if r.status_code==200: r.encoding='utf-8' return r.text return None



* * *





6.4　HTML解析器


HTML解析器使用BeautifulSoup4进行HTML解析。需要解析的部分主要分为提取相关词条页面的URL和提取当前词条的标题和摘要信息。

先使用Firebug查看一下标题和摘要所在的结构位置，如图6-4所示。



图6-4　HTML结构位置

从上图可以看到标题的标记位于<dd class=“lemmaWgt-lemmaTitle-title”><h1></h1>，摘要文本位于<div class=“lemma-summary”label-module=“lemmaSummary”>。

最后分析一下需要抽取的URL的格式。相关词条的URL格式类似于<a target=“_blank”href=“/view/7833.htm”>万维网</a>这种形式，提取出a标记中的href属性即可，从格式中可以看到href属性值是一个相对网址，可以使用urlparse.urljoin函数将当前网址和相对网址拼接成完整的URL路径。

HTML解析器主要提供一个parser对外接口，输入参数为当前页面的URL和HTML下载器返回的网页内容。解析器HtmlParser.py程序的代码如下：



* * *



# coding:utf-8 import re import urlparse from bs4 import BeautifulSoup class HtmlParser(object): def parser(self,page_url,html_cont): ''' 用于解析网页内容，抽取URL和数据 :param page_url: 下载页面的URL :param html_cont: 下载的网页内容 :return:返回URL和数据 ''' if page_url is None or html_cont is None: return soup = BeautifulSoup(html_cont,'html.parser',from_encoding='utf-8') new_urls = self._get_new_urls(page_url,soup) new_data = self._get_new_data(page_url,soup) return new_urls,new_data def _get_new_urls(self,page_url,soup): ''' 抽取新的URL集合 :param page_url: 下载页面的URL :param soup:soup :return: 返回新的URL集合 ''' new_urls = set() # 抽取符合要求的a标记 links = soup.find_all('a',href=re.compile(r'/view/\d+\.htm')) for link in links: # 提取href属性 new_url = link['href'] # 拼接成完整网址 new_full_url = urlparse.urljoin(page_url,new_url) new_urls.add(new_full_url) return new_urls def _get_new_data(self,page_url,soup): ''' 抽取有效数据 :param page_url:下载页面的URL :param soup: :return:返回有效数据 ''' data={} data['url']=page_url title = soup.find('dd',class_='lemmaWgt-lemmaTitle-title').find('h1') data['title']=title.get_text() summary = soup.find('div',class_='lemma-summary') # 获取tag中包含的所有文本内容，包括子孙tag中的内容,并将结果作为Unicode字符串返回 data['summary']=summary.get_text() return data



* * *





6.5　数据存储器


数据存储器主要包括两个方法：store_data（data）用于将解析出来的数据存储到内存中，output_html（）用于将存储的数据输出为指定的文件格式，我们使用的是将数据输出为HTML格式。DataOutput.py程序如下：



* * *



# coding:utf-8 import codecs class DataOutput(object): def __init__(self): self.datas=[] def store_data(self,data): if data is None: return self.datas.append(data) def output_html(self): fout=codecs.open('baike.html','w',encoding='utf-8') fout.write("<html>") fout.write("<body>") fout.write("<table>") for data in self.datas: fout.write("<tr>") fout.write("<td>%s</td>"%data['url']) fout.write("<td>%s</td>"%data['title']) fout.write("<td>%s</td>"%data['summary']) fout.write("</tr>") self.datas.remove(data) fout.write("</table>") fout.write("</body>") fout.write("</html>") fout.close()



* * *



其实上面的代码并不是很好的方式，更好的做法应该是将数据分批存储到文件，而不是将所有数据存储到内存，一次性写入文件容易使系统出现异常，造成数据丢失。但是由于我们只需要100条数据，速度很快，所以这种方式还是可行的。如果数据很多，还是采取分批存储的办法。





6.6　爬虫调度器


以上几节已经对URL管理器、HTML下载器、HTML解析器和数据存储器等模块进行了实现，接下来编写爬虫调度器以协调管理这些模块。爬虫调度器首先要做的是初始化各个模块，然后通过crawl（root_url）方法传入入口URL，方法内部实现按照运行流程控制各个模块的工作。爬虫调度器SpiderMan.py的程序如下：



* * *



# coding:utf-8 from firstSpider.DataOutput import DataOutput from firstSpider.HtmlDownloader import HtmlDownloader from firstSpider.HtmlParser import HtmlParser from firstSpider.UrlManager import UrlManager class SpiderMan(object): def __init__(self): self.manager = UrlManager() self.downloader = HtmlDownloader() self.parser = HtmlParser() self.output = DataOutput() def crawl(self,root_url): # 添加入口URL self.manager.add_new_url(root_url) # 判断url管理器中是否有新的url，同时判断抓取了多少个url while(self.manager.has_new_url() and self.manager.old_url_size()<100): try: # 从URL管理器获取新的url new_url = self.manager.get_new_url() # HTML下载器下载网页 html = self.downloader.download(new_url) # HTML解析器抽取网页数据 new_urls,data = self.parser.parser(new_url,html) # 将抽取的url添加到URL管理器中 self.manager.add_new_urls(new_urls) # 数据存储器存储文件 self.output.store_data(data) print "已经抓取%s个链接"%self.manager.old_url_size() except Exception,e: print "crawl failed" # 数据存储器将文件输出成指定格式 self.output.output_html() if __name__=="__main__": spider_man = SpiderMan() spider_man.crawl("http://baike.baidu.com/view/284853.htm")



* * *



到这里基础爬虫架构所需的模块都已经完成，启动程序，大约1分钟左右，数据都被存储为baike.html。使用浏览器打开，效果如图6-5所示。



图6-5　baike.html





6.7　小结


本章介绍了基础爬虫架构的五个模块，无论大型还是小型爬虫都不会脱离这五个模块，希望大家对整个运行流程有清晰的认识，之后介绍的实战项目都会见到这五个模块的身影。





第7章　实战项目：简单分布式爬虫


本章继续实战项目，介绍如何打造分布式爬虫，这对初学者来说是一个不小的挑战，也是一次有意义的尝试。这次打造的分布式爬虫采用比较简单的主从模式，完全手工打造，不使用成熟框架，基本上涵盖了前六章的主要知识点，其中涉及的分布式知识点是分布式进程和进程间通信的内容，算是对Python爬虫基础篇的总结。

目前，大型的爬虫系统都采取分布式爬取结构，通过此次实战项目，大家会对分布式爬虫有一个比较清晰的了解，为之后系统地学习分布式爬虫打下基础。实战目标：爬取2000个百度百科网络爬虫词条以及相关词条的标题、摘要和链接等信息，采用分布式结构改写第6章的基础爬虫，使其功能更加强大。





7.1　简单分布式爬虫结构


本次分布式爬虫采用主从模式。主从模式是指由一台主机作为控制节点，负责管理所有运行网络爬虫的主机，爬虫只需要从控制节点那里接收任务，并把新生成任务提交给控制节点就可以了，在这个过程中不必与其他爬虫通信，这种方式实现简单、利于管理。而控制节点则需要与所有爬虫进行通信，因此可以看到主从模式是有缺陷的，控制节点会成为整个系统的瓶颈，容易导致整个分布式网络爬虫系统性能下降。

此次使用三台主机进行分布式爬取，一台主机作为控制节点，另外两台主机作为爬虫节点。爬虫结构如图7-1所示。





7.2　控制节点


控制节点（ControlNode）主要分为URL管理器、数据存储器和控制调度器。控制调度器通过三个进程来协调URL管理器和数据存储器的工作：一个是URL管理进程，负责URL的管理和将URL传递给爬虫节点；一个是数据提取进程，负责读取爬虫节点返回的数据，将返回数据中的URL交给URL管理进程，将标题和摘要等数据交给数据存储进程；最后一个是数据存储进程，负责将数据提取进程中提交的数据进行本地存储。执行流程如图7-2所示。



图7-1　主从爬虫结构



图7-2　控制节点执行流程





7.2.1　URL管理器


参考第6章的代码，我们对URL管理器做了一些优化。我们采用set内存去重的方式，如果直接存储大量的URL链接，尤其是URL链接很长的时候，很容易造成内存溢出，所以我们将爬取过的URL进行MD5处理。字符串经过MD5处理后的信息摘要长度为128位，将生成的MD5摘要存储到set后，可以减少好几倍的内存消耗，不过Python中的MD5算法生成的是256位，取中间的128位即可。我们同时添加了save_progress和load_progress方法进行序列化的操作，将未爬取URL集合和已爬取的URL集合序列化到本地，保存当前的进度，以便下次恢复状态。URL管理器URLManager.py代码如下：



* * *



# coding:utf-8 import cPickle import hashlib class UrlManager(object): def __init__(self): self.new_urls = self.load_progress('new_urls.txt')# 未爬取URL集合 self.old_urls = self.load_progress('old_urls.txt')# 已爬取URL集合 def has_new_url(self): ''' 判断是否有未爬取的URL :return: ''' return self.new_url_size()!=0 def get_new_url(self): ''' 获取一个未爬取的URL :return: ''' new_url = self.new_urls.pop() m = hashlib.md5() m.update(new_url) self.old_urls.add(m.hexdigest()[8:-8]) return new_url def add_new_url(self,url): ''' 将新的URL添加到未爬取的URL集合中 :param url:单个URL :return: ''' if url is None: return m = hashlib.md5() m.update(url) url_md5 = m.hexdigest()[8:-8] if url not in self.new_urls and url_md5 not in self.old_urls: self.new_urls.add(url) def add_new_urls(self,urls): ''' 将新的URL添加到未爬取的URL集合中 :param urls:url集合 :return: ''' if urls is None or len(urls)==0: return for url in urls: self.add_new_url(url) def new_url_size(self): ''' 获取未爬取URL集合的大小 :return: ''' return len(self.new_urls) def old_url_size(self): ''' 获取已经爬取URL集合的大小 :return: ''' return len(self.old_urls) def save_progress(self,path,data): ''' 保存进度 :param path:文件路径 :param data:数据 :return: ''' with open(path, 'wb') as f: cPickle.dump(data, f) def load_progress(self,path): ''' 从本地文件加载进度 :param path:文件路径 :return:返回set集合 ''' print '[+] 从文件加载进度: %s' % path try: with open(path, 'rb') as f: tmp = cPickle.load(f) return tmp except: print '[!] 无进度文件, 创建: %s' % path return set()



* * *





7.2.2　数据存储器


数据存储器的内容基本上和第6章的一样，不过生成的文件按照当前时间进行命名，以避免重复，同时对文件进行缓存写入。代码如下：



* * *



# coding:utf-8 import codecs import time class DataOutput(object): def __init__(self): self.filepath='baike_%s.html'%(time.strftime("%Y_%m_%d_%H_%M_%S", time. localtime()) ) self.output_head(self.filepath) self.datas=[] def store_data(self,data): if data is None: return self.datas.append(data) if len(self.datas)>10: self.output_html(self.filepath) def output_head(self,path): ''' 将HTML头写进去 :return: ''' fout=codecs.open(path,'w',encoding='utf-8') fout.write("<html>") fout.write("<body>") fout.write("<table>") fout.close() def output_html(self,path): ''' 将数据写入HTML文件中 :param path: 文件路径 :return: ''' fout=codecs.open(path,'a',encoding='utf-8') for data in self.datas: fout.write("<tr>") fout.write("<td>%s</td>"%data['url']) fout.write("<td>%s</td>"%data['title']) fout.write("<td>%s</td>"%data['summary']) fout.write("</tr>") self.datas.remove(data) fout.close() def ouput_end(self,path): ''' 输出HTML结束 :param path: 文件存储路径 :return: ''' fout=codecs.open(path,'a',encoding='utf-8') fout.write("</table>") fout.write("</body>") fout.write("</html>") fout.close()



* * *





7.2.3　控制调度器


控制调度器主要是产生并启动URL管理进程、数据提取进程和数据存储进程，同时维护4个队列保持进程间的通信，分别为url_queue、result_queue、conn_q、store_q。4个队列说明如下：

·url_q队列是URL管理进程将URL传递给爬虫节点的通道。

·result_q队列是爬虫节点将数据返回给数据提取进程的通道。

·conn_q队列是数据提取进程将新的URL数据提交给URL管理进程的通道。

·store_q队列是数据提取进程将获取到的数据交给数据存储进程的通道。

因为要和工作节点进行通信，所以分布式进程必不可少。参考1.4.4节中服务进程的代码（Linux版），创建一个分布式管理器，定义为start_manager方法。方法代码如下：



* * *





def start_Manager(self,url_q,result_q): ''' 创建一个分布式管理器 :param url_q: url队列 :param result_q: 结果队列 :return: ''' # 把创建的两个队列注册在网络上，利用register方法，callable参数关联了Queue对象， # 将Queue对象在网络中暴露 BaseManager.register('get_task_queue',callable=lambda:url_q) BaseManager.register('get_result_queue',callable=lambda:result_q) # 绑定端口8001，设置验证口令“baike”。这个相当于对象的初始化 manager=BaseManager(address=('',8001),authkey='baike') # 返回manager对象 return manager




* * *



URL管理进程将从conn_q队列获取到的新URL提交给URL管理器，经过去重之后，取出URL放入url_queue队列中传递给爬虫节点，代码如下：



* * *



def url_manager_proc(self,url_q,conn_q,root_url): url_manager = UrlManager() url_manager.add_new_url(root_url) while True: while(url_manager.has_new_url()): # 从URL管理器获取新的URL new_url = url_manager.get_new_url() # 将新的URL发给工作节点 url_q.put(new_url) print 'old_url=',url_manager.old_url_size() # 加一个判断条件，当爬取2000个链接后就关闭，并保存进度 if(url_manager.old_url_size()>2000): # 通知爬行节点工作结束 url_q.put('end') print '控制节点发起结束通知!' # 关闭管理节点，同时存储set状态 url_manager.save_progress('new_urls.txt',url_manager.new_urls) url_manager.save_progress('old_urls.txt',url_manager.old_urls) return # 将从result_solve_proc获取到的URL添加到URL管理器 try: if not conn_q.empty(): urls = conn_q.get() url_manager.add_new_urls(urls) except BaseException,e: time.sleep(0.1)# 延时休息



* * *



数据提取进程从result_queue队列读取返回的数据，并将数据中的URL添加到conn_q队列交给URL管理进程，将数据中的文章标题和摘要添加到store_q队列交给数据存储进程。代码如下：



* * *



def result_solve_proc(self,result_q,conn_q,store_q): while(True): try: if not result_q.empty(): content = result_q.get(True) if content['new_urls']=='end': # 结果分析进程接收通知然后结束 print '结果分析进程接收通知然后结束!' store_q.put('end') return conn_q.put(content['new_urls'])# url 为set 类型 store_q.put(content['data'])# 解析出来的数据为dict 类型 else: time.sleep(0.1)# 延时休息 except BaseException,e: time.sleep(0.1)# 延时休息



* * *



数据存储进程从store_q队列中读取数据，并调用数据存储器进行数据存储。代码如下：



* * *



def store_proc(self,store_q): output = DataOutput() while True: if not store_q.empty(): data = store_q.get() if data=='end': print '存储进程接受通知然后结束!' output.ouput_end(output.filepath) return output.store_data(data) else: time.sleep(0.1)



* * *



最后启动分布式管理器、URL管理进程、数据提取进程和数据存储进程，并初始化4个队列。代码如下：



* * *



if __name__=='__main__': # 初始化4个队列 url_q = Queue() result_q = Queue() store_q = Queue() conn_q = Queue() # 创建分布式管理器 node = NodeManager() manager = node.start_Manager(url_q,result_q) # 创建URL管理进程、 数据提取进程和数据存储进程 url_manager_proc = Process(target=node.url_manager_proc, args=(url_q,conn_q, 'http://baike.baidu.com/view/284853.htm',)) result_solve_proc = Process(target=node.result_solve_proc, args=(result_q, conn_q,store_q,)) store_proc = Process(target=node.store_proc, args=(store_q,)) # 启动3个进程和分布式管理器 url_manager_proc.start() result_solve_proc.start() store_proc.start() manager.get_server().serve_forever()



* * *





7.3　爬虫节点


爬虫节点（SpiderNode）相对简单，主要包含HTML下载器、HTML解析器和爬虫调度器。执行流程如下：

·爬虫调度器从控制节点中的url_q队列读取URL。

·爬虫调度器调用HTML下载器、HTML解析器获取网页中新的URL和标题摘要。

·爬虫调度器将新的URL和标题摘要传入result_q队列交给控制节点。





7.3.1　HTML下载器


HTML下载器的代码和第6章的一致，只要注意网页编码即可。代码如下：



* * *



# coding:utf-8 import requests class HtmlDownloader(object): def download(self,url): if url is None: return None user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers={'User-Agent':user_agent} r = requests.get(url,headers=headers) if r.status_code==200: r.encoding='utf-8' return r.text return None



* * *





7.3.2　HTML解析器


HTML解析器的代码和第6章的一致，详细的网页分析过程可以回顾第6章。代码如下：



* * *



# coding:utf-8 import re import urlparse from bs4 import BeautifulSoup class HtmlParser(object): def parser(self,page_url,html_cont): ''' 用于解析网页内容，抽取URL和数据 :param page_url: 下载页面的URL :param html_cont: 下载的网页内容 :return:返回URL和数据 ''' if page_url is None or html_cont is None: return soup = BeautifulSoup(html_cont,'html.parser',from_encoding='utf-8') new_urls = self._get_new_urls(page_url,soup) new_data = self._get_new_data(page_url,soup) return new_urls,new_data def _get_new_urls(self,page_url,soup): ''' 抽取新的URL集合 :param page_url: 下载页面的URL :param soup:soup :return: 返回新的URL集合 ''' new_urls = set() # 抽取符合要求的a标记 links = soup.find_all('a',href=re.compile(r'/view/\d+\.htm')) for link in links: # 提取href属性 new_url = link['href'] # 拼接成完整网址 new_full_url = urlparse.urljoin(page_url,new_url) new_urls.add(new_full_url) return new_urls def _get_new_data(self,page_url,soup): ''' 抽取有效数据 :param page_url:下载页面的URL :param soup: :return:返回有效数据 ''' data={} data['url']=page_url title = soup.find('dd',class_='lemmaWgt-lemmaTitle-title').find('h1') data['title']=title.get_text() summary = soup.find('div',class_='lemma-summary') # 获取tag中包含的所有文本内容，包括子孙tag中的内容,并将结果作为Unicode字符串返回 data['summary']=summary.get_text() return data



* * *





7.3.3　爬虫调度器


爬虫调度器需要用到分布式进程中工作进程的代码，具体内容可以参考第1章的分布式进程章节。爬虫调度器需要先连接上控制节点，然后从url_q队列中获取URL，下载并解析网页，接着将获取的数据交给result_q队列并返回给控制节点，代码如下：



* * *



class SpiderWork(object): def __init__(self): # 初始化分布式进程中工作节点的连接工作 # 实现第一步：使用BaseManager注册用于获取Queue的方法名称 BaseManager.register('get_task_queue') BaseManager.register('get_result_queue') # 实现第二步：连接到服务器 server_addr = '127.0.0.1' print('Connect to server %s...' % server_addr) # 注意保持端口和验证口令与服务进程设置的完全一致 self.m = BaseManager(address=(server_addr, 8001), authkey='baike') # 从网络连接 self.m.connect() # 实现第三步：获取Queue的对象 self.task = self.m.get_task_queue() self.result = self.m.get_result_queue() # 初始化网页下载器和解析器 self.downloader = HtmlDownloader() self.parser = HtmlParser() print 'init finish' def crawl(self): while(True): try: if not self.task.empty(): url = self.task.get() if url =='end': print '控制节点通知爬虫节点停止工作...' # 接着通知其他节点停止工作 self.result.put({'new_urls':'end','data':'end'}) return print '爬虫节点正在解析:%s'%url.encode('utf-8') content = self.downloader.download(url) new_urls,data = self.parser.parser(url,content) self.result.put({"new_urls":new_urls,"data":data}) except EOFError,e: print "连接工作节点失败" return except Exception,e: print e print 'Crawl fali ' if __name__=="__main__": spider = SpiderWork() spider.crawl()



* * *



在爬虫调度器中设置了一个本地IP 127.0.0.1，大家可以在一台机器上测试代码的正确性。当然也可以使用三台VPS服务器，两台运行爬虫节点程序，将IP改为控制节点主机的公网IP，一台运行控制节点程序，进行分布式爬取，这样更贴近真实的爬取环境。图7-3为最终爬取的数据，图7-4为new_urls.txt的内容，图7-5为old_urls.txt的内容，大家可以进行对比测试，这个简单的分布式爬虫还有很大的发挥空间，希望大家发挥自己的聪明才智进一步完善。



图7-3　最终爬取的数据



图7-4　new_urls.txt



图7-5　old_urls.txt





7.4　小结


本章讲解了一个简单的分布式爬虫结构，主要目的是帮助大家对Python爬虫基础篇的知识进行总结和强化，开拓思维，同时也让大家知道分布式爬虫并不是高不可攀。不过当你亲手打造一个分布式爬虫后，就会知道分布式爬虫的难点在于节点的调度，什么样的结构能让各个节点稳定高效地运作才是分布式爬虫要考虑的核心内容。到本章为止，Python爬虫基础篇已经结束，这个时候大家基本上可以编写简单的爬虫，爬取一些静态网站的内容，但是Python爬虫开发不仅如此，大家接着往下学习吧。





中级篇


·第8章　数据存储（数据库版）

·第9章　动态网站抓取

·第10章　Web端协议分析

·第11章　终端协议分析

·第12章　初窥Scrapy爬虫框架

·第13章　深入Scrapy爬虫框架

·第14章　实战项目：Scrapy爬虫





第8章　数据存储（数据库版）


第5章已经讲解了数据存储（无数据库版），本章继续讲解数据存储（数据库版），主要讲解SQLite、MySQL和MongoDB三种数据库的基本用法和如何使用Python对数据库进行操作。





8.1　SQLite


SQLite是一个开源的嵌入式关系数据库，实现自包容、零配置、支持事务的SQL数据库引擎。其特点是高度便携、使用方便、结构紧凑、高效、可靠。与其他数据库管理系统不同，SQLite的安装和运行非常简单，如果对并发性要求不是特别高，SQLite是一个不错的选择。SQLite还是单文件数据库引擎，一个文件即是一个数据库，方便存储和转移。





8.1.1　安装SQLite


下面主要介绍如何在Ubuntu和Windows下安装SQLite数据库引擎。

1.Ubuntu

目前，大多数的Linux系统都预安装了SQLite，只需要在shell中输入：sqlite3，如图8-1所示。

如果没有看到图8-1的效果，可以在shell中输入以下命令进行安装：



* * *



sudo apt-get install sqlite3



* * *



安装完成后可以使用sqlite3-version命令查看SQLite的版本信息。

2.Windows

首先到SQLite下载页面http://www.sqlite.org/download.html ，根据windows系统版本下载sqlite-dll-*.zip和sqlite-tools-win32-*.zip两个压缩包，在硬盘上创建一个文件夹，比如D：\sqlite3，将两个压缩包中的文件解压到D：\sqlite3中，最后将D：\sqlite3添加到环境变量PATH中即可。打开cmd命令行窗口，输入sqlite3，效果如图8-2所示，则证明配置成功。



图8-1　SQLite（Ubuntu）



图8-2　SQLite（Windows）





8.1.2　SQL语法


进行数据库操作，必然要了解SQL语法。SQL是一门ANSI的标准计算机语言，用来访问和操作数据库系统，用于取回和更新数据库中的数据，并与数据库程序协同工作，比如MS Access、DB2、Informix、MS SQL Server、Oracle、Sybase以及其他数据库系统。

不幸的是，存在着很多不同版本的SQL语言，每个数据库都有一些它们独特的SQL语法，但是为了与ANSI标准相兼容，它们必须以相似的方式共同地来支持一些主要的关键词，比如SELECT、UPDATE、DELETE、INSERT、WHERE等等。因此本小节主要讲解一些常见的SQL语法。

SQL语言主要分为两个部分：数据定义语言（DDL）和数据操作语言（DML）。数据定义语言（DDL）使我们有能力创建或删除表格，也可以定义索引（键），规定表之间的链接，以及施加表间的约束。数据操作语言（DML）用于执行查询、更新、插入和删除记录。有一点需要注意：SQL语法对大小写不敏感。

1.数据定义语言（DDL）

对于数据定义语言，主要讲解表8-1所示内容。

表8-1　数据定义语言



CREATE DATABASE用于创建数据库，语法格式：CREATE DATABASE database_name。比如创建名称为first_db的数据库，SQL语句为CREATE DATABASE first_db。

DROP DATABASE用于删除数据库，语法格式：DROP DATABASE database_name。比如删除名称为first_db的数据库，SQL语句为DROP DATABASE first_db。

CREATE TABLE语句用于创建数据库中的表，语法格式：CREATE TABLE表名称（列名称1数据类型，列名称2数据类型，列名称3数据类型，...）。SQL支持的数据类型如表8-2所示。

表8-2　SQL支持的数据类型



比如创建一个名称为student的表，表里面包含5列，列名分别是：“id”、“Name”、“Birth”、“Address”以及“City”。语句如下：CREATE TABLE student（id integer，Name varchar（255），Birth date，Address varchar（255），City varchar（255））。id列的数据类型是integer，包含整数，Birth为日期类型，其余的数据类型是varchar，最大长度为255个字符。

ALTER TABLE语句用于在已有的表中添加、修改或删除列。

·在表中添加列：ALTER TABLE table_name ADD column_name datatype

·修改表中某一列的数据类型：ALTER TABLE table_name ALTER COLUMN column_name datatype

·删除表中的某一列：ALTER TABLE table_name DROP COLUMN column_name

例如在之前创建的student表中添加名为class的一列，语句如下：ALTER TABLE student ADD class varchar（255）。接着将class列的数据类型改为char（10），语句如下：ALTER TABLE student ALTER COLUMN class char（10）。最后将class列删除，语句如下：ALTER TABLE student DROP COLUMN class。

DROP TABLE语句用于删除表（表的结构、属性以及索引也会被删除），语法格式：DROP TABLE table_name。比如删除表名为student的表，SQL语句为DROP TABLE student。

CREATE INDEX语句用于创建索引，索引有助于加快SELECT查询和WHERE子句，但它会减慢使用UPDATE和INSERT语句时的数据输入。索引可以创建或删除，但不会影响数据。CREATE INDEX的基本语法如下：CREATE INDEX index_name ON table_name。创建索引还分为创建单一索引、唯一索引、组合索引和隐式索引。单一索引指的是在表的某一列设置索引，语法如下：CREATE INDEX index_nameON table_name（column_name）。唯一索引指的是不允许任何重复的值插入到表中，语法如下：CREATE UNIQUE INDEX index_name on table_name（column_name）。组合索引可以对一个表中的几列进行索引，语法如下：CREATE INDEX index_name on table_name（column1，column2）。隐式索引是在创建对象时，由数据库服务器自动创建的索引。比如在之前的student表中对Name添加名称为name_index的索引，语句为：CREATE INDEX name_index ON student（Name）。

DROP INDEX语句用于删除索引，语法格式为：DROP INDEX index_name。比如将上面创建的name_index索引删除，语句为：DROP INDEX name_index。

2.数据操作语言（DML）

对于数据操作语言的定义，主要讲解一下增删改查四个部分的语法：

·SELECT用于查询数据库表中数据。

·UPDATE用于更新数据库表中数据。

·DELETE用于从数据库表中删除数据。

·INSERT INTO用于向数据库表中插入数据。

SELECT用来从表中选取数据，结果存储在一个结果集中。语法格式：SELECT列名称1，列名称2，...FROM表名称以及SELECT FROM表名称。以表8-3的student表为例：

表8-3　student表



比如我们想获取student表中Name和City列的内容，SQL语句为：SELECT Name，City FROM student。最后查询的结果如下所示：





如果想获取student表中的所有列，使用通配符*来代替列名称，SQL语句为：SELECT FROM student。


在表中可能会包含重复值，关键词DISTINCT用于返回唯一不同的值。语法格式为：SELECT DISTINCT列名称1，列名称2，...FROM表名称。上面的查询结果marry出现两次。去重复可以使用如下SQL语句：SELECT DISTINCT Name，City FROM student。查询结果如下所示：



对表中数据进行有条件查找，需要用到WHERE子句，将WHERE子句添加到SELECT语句中。语法格式为：SELECT列名称FROM表名称WHERE列运算符值。以下运算符可在WHERE子句中使用：



如果我们想选取City为beijing的记录，SQL语句为：SELECT Name，City FROM student WHERE City=’beijing‘。查询结果如下所示：



大家可能注意到了，WHERE City=’beijing‘子句中beijing使用单引号包裹起来了，一般使用文本值进行选取时，需要使用单引号进行包裹，如果使用数值进行选取，则不需要用单引号。比如选取id>2的记录，SQL语句为：SELECT Name，City FROM student WHERE id>2。

上面讲到WHERE语句使用的是单一条件，可以在WHERE子句中添加OR或者AND运算符实现一个以上条件的筛选。AND运算符相连的条件，必须所有条件都成立，才能显示一条有效记录。OR运算符相连的条件，只要有一个条件成立，就能显示一条有效记录。示例如表8-4所示。

表8-4　AND和OR使用示例



使用多个条件进行筛选时，可以使用圆括号组成复杂的表达式。

如果对查询到的数据进行排序，需要和ORDER BY语句配合使用。ORDER BY语句用于根据指定的列对结果集进行排序，默认按照升序ASC对记录进行排序。如果想按照降序对记录进行排序，可以使用DESC关键字。示例如表8-5所示。

表8-5　OROER BY语句使用示例



UPDATE语句用于修改表中的数据。语法格式：UPDATE表名称SET列名称=新值WHERE列名称=某值。比如想修改id=1这条记录中Name和City表项的内容，SQL语句如下：UPDATE student SET Name=’jack‘，City=’Nanjing‘WHERE id=1。

DELETE语句用于删除表中的行。语法格式：DELETE FROM表名称WHERE列名称=值。比如删除Name为jack这条记录，SQL语句为：DELETE FROM student WHERE Name=’jack‘。

INSERT INTO语句用于向表格中插入新的行。语法格式：INSERT INTO表名称VALUES（值1，值2，....）或者指定要插入数据的列：INSERT INTO table_name（列1，列2，...）VALUES（值1，值2，....）。以表8-3为例，向其中插入一条记录，SQL语句为：INSERT INTO student VALUES（5，’Bill‘，’1999-8-10‘，’beijing‘）。结果如下所示：



向指定的列插入一条记录，SQL语句为：INSERT INTO student（id，Name，city）VALUES（6，’Rose‘，’shenzhen‘）。结果如下所示：





8.1.3　SQLite增删改查


讲解完了SQL语法，基本上可以完成大多数数据库的增删改查的操作。下面讲解一下在SQLite的命令行窗口中，进行一系列增删改查的工作。

1.创建数据库和表

在命令行窗口中输入：sqlite3D：\test.db，就可以在D盘创建test.db数据库。接着在数据库中创建person表，包含id，name，age等3列，输入语句：CREATE TABLE person（id integer primary key，name varchar（20），age integer）；，效果如图8-3所示。



图8-3　创建数据库和表

2.增删改查操作

增加：插入一条name为qiye，age为20的记录：



* * *



INSERT INTO person(name,age) VALUES('qiye',20);



* * *



修改：将name为qiye的记录中age修改为17：



* * *



UPDATE person SET age=17 WHERE name='qiye';



* * *



查询：查询表中的记录：



* * *



SELECT * FROM person;



* * *



删除name为qiye的记录：



* * *



DELETE FROM person WHERE name='qiye';



* * *



以上操作如图8-4所示。



图8-4　增删改查操作

3.常用SQLite命令

下面主要说一下常用的SQLite命令，方便大家对SQLite进行操作。以下均是在命令行中的效果：

·显示表结构：



* * *



sqlite> .schema [table]



* * *



·获取所有表和视图：



* * *



sqlite > .tables



* * *



·获取指定表的索引列表：



* * *



sqlite > .indices [table ]



* * *



·导出数据库到SQL文件：



* * *



sqlite > .output [filename ] sqlite > .dump qlite > .output stdout



* * *



·从SQL文件导入数据库：



* * *



sqlite > .read [filename ]



* * *



·格式化输出数据到CSV格式：



* * *



sqlite >.output [filename.csv ] sqlite >.separator , sqlite > select * from test; sqlite >.output stdout



* * *



·从CSV文件导入数据到表中：



* * *



sqlite >create table newtable (id integer primary key,name varchar(20),age integer ); sqlite >.import [filename.csv ] newtable



* * *



·备份数据库：



* * *



sqlite3 test.db .dump > backup.sql



* * *



·恢复数据库：



* * *



sqlite3 test.db < backup.sql



* * *





8.1.4　SQLite事务


数据库事务指的是作为单个逻辑工作单元执行的一系列操作，要么完全执行，要么完全不执行。设想一个网上购物的场景，用户付款的过程至少包括以下几步操作：

1）更新客户所购商品的库存信息。

2）保存客户付款信息，同时与银行系统交互。

3）生成订单并且保存到数据库中。





4）更新用户相关信息，例如购物数量等数据。


正常情况下，这些操作完全成功执行，才算一次有效的交易。交易成功后，与交易相关的所有数据库信息也将成功更新。但是以上四步任意一个环节出现了异常，例如网络中断、客户银行帐户存款不足等，都会导致交易的失败。大家可以想象一下，假如数据库更新完第二步，到第三步时操作失败了，就会出现成功付款但是没有买到商品的情况，这是非常不合理的情况。这个时候事务的作用就体现出来了，一旦交易失败，数据库中所有信息都必须保持交易前的状态，即使进行到最后一步才出错，也要恢复到交易前状态，因此事务是用来保证这种情况下交易的平稳性和可预测性的技术。通俗来说，事务是将四个步骤打包成一件事来做，其中任何一个步骤出错，都代表这件事情没完成，数据库就会回滚到之前的状态。

SQLite主要通过以下命令来控制事务：

·BEGIN TRANSACTION：启动事务处理。

·COMMIT：保存更改，或者使用END TRANSACTION命令。

·ROLLBACK：回滚所做的更改。

控制事务的命令只与DML命令中的INSERT、UPDATE和DELETE一起使用，不能在创建表和删除表时使用，因为这两个操作是数据库自动提交的。

下面在命令行中演示一下如何使用事务，打开之前创建的test.db文件，向person里面插入一条记录，在插入数据之前要先查看一下表中数据，用来进行对比。如图8-5所示。

经过回滚操作，可以看到数据并没有插入到person数据表中。下面使用COMMIT命令进行提交，如图8-6所示。



图8-5　事务回滚





8.1.5　Python操作SQLite


1.导入sqlite数据库模块

Python中使用sqlite3模块操作SQLite。从Python 2.5以后，sqlite3成为内置模块，不需要额外安装，只需要导入即可。



* * *



import sqlite3



* * *





图8-6　事务提交

2.创建/打开数据库

sqlite3模块中使用connect方法创建/打开数据库，需要指定数据库路径，如果数据库存在则打开，不存在则创建一个新的数据库。



* * *



con = sqlite3.connect('D:\test.db')



* * *



不仅可以在硬盘上创建数据库文件，还可以在内存中创建。



* * *



con = sqlite3.connect(':memory:')



* * *



3.数据库连接对象

上面通过connect方法返回的con对象，即是数据库连接对象，它提供了以下方法：

·cursor（）方法用来创建一个游标对象。

·commit（）方法用来事务提交。

·rollback（）方法用来事务回滚。

·close（）方法用来关闭一个数据库连接。

4.游标对象的使用

对数据库的查询需要使用到游标对象，首先通过cursor（）方法创建一个游标对象：



* * *



cur = con.cursor()



* * *



游标对象有以下方法支持数据库的操作：

·execute（）用来执行sql语句。

·executemany（）用来执行多条sql语句。

·close（）用来关闭游标。

·fetchone（）用来从结果中取一条记录，并将游标指向下一条记录。

·fetchmany（）用来从结果中取多条记录。

·fetchall（）用来从结果中取出所有记录。

·scroll（）用于游标滚动。

5.建表

首先使用游标对象创建一个person表，包含id、name、age等3列，代码如下：



* * *



cur.execute(' CREATE TABLE person (id integer primary key,name varchar(20),age integer)')



* * *



6.插入数据

向person表中插入两条数据。插入数据一般有两种做法，第一种做法是直接构造一个插入的SQL语句，代码如下：



* * *



data="0,'qiye',20" cur.execute(' INSERT INTO person VALUES (%s)'%data)



* * *



但是这种做法是非常不安全的，容易导致SQL注入。另一种做法使用占位符“”的方式来规避这个问题，代码如下：



* * *



cur.execute(' INSERT INTO person VALUES (,,)',(0,'qiye',20))



* * *



还可以使用executemany（）执行多条SQL语句，使用executemany（）方法比循环使用execute（）方法执行多条SQL语句效率高很多。



* * *



cur.executemany(' INSERT INTO person VALUES (,,)',[(3,'marry',20),(4,'jack',20)])



* * *



这两种方法插入数据都不会立即生效，需要使用数据库对象con进行提交操作：

如果出现错误，还可以使用回滚操作：



* * *



con.commit()



* * *



7.查询数据

查询person表中的所有数据，代码如下：



* * *



cur.execute('SELECT * FROM person')



* * *



要提取查询数据，游标对象提供了fetchall（）和fetchone（）方法。fetchall（）方法获取所有数据，返回一个二维的列表。fetchone（）方法获取其中的一个结果，返回一个元组。使用方法如下：



* * *



cur.execute('SELECT * FROM person') res = cur.fetchall() for line in res: print line cur.execute('SELECT * FROM person') res = cur.fetchone() print res



* * *



8.修改和删除数据



* * *



cur.execute('UPDATE person SET name= WHERE id=',('rose',1)) cur.execute('DELETE FROM person WHERE id=',(0,)) con.commit() con.close()



* * *



注意 　执行完所有操作记得关闭数据库，插入或者修改中文数据时，记得在中文字符串之前加上“u”。





8.2　MySQL


MySQL是一个关系型数据库管理系统，由瑞典MySQL AB公司开发，目前属于Oracle公司。MySQL是一种关联数据库管理系统，关联数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。Mysql是开源的，而且支持大型的数据库，可以处理上千万条记录，因此如果你的数据量很大的话，MySQL确实是一个不错的选择。本节将对MySQL的一些基本操作进行讲解。





8.2.1　安装MySQL


接下来开始进行MySQL的安装，以Ubuntu和Windows为例。

1.Ubuntu下安装和配置MySQL

Ubuntu上安装MySQL非常简单，打开命令行窗口，输入以下命令即可。

·sudo apt-get install mysql-server

·sudo apt-get install mysql-client

·sudo apt-get install libmysqlclient-dev

在安装的过程中需要根据提示设置MySQL账号和密码，如图8-7所示。注意设置完不要忘记，之后会使用账号密码登录MySQL。

安装完成之后可以使用如下命令来检查是否安装成功：sudo netstat-tap|grep mysql。如果看到有mysql的socket处于listen状态则表示安装成功，效果如图8-8所示。

接下来使用之前设置的账号和密码进行登录，在命令行中输入：mysql-u root-p。这时候会提示输入密码。成功登录如图8-9所示。



图8-7　配置MySQL



图8-8　安装MySQL成功



图8-9　Ubuntu下登录MySQL成功

2.Windows下安装和配置MySQL

Windows版MySQL的下载地址为http://dev.mysql.com/downloads/mysql/ ，大家可以根据自己的系统版本进行下载，下载的是一个zip格式的压缩包。配置步骤如下：

1）将下载的mysql-*.zip解压到文件夹，如：D：\MySQL

2）在安装文件夹下找到my-default.ini配置文件，将其重命名为config.ini，打开该文件进行编辑，修改basedir和datadir。basedir为MySQL所在目录，datadir为basedir下的data文件夹（如果没有，自行创建），修改内容如下：



* * *



basedir = D:\MySQL\mysql-5.7.16-winx64; datadir = D:\MySQL\mysql-5.7.16-winx64\data。



* * *



3）打开Windows环境变量设置，新建变量名MYSQL_HOME，变量值为MySQL安装目录路径，这里为D：\MySQL\mysql-5.7.16-winx64。在环境变量的Path变量中追加“%MYSQL_HOME%\bin；”

4）安装MySQL服务，以管理员权限运行Windows命令提示符，并切换到bin目录下，执行命令：mysqld--install MySQL--defaults-file=“D：\MySQL\mysql-5.7.16-winx64\config.ini”，提示“Service successfully installed.”表示成功

安装完成后，接着在windows命令行中输入：mysqld--initialize-insecure--user=mysql，最后通过以下命令实现对MySQL服务的控制：

·启动：net start MySQL

·停止：net stop MySQL

·卸载：sc delete MySQL

服务启动完成后，登录MySQL，输入：mysql-u root-p。一开始是没有密码的，要求输入密码时直接回车即可，登录成功如图8-10所示。



图8-10　Windows下登录MySQL成功





8.2.2　MySQL基础


完成MySQL的配置，接下来开始讲解MySQL的基础内容。

1.MySQL数据类型

MySQL数据类型比之前讲的SQL语法中的数据类型多了一些，下面通过表8-6对MySQL的数据类型进行以下总结。

表8-6　MySQL数据类型总结



2.MySQL关键字

MySQL有一些和数据类型有关的关键字，如表8-7所示。

表8-7　MySQL的关键字总结



3.创建数据库与表

登录成功MySQL之后，使用create database语句可完成对数据库的创建，创建命令的格式如下：



* * *



create database 数据库名 [其他选项];



* * *



比如创建一个名称为test的数据库，并设置编码为gbk：



* * *



create database test character set gbk;



* * *



创建成功后，会返回如下信息：



* * *



Query OK, 1 row affected (0.01 sec)



* * *



创建成功后，我们需要对数据库进行选择，选择成功后才能对数据库进行操作。使用use语句指定，命令如下：



* * *



use 数据库名;



* * *



选择创建的test数据库：



* * *



use test;



* * *



选择成功后会提示：Database changed

使用create table语句可实现对表的创建，创建命令的格式如下：



* * *



create table 表名称(列声明1, 列声明2,...);



* * *



以创建student表为例，表中有学号（id）、姓名（name）、性别（sex）、年龄（age）等列：



* * *



create table student ( id int unsigned not null auto_increment primary key, name char(8) not null, sex char(4) not null, age tinyint unsigned not null );



* * *



如果担心在命令行中输入这么长的SQL语句会出错，可以将以上SQL语句保存为create_student.sql文件，比如将它保存在D盘根目录。有两种方式可以让MySQL执行sql文件。

·在登录MySQL的时候输入：mysql-D test-u root-p< li="">

·在登录MySQL之后，输入：source D：\create_student.sql，或输入：\.D：\create_student.sql。

4.增删改查操作

MySQL中的增删改查操作基本上和SQLite一样，下面以student表进行演示：

·insert into student values（NULL，“七夜”，“男”，24）；

·update student set age=18where name=“七夜”；

·select name，age from student；

·delete from student where age=18；

效果如图8-11所示。



图8-11　MySQL下增删改查

注意 　在MySQL中字符串既可以使用单引号包裹，也可以使用双引号包裹。

5.对表结构的操作

alter table语句用于对创建后的表进行修改，MySQL对表结构的操作相对于SQLite更加完整和丰富。基本用法如表8-8所示。

表8-8　MySQL对表结构的操作



示例如下：

·alter table student add address varchar（60）after age；

·alter table student change address addr char（60）；

·alter table student drop addr；

·alter table student rename students；

效果如图8-12所示。



图8-12　表结构操作

6.删除数据库和表

基本用法如表8-9所示。

表8-9　MySQL删除数据库和表的操作



7.MySQL常用命令

下面主要说一下常用的MySQL命令，其中例子均是在命令行中的操作，用法如表8-16所示：

连接MySQL。 命令格式为：mysql-h主机地址-u用户名－p用户密码。示例如下：

1）连接到本机MySQL。

在命令行中输入mysql-u root-p；，回车后按提示输入密码。

2）连接到远程主机上的MySQL。

远程主机的IP为：10.110.18.120，用户名为root，密码为123：mysql-h 10.110.18.120-u root-p 123；

修改密码。 命令格式为：mysqladmin-u用户名-p旧密码password新密码。示例如下：

1）给root加个密码abc12。

在命令行中输入mysqladmin-u root-password abc123；，开始的时候root没有密码，所以“-p旧密码”一项就可以省略了。

2）再将root的密码改为root123。



* * *



mysqladmin -u root -p abc123 -password root123;



* * *



增加新用户。 命令格式：grant权限1，权限2，…权限n on数据库名称.表名称to用户名@用户地址identified by’密码‘；，示例如下：

给来自10.163.215.87的用户qiye分配可对数据库company的employee表进行select、insert、update、delete、create、drop等操作的权限，并设定口令为123。



* * *



mysql>grant select,insert,update,delete,create,drop on company.employee to qiye@10.163.215.87 identified by '123';



* * *



显示数据库。 命令格式：show databases。示例如下：



* * *



mysql> show databases;



* * *



备份数据库。 数据库的备份包括数据库的备份、表的备份。格式：mysqldump-h主机名-P端口-u用户名-p密码-database数据库名表名>文件名.sql。示例如下：

1）导出整个数据库。



* * *



mysqldump -u user_name -p123456 database_name > outfile_name.sql



* * *



2）导出一个表。



* * *



mysqldump -u user_name -p123456 database_name table_name > outfile_name.sql



* * *





8.2.3　Python操作MySQL


1.导入MySQLdb数据库模块

在导入MySQLdb之前，需要安装MySQLdb模块。使用pip安装，命令如下：



* * *



pip install MySQL-python



* * *



安装成功后，导入MySQLdb模块：



* * *



import MySQLdb



* * *



2.打开数据库

sqlite3模块使用connect方法打开数据库，方法参数可以为主机ip（host）、用户名（user）、密码（passwd）、数据库名称（db）、端口（port）和编码（charset）。



* * *



con = MySQLdb.connect(host='localhost',user='root',passwd='',db='test', port=3306, charset='utf8')



* * *



3.数据库连接对象

上面通过connect方法返回的con对象，即是数据库连接对象，它提供了以下方法：

·cursor（）方法用来创建一个游标对象。

·commit（）方法用来事务提交。

·rollback（）方法用来事务回滚。

·close（）方法用来关闭一个数据库连接。

4.游标对象的使用

对数据库的查询需要使用到游标对象，首先通过cursor（）方法创建一个游标对象：



* * *



cur = con.cursor()



* * *



游标对象有以下方法支持数据库的操作：

·execute（）用来执行SQL语句。

·executemany（）用来执行多条SQL语句。

·close（）用来关闭游标。

·fetchone（）用来从结果中取一条记录，并将游标指向下一条记录。

·fetchmany（）用来从结果中取多条记录。

·fetchall（）用来从结果中取出所有记录。

·scroll（）用于游标滚动。

5.建表

首先使用游标对象创建一个person表，包含id、name、age等3列，代码如下：



* * *



cur.execute(' CREATE TABLE person (id int not null auto_increment primary key,name varchar(20),age int)')



* * *



6.插入数据

向person表中插入两条数据。插入数据一般有两种做法，第一种做法是直接构造一个插入的SQL语句，代码如下：



* * *



data="'qiye',20" cur.execute(' INSERT INTO person (name,age) VALUES (%s)'%data)



* * *



但是这种做法非常不安全，容易导致SQL注入。另一种做法使用占位符“%s”的方式来规避这个问题，代码如下：



* * *



cur.execute(' INSERT INTO person (name,age) VALUES (%s,%s)',('qiye',20))



* * *



还可以使用executemany（）执行多条SQL语句，使用executemany（）方法比循环使用execute（）方法执行多条SQL语句效率高很多。



* * *



cur.executemany(' INSERT INTO person (name,age) VALUES (%s,%s)',[('marry',20), ('jack',20)])



* * *



这两种方法插入数据都不会立即生效，需要使用数据库对象con进行提交操作：



* * *





con.commit()




* * *



如果出现错误，还可以使用回滚操作：



* * *



con.rollback()



* * *



7.查询数据

查询person表中的所有数据，代码如下：



* * *



cur.execute('SELECT * FROM person')



* * *



要提取查询数据，游标对象提供了fetchall（）和fetchone（）方法。fetchall（）方法获取所有数据，返回一个二维的列表。fetchone（）方法获取其中的一个结果，返回一个元组。使用方法如下：



* * *



cur.execute('SELECT * FROM person') res = cur.fetchall() for line in res: print line cur.execute('SELECT * FROM person') res = cur.fetchone() print res



* * *



8.修改和删除数据



* * *



cur.execute('UPDATE person SET name=%s WHERE id=%s',('rose',1)) cur.execute('DELETE FROM person WHERE id=%s',(0,)) con.commit() con.close()



* * *





8.3　更适合爬虫的MongoDB


MongoDB是一个基于分布式文件存储的数据库，由C++语言编写，旨在为Web应用提供可扩展的高性能数据存储解决方案。和MySQL不同的，MongoDB是一个介于关系数据库和非关系数据库之间的产品，属于非关系数据库，但是非常像关系型数据库。MongoDB功能比较丰富，非常适合在爬虫开发中用作大规模数据的存储。





8.3.1　安装MongoDB


接下来开始进行MongoDB的安装，以Ubuntu和Windows为例。

1.Ubuntu下安装和配置MongoDB

MongoDB提供了Linux平台上32位和64位的安装包，可以在官网进行下载。下载地址：http://www.mongodb.org/downloads ，如图8-13所示。



图8-13　下载页面

下载压缩包，并解压到新建的mongodb目录下：

·mkdir mongodb

·curl-O https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1404-3.2.11.tgz

·tar-zxvf mongodb-linux-x86_64-ubuntu1404-3.2.11.tgz

·mv mongodb-linux-x86_64-ubuntu1404-3.2.11/mongodb/

MongoDB的可执行文件位于mongodb-linux-x86_64-ubuntu1404-3.2.11文件夹下的bin目录下，将bin目录所在路径添加到环境变量中。本人Ubuntu系统所使用的bin目录路径为/home/ubuntu/mongodb/mongodb-linux-x86_64-ubuntu1404-3.2.11/bin。



* * *



export PATH=/home/ubuntu/mongodb/mongodb-linux-x86_64-ubuntu1404-3.2.11/bin:$PATH



* * *



接着需要创建数据库路径，MongoDB的数据存储默认在data目录的db目录下。



* * *



mkdir -p /data/db



* * *



这个时候，进入bin目录，运行mongod服务，效果如图8-14所示。



* * *



sudo ./mongod



* * *





图8-14　启动mongodb服务

如果你创建数据库的目录不是在根目录下，可以使用--dbpath参数指定。示例如下：



* * *



sudo ./mongod --dbpath /home/data/db



* * *



2.Windows下安装和配置MongoDB

下载Windows下的安装包，双击开始安装，设置安装路径为D：\Program Files\MongoDB\　Server\3.3\，安装效果如图8-15所示。



图8-15　安装mongoDB

安装完mongoDB，将mongodb下的bin目录添加到PATH环境变量中。最后配置mongoDB的存储路径，例如在D盘下建立D：\mongodb\data\db目录结构。

当以上的工作都完成后，就可以在命令行中启动mongoDB服务，输入：mongod--dbpath D：\mongodb\data\db，运行效果如图8-16所示。



图8-16　启动mongoDB

大家可以将这mongod--dbpath D：\mongodb\data\db做成一个批处理文件，方便使用。

除了在命令行中启动，还可以将mongoDB注册成一个服务，在系统启动时自动运行。命令格式如下：



* * *



mongod --bind_ip yourIPadress --logpath <logpath> --logappend --dbpath <dbpath> --port yourPortNumber --serviceName "YourServiceName" --serviceDisplayName "YourServiceName" --install



* * *



mongoDB启动的参数说明如表8-10所示。

表8-10　mongoDB启动的参数说明



例如先将命令行窗口以管理员权限启动，并输入命令：mongod--logpath“D：\mongodb\log.txt”--dbpath“D：\mongodb\data\db”--install。注册完成后，接着输入net start mongodb就可以启动服务，效果如图8-17所示。



图8-17　注册MongoDB服务

如果想连接MongoDB进行数据库操作，只需要在命令行中输入mongo，就可以进入shell操作界面，如图8-18所示。



图8-18　MongoDB shell界面





8.3.2　MongoDB基础


MongoDB属于NoSQL数据库，里面的一些概念和MySQL等关系型数据库大不相同。MongoDB中基本的概念是文档、集合、数据库，下面通过表8-11将SQL概念和MongoDB中的概念进行对比。

表8-11　SQL概念和MongoDB中的概念进行对比



1.MongoDB中文档、集合、数据库的概念

文档：文档是MongoDB中数据的基本单元（即BSON），类似关系型数据库中的行。文档有唯一的标识“_id”，数据库可自动生成。文档以key/value的方式，比如{“name”：“qiye”，“age”：20}，可类比数据表的列名，以及列对应的值。下面通过三个不同的文档来说明文档的特性。

·文档1：

·{“name”：“qiye”，“age”：24，“email”：[“qq_email”，“163_email”，“gmail”]，“chat”：{“qq”：“11111”，“weixin”：“11111”}}

·文档2：

·{“Name”：“qiye”，“Age”：24，“email”：[“qq_email”，“163_email”，“gmail”]，“chat”：{“qq”：“11111”，“weixin”：“11111”}}

·文档3：

·{“name”：“qiye”，“email”：[“qq_email”，“163_email”，“gmail”]，“age”：24，“chat”：{“qq”：“11111”，“weixin”：“11111”}}

主要说明三个文档的特性：

·文档的键值对是有序的，顺序不同文档亦不同。

·文档的值可以使字符串、整数、数组以及文档等类型。

·文档的键是用双引号标识的字符串（常见的）；除个别例外，可用任务UTF-8字符。要求如下：键不能含有“\0”（空字符），这个字符用来标识键的结尾；“.”和“$”被保留，存在特别含义，最好不要用来命名键名；以“_”开头的键是保留的，建议不要使用。

·文档区分大小写以及值类型，比如：{“name”：“qiye”，“age”：24}和{“name”：“qiye”，“age”：“

·24”}，{“Name”：“qiye”，“Age”：24}和{“name”：“qiye”，“age”：“24”}都是不同的。





集合： 集合在MongoDB中是一组文档，类似关系型数据库中的数据表。集合存在于数据库中，集合没有固定的结构，这意味着你在集合中可以插入不同格式和类型的数据，比如{“name”：“qiye”，“age”：19}、{“name”：“admin”，“age”：22，“sex”：“1”}，可以存在同一个集合当中。合法的集合名为：


·集合名不能是空字符串。

·集合名不能含有“\0”字符（空字符），这个字符表示集合名的结尾。

·集合名不能以“system.”开头，这是为系统集合保留的前缀。

·用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含保留字符，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现“$”。

数据库： 一个MongoDB中可以建立多个数据库，默认数据库为“db”，该数据库存储在data目录中，这就是我们当时为什么在data目录下创建db文件夹。MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。在MongoDB的shell窗口中，使用show dbs命令可以查看所有的数据库，使用db命令可以看当前的数据库。

2.MongoDB常见数据类型

MongoDB中常用的几种数据类型如表8-12所示。

表8-12　MongoDB中常用的几种数据类型



3.创建/删除数据库

MongoDB创建数据库的语法格式如下：use DATABASE_NAME。如果数据库不存在，则创建数据库，否则切换到指定数据库。如果你想查看所有数据库，可以使用show dbs命令，但是数据库中如果没有数据，就显示不出来。

MongoDB删除数据库的语法格式如下：db.dropDatabase（）。此语句可以删除当前数据库，你可以使用db命令查看当前数据库名。

下面在MongoDB的shell中新建一个名称为pythonSpider的数据库，接着再删除，如图8-19所示。



图8-19　创建/删除数据库

4.集合中文档的增删改查

上面我们已经创建了一个pythonSpider数据库，以下均是在这个数据库中进行操作。文档的数据结构和JSON基本一样，所有存储在集合中的数据都是BSON格式，BSON是类JSON的一种二进制形式的存储格式。

插入文档 。MongoDB使用insert（）或save（）方法向集合中插入文档，语法如下：



* * *



db.COLLECTION_NAME.insert(document)



* * *



示例如下：



* * *



>db.python.insert({title: 'python', description: '动态语言', url: 'http://www.python.org', tags: ['动态', '编程', '脚本'], likes: 100 })



* * *



以上示例中python是我们的集合名称，如果该集合不在该数据库中，MongoDB会自动创建该集合并插入文档。插入的数据必须符合JSON格式。

查询文档。 MongoDB使用find（）方法从集合中查询文档。查询数据的语法格式如下：



* * *



db.COLLECTION_NAME.find()



* * *



如果你需要以易读的方式来读取数据，可以使用pretty（）方法，语法格式如下：



* * *



db.COLLECTION_NAME.find().pretty()



* * *



示例如下：



* * *



> db.python.find()



* * *



上面的代码用于查出python集合中的所有的文档，相当于select*ftom table。如果我们想进行条件操作，就需要了解一下MongoDB中的条件语句和操作符，如表8-13所示。

表8-13　MongoDB中的条件语句和操作符



表中的例子都是单条件操作，下面说一下条件组合，类似于and和or的功能。

MongoDB的find（）方法可以传入多个键（key），每个键以逗号隔开，来实现AND条件。语法格式如下：



* * *



>db. COLLECTION_NAME.find({key1:value1, key2:value2}).pretty()



* * *



查找python集合中likes大于等于100且title等于python的文档，示例如下：



* * *



>db.python.find({"likes": {$gte:100}, "title":"python"}).pretty()



* * *



运行效果为：



* * *



{ "_id" : ObjectId("5833f31a386f0b6ffa7aedf4"), "title" : "python", "description" : "动态语言", "url" : "http://www.python.org", "tags" : [ "动态", "编程", "脚本" ], "likes" : 100 }



* * *



MongoDB OR条件语句使用了关键字“$or”，语法格式如下：



* * *



>db.COLLECTION_NAME.find( { $or: [ {key1: value1}, {key2:value2} ] } ).pretty()



* * *



查找python集合中likes大于等于100或者title等于python的文档，示例如下：



* * *



> db. python.find( { $or: [ {"likes": {$gte:100}}, {"title":"python"} ] } ).pretty()



* * *



MongoDB AND和OR条件可以联合使用，示例如下：



* * *



>db.python.find({"likes": {$gt:50}, $or: [{"description ": "动态语言"},{"title": "python"}]}).pretty()



* * *



更新文档。 MongoDB使用update（）和save（）方法来更新集合中的文档。首先看一下update（）方法，用于更新已经存在的文档，方法原型如下：



* * *



db.collection.update( query, update, { upsert: boolean multi: boolean writeConcern: document } )



* * *



参数分析：

·query：update的查询条件，类似where子句。

·update：update的对象和一些更新的操作符等，类似于set后面的内容。

·upsert：可选，这个参数的意思是如果不存在update的记录，是否插入新的文档，true为插入，默认是false。

·multi：可选，mongodb默认是false，只更新找到的第一条记录，如果这个参数为true，就把按条件查出来多条记录全部更新。

·writeConcern：可选，抛出异常的级别。





我们将title为python的文档修改为title为“python”爬虫，示例如下：




* * *



>db.python.update({'title':'python'},{$set:{'title':'python爬虫'}})



* * *



以上语句只会修改第一条发现的文档，如果你要修改多条相同的文档，则需要设置multi参数为true。示例如下：



* * *



>db.python.update({'title':'python'},{$set:{'title':'python爬虫'}},{multi:true})



* * *



save（）方法通过传入的文档来替换已有文档。方法原型如下：



* * *



db.collection.save( document, { writeConcern: document } )



* * *



参数说明：

·document：文档数据。

·writeConcern：可选，抛出异常的级别。

我们替换一下_id为5833f31a386f0b6ffa7aedf4的文档数据，示例如下：



* * *



>db.python.save( { "_id" : ObjectId("5833f31a386f0b6ffa7aedf4"), "title" : "Mongodb", "description" : "数据库", "url" : "http://www.python.org", "tags" : [ "分布式", "mongo" ], "likes" : 100 } )



* * *



删除文档。 MongoDB提供了remove（）方法来删除文档，函数原型如下：



* * *



db.collection.remove( query, { justOne: boolean, writeConcern: document } )



* * *



参数说明：

·query：可选，删除的文档的条件。

·justOne：可选，如果设为true或1，则只删除一个文档。

·writeConcern：可选，抛出异常的级别。

将刚才更新的文档删除，也就是删除title等于MongoDB的文档，示例如下：



* * *



>db.python.remove({'title':'Mongodb'})



* * *



如果没有query条件，意味着删除所有文档。





8.3.3　Python操作MongoDB


1.导入pymongo数据库模块

在导入pymongo之前，需要安装pymongo模块。使用pip安装，命令如下：



* * *



pip install pymongo



* * *



安装成功后，导入pymongo模块：



* * *



import pymongo



* * *



2.建立连接

pymongo模块使用MongoClient对象来描述一个数据库客户端，创建对象所需的参数主要是host和port。常见的有三种形式：

·client=pymongo.MongoClient（）

·client=pymongo.MongoClient（’localhost‘，27017）

·client=pymongo.MongoClient（’mongodb：//localhost：27017/‘）

第一种方式是连接默认的主机IP和端口，第二种显式指定IP和端口，第三种是采用URL格式进行连接。

3.获取数据库

一个MongoDB实例可以支持多个独立的数据库。使用pymongo时，可以通过访问MongoClient的属性的方式来访问数据库：



* * *



db = client.papers



* * *



如果数据库名字导致属性访问方式不能用（比如pa-pers），可以通过字典的方式访问数据库：



* * *



db = client['pa-pers']



* * *



4.获取一个集合

一个collection指一组存在于MongoDB中的文档，获取Collection方法与获取数据库方法一致：



* * *



collection = db.books



* * *



或者使用字典方式：



* * *



collection = db['books']



* * *



需要强调的一点是，MongoDB里的collection和数据库都是惰性创建的，之前我们提到的所有命令实际并没有对MongoDB Server进行任何操作。直到第一个文档插入后，才会创建，这就是为什么在不插入文档之前，使用show dbs查看不到之前创建的数据库。

5.插入文档

数据在MongoDB中是以JSON类文件的形式保存起来的。在PyMongo中用字典来代表文档，使用insert（）方法插入文档，示例如下：



* * *



book = {"author": "Mike", "text": "My first book!", "tags": ["爬虫", "python", "网络"], "date": datetime.datetime.utcnow() } book_id= collection .insert(book)



* * *



文件被插入之后，如果文件内没有_id这个键值，那么系统自动添加一个到文件里。这是一个特殊键值，它的值在整个collection里是唯一的。insert（）返回这个文件的_id值。

除了单个文件插入，也可以通过给insert（）方法传入可迭代的对象作为第一个参数，进行批量插入操作。这将会把迭代表中的每个文件插入，而且只向Server发送一条命令：



* * *



books = [{"author": "Mike", "text": "My first book!", "tags": ["爬虫", "python", "网络"], "date": datetime.datetime.utcnow() },{"author": "qiye", "text": "My sec book!", "tags": ["hack", "python", "渗透"], "date": datetime.datetime.utcnow() }] books_id = collection.insert(books)



* * *



6.查询文档

MongoDB中最基本的查询就是find_one。这个函数返回一个符合查询的文件，或者在没有匹配的时候返回None。示例如下：



* * *



collection.find_one()



* * *



返回结果是一个之前插入的符合条件的字典类型值。注意，返回的文件里已经有了_id这个键值，是数据库自动添加的。

find_one（）还支持对特定元素进行匹配查询。例如筛选出author为qiye的文档，代码如下：



* * *



collection.find_one({"author": "qiye"})



* * *



通过_id也可以进行查询，book_id就是返回的id对象，类型为ObjectId。示例如下：



* * *



collection.find_one({'_id':ObjectId('58344fcc1123ea2e54cb2e0f')})



* * *



这个常用于Web应用，可以从URL抽取id，从数据库中进行查询。

如果想获取多个文档，可以使用find（）方法。find（）返回一个Cursor实例，通过它我们可以获取每个符合查询条件的文档。示例如下：



* * *



for book in collection.find(): print book



* * *



与使用find_one（）时候相同，可以传入条件来限制查询结果。比如查询所有作者是qiye的书：



* * *



for book in collection.find({"author": "qiye"}): print book



* * *





如果只想知道符合查询条件的文件有多少，可以用count（）操作，而不必进行完整的查询。示例如下：




* * *



collection.find({"author": "qiye"}).count()



* * *



7.修改文档

MongoDB可以使用update（）和save（）方法来更新文档，和之前在MongoDB shell中的操作类似。示例如下：



* * *



collection.update({"author": "qiye"},{"$set":{"text":"python book"}})



* * *



8.删除文档

MongoDB使用remove（）方法来删除文档。示例如下：



* * *



collection.remove({"author": "qiye"})



* * *





8.4　小结


本章讲解了三种数据库的基础操作和Python调用方式，相对比较简单。熟练掌握数据库的操作，对之后的大数据存储非常有益。但是本章的知识只适合初级者的需要，对于亿万级数据的存储和搜索优化，需要大家更多的努力。





第9章　动态网站抓取


前面所讲的都是对静态网页进行抓取，从今天开始开始讲解动态网站的抓取。动态网站的抓取相比静态网页来说困难一些，主要涉及的技术是Ajax和动态Html。简单的网页访问是无法获取完整的数据的，需要对数据加载流程进行分析。下面介绍几种抓取动态网站的方法，基本上都是有利有弊。





9.1　Ajax和动态HTML


对于传统的Web应用，当我们提交一个表单请求给服务器，服务器接收到请求之后，返回一个新的页面给浏览器，这种方式不仅浪费网络带宽，还会极大地影响用户体验，因为原网页和发送请求后获得的新页面两者中大部分的HTML内容是相同的，而且每次用户的交互都需要向服务器发送请求，同时需要对整个网页进行刷新。这种问题的存在催生出了Ajax技术。

Ajax的全称是Asynchronous JavaScript and XML，中文名称定义为异步的JavaScript和XML，是JavaScript异步加载技术、XML以及Dom，还有表现技术XHTML和CSS等技术的组合。使用Ajax技术不必刷新整个页面，只需对页面的局部进行更新，Ajax只取回一些必需的数据，它使用SOAP、XML或者支持JSON的Web Service接口，我们在客户端利用JavaScript处理来自服务器的响应，这样客户端和服务器之间的数据交互就减少了，访问速度和用户体验都得到了提升。

DHTML是Dynamic HTML的简称，就是动态的HTML，是相对传统的静态HTML而言的一种制作网页的概念。所谓动态HTML（Dynamic HTML，简称DHTML），其实并不是一门新的语言，它只是HTML、CSS和客户端脚本的一种集成，即一个页面中包括HTML+CSS+JavaScript（或其他客户端脚本）。DHTML不是一种技术、标准或规范，只是一种将目前已有的网页技术、语言标准整合运用，制作出能实时变换页面元素效果的网页设计概念。比如，当鼠标移至文章段落中，段落能够变成蓝色，或者当你点击一个超链后会自动生成一个下拉式的子超链目录。

如何判断要爬取的网站是动态网站还是静态网站呢？一个比较简单做法，是看看有没有“查看更多”这样的字样，一般有这样的字样差不多是动态网站。当然，这种做法太经验化了，其实更准确的做法是当你使用Requests访问一个网页，返回的Response内容和在浏览器上看的HTML内容不一样时，不要奇怪，这就是用了动态技术，这就是为什么你无法从响应中抽取出有效的数据。

那怎么解决这个问题呢？一般有两种做法：一种是直接从JavaScript中采集加载的数据，另一种方式是直接采集浏览器中已经加载好的数据。接下来，我会一一进行讲解。





9.2　动态爬虫1：爬取影评信息


接下来就以MTime电影网（www.mtime.com ）为例进行分析。首先先判断一下是不是动态网站，使用Firefox浏览器访问http://movie.mtime.com/217130/ 其中一部电影，打开Firebug，监听网络，如图9-1所示。



图9-1　MTime电影网

在网络响应中搜索“票房”是搜索不到的，但是在网页中确实显示了票房是多少，这基本上可以确定使用了动态加载技术。这个时候我们需要做的是找出哪个JavaScript文件进行了加载请求。将Firebug中网络选项的JavaScript分类选中，然后查看一下包含敏感内容的链接，比如含有Ajax字符串。如图9-2所示，在一个链接http://service.library.mtime.com/Movie.apiAjax_CallBack=true&Ajax_CallBackType=Mtime.Library.Services&Ajax_CallBackMethod=GetMovieOverviewRating&Ajax_CrossDomain=1&Ajax_RequestUrl=http%3A%2F%2Fmovie.mtime.com%2F217130%2F&t=2016111321341844484&Ajax_CallBackArgument0=217130 中，找到了评分、票房的信息。



图9-2　Ajax链接

找到了我们所需要的链接和响应内容，接下来需要做两件事情，第一件事是如何构造这样的链接，链接中的参数有什么特征，第二件事是如何提取响应信息的内容，为我所用。

在http://service.library.mtime.com/Movie.apiAjax_CallBack=true&Ajax_CallBackType=Mtime.Library.Services&Ajax_CallBackMethod=GetMovieOverviewRating&Ajax_CrossDomain=1&Ajax_RequestUrl=http%3A%2F%2Fmovie.mtime.com%2F217130%2F&t=2016111321341844484&Ajax_CallBackArgument0=217130 这个GET请求中，总共有7个参数，这些参数中哪些是变化的？哪些是不变化的？我们首先要确定一下。最有效的办法就是从另外的一部电影的访问请求中找到加载票房和评分的链接，进行一下对比。比如我访问http://movie.mtime.com/108737/ 这个网页，动态加载票房的链接为：http://service.library.mtime.com/Movie.apiAjax_CallBack=true&Ajax_CallBackType=Mtime.Library.Services&Ajax_CallBackMethod=GetMovieOverviewRating&Ajax_CrossDomain=1&Ajax_RequestUrl=http%3A%2F%2Fmovie.mtime.com%2F108737%2F&t=201611132231493282&Ajax_CallBackArgument0=108737 。通过对比，我们可以发现只有Ajax_RequestUrl、t和Ajax_CallBackArgument0这三个参数是变化的。通过分析，还会发现Ajax_RequestUrl是当前网页的链接，Ajax_CallBackArgument0是http://movie.mtime.com/108737/ 链接中的数字，t为当前的时间。知道以上信息，我们就可以构造一个获取票房和评分的链接了。

最后要提取响应中的内容，首先看一下响应内容的格式。响应内容主要分三种，一种是正在上映的电影信息，一种是即将上映的电影信息，最后一种是还有较长时间才能上映的电影信息。

正在上映的电影信息格式如下：



* * *



var result_201611132231493282 = { "value":{"isRelease":true,"movieRating":　{"MovieId":108737,"RatingFinal":7.7,"RDirectorFinal":7.7,"ROtherFinal":7,"RPictureFinal":8.4,"RShowFinal":10,"RStoryFinal":7.3,"RTotalFinal":10,"Usercount":4067,"AttitudeCount":4300,"UserId":0,"EnterTime":0,"JustTotal":0,"RatingCount":0,"TitleCn":"","TitleEn":"","Year":"","IP":0},"movieTitle":"奇异博士","tweetId":0,"userLastComment　":"","userLastCommentUrl":"","releaseType":1,"boxOffice":{"Rank":1,"TotalBoxOffice":"5.66","TotalBoxOfficeUnit":"亿","TodayBoxOffice":"4776.8","TodayBoxOfficeUnit":"万","ShowDays":10,"EndDate":"2016-11-13 22:00","FirstDayBoxOffice":"8146.21","　FirstDayBoxOfficeUnit":"万"}},"error":null};var movieOverviewRatingResult=result_　201611132231493282;



* * *



即将上映的电影信息格式如下：



* * *



var result_2016111414381839596 ={ "value":{"isRelease":true,"movieRating":{"Mo vieId":229639,"RatingFinal":-1,"RDirectorFinal":0,"ROtherFinal":0,"RPictureFinal":0,"RShowFinal":0,"RStoryFinal":0,"RTotalFinal":0,"Usercount":130,"AttitudeCount":2119,"UserId":0,"EnterTime":0,"JustTotal":0,"RatingCount":0,"TitleCn":"","TitleEn":"","Year":"","IP":0},"movieTitle":"我不是潘金莲 ","tweetId":0,"userLastComment":"","userLastCommentUrl":"","releaseType":2,"hotValue":{"MovieId":229639,"Ranking":1,"Changing":0,"YesterdayRanking":1}},"error":null};var movieOverviewRatingResult=result_2016111414381839596;



* * *



还有较长时间才能上映的电影信息格式如下：



* * *



var result_201611141343063282 = { "value":{"isRelease":false,"movieRating": {"MovieId":236608,"RatingFinal":-1,"RDirectorFinal":0,"ROtherFinal":0, "RPictureFinal":0,"RShowFinal":0,"RStoryFinal":0,"RTotalFinal":0, "Usercount":5,"AttitudeCount":19,"UserId":0,"EnterTime":0, "JustTotal":0,"RatingCount":0,"TitleCn":"","TitleEn":"","Year":"", "IP":0},"movieTitle":"江南灵异录之白云桥","tweetId":0, "userLastComment":"","userLastCommentUrl":"","releaseType":2, "hotValue":{"MovieId":236608,"Ranking":53,"Changing":4, "YesterdayRanking":57}},"error":null}; var movieOverviewRatingResult=result_201611141343063282;



* * *



这三种格式的区别只是多了或者少了一些字段，需要在异常处理时加一些判断。

“=”和“；”之间的内容是一个标准的JSON格式，我们要提取的字段含义如表9-1所示。

表9-1　字段的定义





确定了链接和提取字段，接下来写一个动态爬虫来爬取电影的评分和票房信息。

1.网页下载器

网页下载器的实现方式和第6章的一样，代码如下：



* * *





# coding:utf-8 import requests class HtmlDownloader(object): def download(self,url): if url is None: return None user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers={'User-Agent':user_agent} r = requests.get(url,headers=headers) if r.status_code==200: r.encoding='utf-8' return r.text return None




* * *



2.网页解析器

网页解析器中主要包括两个部分，一个是从当前网页中提取所有正在上映的电影链接，另一个是从动态加载的链接中提取我们所需的字段。

提取当前正在上映的电影链接，使用正则表达式，电影页面链接类似http://movie.mtime.com/17681/ 这个样子，正则表达式可以写成如下的样子进行匹配：

http://movie.mtime.com/\d+/ 。在HtmlParser类定义一个parser_url方法，代码如下：



* * *



def parser_url(self,page_url,response): pattern = re.compile(r'(http://movie.mtime.com/(\d+)/)') urls = pattern.findall(response) if urls!=None : # 将urls进行去重 return list(set(urls)) else: return None



* * *



接着从动态加载的链接中提取我们所需的字段，首先使用正则表达式取出“=”和“；”之间的内容，接着就可以使用JSON模块进行处理了。下面只需要提取不同格式的信息，其中parser_json为主方法，负责解析响应，同时又使用了两个辅助方法_parser_no_release和_parser_release。代码如下：



* * *



def parser_json(self,page_url,response): ''' 解析响应 :param response: :return: ''' # 将“=”和“；”之间的内容提取出来 pattern = re.compile(r'=(.*);') result = pattern.findall(response)[0] if result!=None: # json模块加载字符串 value = json.loads(result) try: isRelease = value.get('value').get('isRelease') except Exception,e: print e return None if isRelease: if value.get('value').get('hotValue')==None: return self._parser_release(page_url,value) else: return self._parser_no_release(page_url,value,isRelease=2) else: return self._parser_no_release(page_url,value) def _parser_release(self,page_url,value): ''' 解析已经上映的影片 :param page_url:电影链接 :param value:json数据 :return: ''' try: isRelease = 1 movieRating = value.get('value').get('movieRating') boxOffice = value.get('value').get('boxOffice') movieTitle = value.get('value').get('movieTitle') RPictureFinal = movieRating.get('RPictureFinal') RStoryFinal = movieRating.get('RStoryFinal') RDirectorFinal = movieRating.get('RDirectorFinal') ROtherFinal = movieRating.get('ROtherFinal') RatingFinal = movieRating.get('RatingFinal') MovieId = movieRating.get('MovieId') Usercount = movieRating.get('Usercount') AttitudeCount = movieRating.get('AttitudeCount') TotalBoxOffice = boxOffice.get('TotalBoxOffice') TotalBoxOfficeUnit = boxOffice.get('TotalBoxOfficeUnit') TodayBoxOffice = boxOffice.get('TodayBoxOffice') TodayBoxOfficeUnit = boxOffice.get('TodayBoxOfficeUnit') ShowDays = boxOffice.get('ShowDays') try: Rank = boxOffice.get('Rank') except Exception,e: Rank=0 # 返回所提取的内容 return (MovieId,movieTitle,RatingFinal, ROtherFinal,RPictureFinal,RDirectorFinal, RStoryFinal,Usercount,AttitudeCount, TotalBoxOffice+TotalBoxOfficeUnit, TodayBoxOffice+TodayBoxOfficeUnit, Rank,ShowDays,isRelease ) except Exception,e: print e,page_url,value return None def _parser_no_release(self,page_url,value,isRelease = 0): ''' 解析未上映的电影信息 :param page_url: :param value: :return: ''' try: movieRating = value.get('value').get('movieRating') movieTitle = value.get('value').get('movieTitle') RPictureFinal = movieRating.get('RPictureFinal') RStoryFinal = movieRating.get('RStoryFinal') RDirectorFinal = movieRating.get('RDirectorFinal') ROtherFinal = movieRating.get('ROtherFinal') RatingFinal = movieRating.get('RatingFinal') MovieId = movieRating.get('MovieId') Usercount = movieRating.get('Usercount') AttitudeCount = movieRating.get('AttitudeCount') try: Rank = value.get('value').get('hotValue').get('Ranking') except Exception,e: Rank = 0 return (MovieId,movieTitle,RatingFinal, ROtherFinal,RPictureFinal,RDirectorFinal, RStoryFinal, Usercount,AttitudeCount,u'无', u'无',Rank,0,isRelease ) except Exception,e: print e,page_url,value return None



* * *



3.数据存储器

数据存储器将返回的数据插入sqlite数据库中，主要包括建表，插入和关闭数据库等操作，表中设置了15个字段，用来存储电影信息。代码如下：



* * *





import sqlite3 class DataOutput(object): def __init__(self): self.cx = sqlite3.connect("MTime.db") self.create_table('MTime') self.datas=[] def create_table(self,table_name): ''' 创建数据表 :param table_name:表名称 :return: ''' values = ''' id integer primary key, MovieId integer, MovieTitle varchar(40) NOT NULL, RatingFinal REAL NOT NULL DEFAULT 0.0, ROtherFinal REAL NOT NULL DEFAULT 0.0, RPictureFinal REAL NOT NULL DEFAULT 0.0, RDirectorFinal REAL NOT NULL DEFAULT 0.0, RStoryFinal REAL NOT NULL DEFAULT 0.0, Usercount integer NOT NULL DEFAULT 0, AttitudeCount integer NOT NULL DEFAULT 0, TotalBoxOffice varchar(20) NOT NULL, TodayBoxOffice varchar(20) NOT NULL, Rank integer NOT NULL DEFAULT 0, ShowDays integer NOT NULL DEFAULT 0, isRelease integer NOT NULL ''' self.cx.execute('CREATE TABLE IF NOT EXISTS %s( %s ) '%(table_name,　values)) def store_data(self,data): ''' 数据存储 :param data: :return: ''' if data is None: return self.datas.append(data) if len(self.datas)>10: self.output_db('MTime') def output_db(self,table_name): ''' 将数据存储到sqlite :return: ''' for data in self.datas: self.cx.execute("INSERT INTO %s (MovieId,MovieTitle," "RatingFinal,ROtherFinal,RPictureFinal," "RDirectorFinal,RStoryFinal, Usercount," "AttitudeCount,TotalBoxOffice,TodayBoxOffice," "Rank,ShowDays,isRelease) VALUES (,,,,,,,,,,,,,) " ""%table_name,data) self.datas.remove(data) self.cx.commit() def output_end(self): ''' 关闭数据库 :return: ''' if len(self.datas)>0: self.output_db('MTime') self.cx.close()




* * *



4.爬虫调度器

爬虫调度器的工作主要是协调以上模块，同时还负责AJax动态链接的构造。代码如下：



* * *



class SpiderMan(object): def __init__(self): self.downloader = HtmlDownloader() self.parser = HtmlParser() self.output = DataOutput() def crawl(self,root_url): content = self.downloader.download(root_url) urls = self.parser.parser_url(root_url,content) # 构造一个获取评分和票房链接 for url in urls: try: t = time.strftime("%Y%m%d%H%M%S3282", time.localtime()) rank_url ='http://service.library.mtime.com/Movie.api' \ 'Ajax_CallBack=true' \ '&Ajax_CallBackType=Mtime.Library.Services' \ '&Ajax_CallBackMethod=GetMovieOverviewRating' \ '&Ajax_CrossDomain=1' \ '&Ajax_RequestUrl=%s' \ '&t=%s' \ '&Ajax_CallBackArgument0=%s'%(url[0],t,url[1]) rank_content = self.downloader.download(rank_url) data = self.parser.parser_json(rank_url,rank_content) self.output.store_data(data) except Exception,e: print "Crawl failed" self.output.output_end() print "Crawl finish" if __name__=='__main__': spider = SpiderMan() spider.crawl('http://theater.mtime.com/China_Beijing/')



* * *



当以上四个模块都完成后，启动爬虫。由于数据量小，大约一分钟后，爬取结束。在shell中使用sqlite命令，查看爬取的结果，如图9-3所示。



图9-3　sqlite查询结果

如果不习惯使用shell来查询，可以使用GUI版查看器SqliteBrowser进行查询，如图9-4所示。



图9-4　SqliteBrowser查询结果





9.3　PhantomJS


9.2节讲了直接从JavaScript中采集加载的数据的方法，本节进行讲解第二种方法，即直接从浏览器中提取渲染好的HTML文档。如果Ajax请求很多，有时请求参数还进行了加密，我们手动分析每一个Ajax请求，将成为一项繁重的工作，而且没有一定的JavaScript分析功底，很难做到。这个时候第二种方法的好处就体现出来了，直接提取浏览器渲染好的结果，不进行Ajax请求分析，PhantomJS就是这样的一个浏览器。

PhantomJS是一个基于WebKit的服务器端JavaScript API。它全面支持Web而无需浏览器支持，不仅运行快，原生支持各种Web标准：DOM处理、CSS选择器、JSON、Canvas，和SVG。PhantomJS可以用于页面自动化、网络监测、网页截屏，以及无界面测试等。PhantomJS可以看做一个没有界面的浏览器，它既有Firefox浏览器、google浏览器的功能，又因为没有界面而更加快速，占更小的内存，在爬虫开发中非常受欢迎。





9.3.1　安装PhantomJS


PhantomJS安装方法有两种，一种是下载源码之后自行编译，另一种是直接下载编译好的二进制文件，官方推荐直接使用编译好的二进制文件。安装下载地址为：http://phantomjs.org/download.html ，包括Windows、Mac OS、Linux版本，自行选择对应版本下载解压即可，建议为PhantomJS设置环境变量。在下载的安装包中，其中有一个example文件夹，里面有很多官方的例子可供学习和参考。

安装完成后在命令行中输入：phantomjs-v。如果正常显示版本号，则证明安装配置成功。图9-5为Windows下的显示结果。



图9-5　phantomJS版本





9.3.2　快速入门


配置完成PhantomJS，下面使用它输出“hello world”。新建一个JavaScript文件hello.js，代码内容为：



* * *



console.log('Hello, world!'); phantom.exit();



* * *



这时候在命令行中输入：



* * *



phantomjs hello.js



* * *



输出内容为：Hello，world！。代码中的第一句是在控制台输出“Hello，world！”，第二句是终止phantom的运行，不然程序会一直运行，不会停止。

通过上面的小例子我们已经了解了PhantomJS的基本操作，PhantomJS还有一些有趣而且强大功能。

1.页面加载

通过PhantomJS，一个网页可以被加载、分析和通过创建网页对象呈现。下面演示一个简单的页面加载的例子，访问我的博客园地址：http://www.cnblogs.com/qiyeboy/ ，并将当前页面进行截图保存。pageload.js代码如下：



* * *



var page = require('webpage').create(); page.open('http://www.cnblogs.com/qiyeboy/', function(status) { console.log("Status: " + status); if(status === "success") { page.render('qiye.png'); } phantom.exit(); });



* * *



在命令行中运行：



* * *



phantomjs pageload.js



* * *



输出内容为：Status：success，并在当前目录下生成对网页的截图qiye.png，如图9-6所示。



图9-6　qiye.png

代码解释：首先使用webpage模块创建一个page对象，然后通过page对象打开http://www.cnblogs.com/qiyeboy/ 网址，如果请求响应状态为success，则通过render方法将当前页面保存为qiye.png图片。

除了打开网页截图之外，还可以对网页的打开进行测速。下面的例子用来计算一个网页的加载速度，同时还用到了给JavaScript脚本传递参数的功能。loadspeed.js代码如下：



* * *



var page = require('webpage').create(), system = require('system'), t, address; if (system.args.length === 1) { console.log('Usage: loadspeed.js <some URL>'); phantom.exit(); } t = Date.now(); address = system.args[1]; page.open(address, function(status) { if (status !== 'success') { console.log('FAIL to load the address'); } else { t = Date.now() - t; console.log('Loading ' + system.args[1]); console.log('Loading time ' + t + ' msec'); } phantom.exit(); });



* * *



在命令行中输入：



* * *



phantomjs loadspeed.js http://www.cnblogs.com/qiyeboy/



* * *



输出结果为：



* * *



Loading http://www.cnblogs.com/qiyeboy/ Loading time 793 msec



* * *



代码解释：首先使用webpage模块创建一个page对象，使用system模块获取系统对象system，并声明了两个变量t和address，用来保存时间和传入参数。如果传入参数的长度等于1，说明要加载的地址没有传入，进行提示并退出phantom。为什么要等于1呢？因为phantomjs loadspeed.js第一个参数是loadspeed.js。接着获取当前的时间，然后打开网页，获取加载完成后的时间，进行相减即可。

2.代码评估

为了评估网页中的JavaScript代码，可以利用evaluate。这个执行是“沙盒式”的，它不会去执行网页外的JavaScript代码。evaluate方法可以返回一个对象，然而返回值仅限于对象，不能包含函数（或闭包）。比如我们可以使用evaluate方法获取http://www.cnblogs.com/qiyeboy/ 页面的标题，evaluate.js代码如下：



* * *



var url = 'http://www.cnblogs.com/qiyeboy/'; var page = require('webpage').create(); page.open(url, function(status) { var title = page.evaluate(function() { return document.title; }); console.log('Page title is ' + title); phantom.exit(); });



* * *



在命令行中输入：



* * *



phantomjs evaluate.js



* * *



输出结果为：



* * *



Page title is七夜的故事 - 博客园



* * *



任何来自于网页并且包括来自evaluate（）内部代码的控制台信息，默认不会显示。要覆盖此行为，使用onConsoleMessage回调方法。将evaluate.js代码改动如下：



* * *



var url = 'http://www.cnblogs.com/qiyeboy/'; var page = require('webpage').create(); page.onConsoleMessage = function(msg) { console.log('Page title is ' + msg); }; page.open(url, function(status) { page.evaluate(function() { console.log(document.title); }); phantom.exit(); });



* * *



在命令行中输入：



* * *



phantomjs evaluate.js



* * *





输出结果为：




* * *



Page title is七夜的故事 - 博客园



* * *





9.3.3　屏幕捕获


上节简单讲解了如何将网页保存为一张图片，下面详细解释一下这个屏幕捕获的功能。由于PhantomJS使用的是WebKit内核，一个真正的布局和渲染引擎，它可以捕捉一个网页的屏幕截图。另外PhantomJS可以渲染网页上的元素，所以它不仅可以用于HTML和CSS的内容转换，还可以用于SVG和画布。PhantomJS不仅可以将网页保存为png格式，还可以保存为jpg、gif和pdf格式。下面将pageload.js代码进行改动，转成pdf格式，代码如下：



* * *



var page = require('webpage').create(); page.open('http://www.cnblogs.com/qiyeboy/', function(status) { console.log("Status: " + status); if(status === "success") { page.render('qiye.pdf'); } phantom.exit(); });



* * *



最后生成的pdf文件，效果如图9-7所示。



图9-7　qiye.pdf

PhantomJS不仅可以将页面转化为不同的文件格式，还可以对视图进行缩放和裁剪，主要用到page对象中两个非常重要的属性：viewportSize和clipRect。viewportSize是视区的大小，其作用可以看做是将打开的浏览器窗口进行缩放。clipRect是在这个视区中裁剪矩形的大小，需要四个参数，前两个是基准点，后两个参数是宽高。下面将pageload.js进行改动，代码如下：



* * *



var page = require('webpage').create(); page.viewportSize = { width: 1024, height: 768 }; page.clipRect = { top: 0, left: 0, width: 512, height: 256 }; page.open('http://www.cnblogs.com/qiyeboy/', function(status) { console.log("Status: " + status); if(status === "success") { page.render('qiye.png'); } phantom.exit(); });



* * *



效果如图9-8所示，只是截取出了顶端一角。



图9-8　网页裁剪





9.3.4　网络监控


因为PhantomJS允许检验网络流量，因此它适合分析网络行为和性能，实现对网络的监听。当向远程服务器发送请求时，可以使用onResourceRequested和onResourceReceived两个方法嗅探所有的资源请求和响应。示例netmonitor.js代码如下：



* * *



var url = 'http://www.cnblogs.com/qiyeboy/'; var page = require('webpage').create(); page.onResourceRequested = function(request) { console.log('Request ' + JSON.stringify(request, undefined, 4)); }; page.onResourceReceived = function(response) { console.log('Receive ' + JSON.stringify(response, undefined, 4)); }; page.open(url);



* * *



在命令行中输入：



* * *



phantomjs netmonitor.js



* * *



请求和响应的信息会以JSON的格式进行显示，效果如图9-9所示。



图9-9　网络监控





9.3.5　页面自动化


PhantomJS可以加载和处理一个网页，非常适用于自动化处理，PhantomJS中标准JavaScript的DOM操作和CSS选择器都是生效的。下面使用一个小例子讲解一下DOM操作，获取MTime时光网的影评信息，HTML标记位置如图9-10所示。



图9-10　评分和票房标记

示例代码如下：



* * *



var page = require('webpage').create(); console.log('The default user agent is ' + page.settings.userAgent); page.settings.userAgent = 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0'; page.open('http://movie.mtime.com/108737/', function(status) { if (status !== 'success') { console.log('Unable to access network'); } else { var ua = page.evaluate(function() { return document.getElementById('ratingRegion').textContent; }); console.log(ua); } phantom.exit(); });



* * *



输出结果如下：



* * *



The default user agent is Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/　538.1(KHTML, like Gecko) PhantomJS/2.1.1 Safari/538.1 7.7总分：104,335人评分 4,299人想看音乐 画面 导演 故事 …票房：5.92亿元



* * *



代码解释：首先创建page对象，接着将默认的User-Agent进行了修改，打开指定网页，当加载完成之后，执行DOM操作，获取id为ratingRegion元素下的内容，并打印出来。

大家可以看一下默认UserAgent的内容，会发现里面包含了PhantomJS关键字，一些网站就是通过这个关键字来识别是否正在使用PhantomJS爬取数据。

在1.6版本之后PhantomJS允许添加外部的JS库，比如下面的例子添加了jQuery，然后执行了jQuery代码。



* * *



var page = require('webpage').create(); page.open('http://www.sample.com', function() { page.includeJs("http://ajax.googleapis.com/ajax/libs/jquery/1.6.1/jquery.min .js", function() { page.evaluate(function() { $("button").click(); }); phantom.exit() }); });



* * *





9.3.6　常用模块和方法


上面的例子中我们用到了phantom、webpage和system模块，在这三个模块基础上再讲一个fs模块。

1.phantom

对于phantom，主要讲解其中的五个方法，如表9-2所示。

表9-2　phantom方法



2.webpage

对于webpage，主要说一下includeJs、open两个普通方法，onInitialized、onLoadFinished两个回调方法。

includeJs方法原型为includeJs（url，callback）{void}，功能是包含从指定的URL获取远程javaScript脚本，并执行回调方法。示例代码如下：



* * *



var webPage = require('webpage'); var page = webPage.create(); page.includeJs( // Include the https version, you can change this to http if you like. 'https:// ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js', function() { (page.evaluate(function() { // jQuery is loaded, now manipulate the DOM var $loginForm = $('form# login'); $loginForm.find('input[name="username"]').value('phantomjs'); $loginForm.find('input[name="password"]').value('c45p3r'); })) } );



* * *





open方法比较复杂，有四种函数重载方式，分别为open（url，callback）{void}、open（url，method，callback）{void}、open（url，method，data，callback）{void}、open（url，settings，callback）{void}。open（url，callback）方法之前已经用过，第二种和第三种方式类似，所以下面主要说一下后两种形式。


open（url，method，data，callback）中url为链接，method为GET或者POST请求，data为附加的数据，callback为回调函数。示例如下，用于发送一个POST请求。



* * *



var webPage = require('webpage'); var page = webPage.create(); var postBody = 'user=username&password=password'; page.open('http://www.google.com/', 'POST', postBody, function(status) { console.log('Status: ' + status); // Do other things here... });



* * *



open（url，settings，callback）中url为链接，setting为对请求头和内容的设置，callback为回调函数。示例如下：



* * *



var webPage = require('webpage'); var page = webPage.create(); var settings = { operation: "POST", encoding: "utf8", headers: { "Content-Type": "application/json" }, data: JSON.stringify({ some: "data", another: ["custom", "data"] }) }; page.open('http://your.custom.api', settings, function(status) { console.log('Status: ' + status); // Do other things here... });



* * *



onInitialized是回调方法，在webpage对象被创建之后，url被加载之前被调用，主要是用来操作一些全局变量。示例代码如下：



* * *



var webPage = require('webpage'); var page = webPage.create(); page.onInitialized = function() { page.evaluate(function() { document.addEventListener('DOMContentLoaded', function() { console.log('DOM content has loaded.'); }, false); }); };



* * *



onLoadFinished是回调方法，在页面加载完成之后调用，方法还有一个参数status。如果加载成功status为success，否则为fail。webpage中open方法就是用这个方法作为回调函数。示例代码如下：



* * *



var webPage = require('webpage'); var page = webPage.create(); page.onLoadFinished = function(status) { console.log('Status: ' + status); // Do other things here... };



* * *



3.system

system模块只有属性，没有方法。下面通过表9-3列举一下system的属性及其含义。

表9-3　system属性





4.fs

fs模块全称为File System，主要是对文件系统进行操作。该模块方法很多，这里主要讲解创建文件、判断文件是否存在、读写文件的方法，如表9-4所示。

表9-4　fs方法



以上介绍了一些常用模块和方法，如果大家想详细了解相关内容，可以去phantom官网（http://phantomjs.org/api/ ）查看完整的API文档。





9.4　Selenium


上一节我们讲解了PhantomJS的用法，它只是一个没有界面的浏览器，运行的还是JavaScript脚本，这和Python爬虫开发有什么联系呢？本节介绍的Selenium能将Python和PhantomJS紧密地联系起来，从而实现爬虫的开发。

Selenium是一个自动化测试工具，支持各种浏览器，包括Chrome、Safari、Firefox等主流界面式浏览器，也包括PhantomJS等无界面浏览器，通俗来说Selenium支持浏览器驱动，可以对浏览器进行控制。而且Selenium支持多种语言开发，比如Java、C、Ruby，还有Python，因此Python+Selenium+PhantomJS的组合就诞生了。PhantomJS负责渲染解析JavaScript，Selenium负责驱动浏览器和与Python对接，Python负责做后期处理，三者构成了一个完整的爬虫结构。





9.4.1　安装Selenium


Selenium现在最新的版本为3.0.1，本书也是以此为标准进行讲解。Selenium官方地址为：http://www.seleniumhq.org/ ，其安装主要有两种方式：

·pip install Selenium==3.0.1

·从https://pypi.python.org/pypi/selenium 下载源代码解压后，运行python setup.py install。

·Selenium3.x和Selenium2.x版本有以下区别：

·Selenium2.x调用高版本浏览器会出现不兼容问题，调用低版本浏览器正常。

·Selenium3.x调用浏览器必须下载一个类似补丁的文件，比如Firefox的为geckodriver，Chrome的为chromedriver。

各种版本浏览器的补丁下载地址为：http://www.seleniumhq.org/download/ ，如图9-11所示。



图9-11　补丁下载地址

根据自己的操作系统，下载指定的geckodriver文件。下面以Firefox为例，对geckodriver进行配置。在ubuntu下，将文件下载下来之后解压到指定目录下，我把它解压到firefoxDriver目录下，如图9-12所示。



图9-12　补丁解压位置

接着配置环境变量，在shell中执行：export PATH=$PATH：/home/ubuntu/firefoxDr-iver，将geckodriver所在的目录配置到环境变量中，其他操作系统配置方式类似。





9.4.2　快速入门


安装和配置完成后，现在开始使用Selenium写一个小例子，功能是打开百度主页，在搜索框中输入网络爬虫，进行搜索。代码如下：



* * *



from selenium import webdriver from selenium.webdriver.common.keys import Keys import time driver = webdriver.Firefox() driver.get("http://www.baidu.com") assert u"百度" in driver.title elem = driver.find_element_by_name("wd") elem.clear() elem.send_keys(u"网络爬虫") elem.send_keys(Keys.RETURN) time.sleep(3) assert u"网络爬虫." not in driver.page_source driver.close()



* * *



效果如图9-13所示。



图9-13　搜索网络爬虫

代码分析：首先使用webdriver.Firefox（）获取Firefox浏览器的驱动，调用get方法，打开百度首页，判断标题中是否包含百度字样，接着通过元素名称wd获取输入框，通过send_keys方法将网络爬虫填写其中，然后回车。延时3秒后，判断搜索页面是否有网络爬虫字样，最后关闭driver。

我相信即使是同样的代码，大家也会遇到各种各样的问题。下面将大家可能遇到的问题进行一下总结：

1）错误信息为：Exception AttributeError：“’Service‘object has no attribute’process‘”in...，可能是geckodriver环境变量有问题，重新将geckodriver所在目录配置到环境变量中。或者直接在代码中指定路径：



* * *



webdriver.Firefox（executable_path=’/home/ubuntu/firefoxDriver/geckodriver‘）



* * *



2）错误信息为：selenium.common.exceptions.WebDriverException：Message：Unsupported Marionette protocol version 2，required 3，可能是Firefox版本太低，使用Selenium3.x要求Firefox>=v47。

3）错误信息为：selenium.common.exceptions.WebDriverException：Message：Failed to start browser，可能是没找到Firefox浏览器，可以在代码中指定Firefox的位置：



* * *



binary = FirefoxBinary(r'E:\Mozilla Firefox\firefox.exe') driver = webdriver.Firefox(firefox_binary=binary)



* * *





9.4.3　元素选取


要想对页面进行操作，首先要做的是选中页面元素。元素选取方法如表9-5所示。

表9-5　定位方法



除了上面具有确定功能的方法，还有两个通用方法find_element和find_elements，可以通过传入参数来指定功能。示例如下：



* * *



from selenium.webdriver.common.by import By driver.find_element(By.XPATH, '// button[text()="Some text"]')



* * *



这一个例子是通过xpath表达式来查找，方法中第一个参数是指定选取元素的方式，第二个参数是选取元素需要传入的值或表达式。第一个参数还可以传入By类中的以下值：

·By.ID

·By.XPATH

·By.LINK_TEXT

·By.PARTIAL_LINK_TEXT

·By.NAME

·By.TAG_NAME

·By.CLASS_NAME

·By.CSS_SELECTOR

下面通过一个HTML文档来讲解一下如何使用以上方法提取内容，HTML文档如下：



* * *



<html> <body> <h1>Welcome</h1> <p class="content">用户登录</p> <form id="loginForm"> <input name="username" type="text" /> <input name="password" type="password" /> <input name="continue" type="submit" value="Login" /> <input name="continue" type="button" value="Clear" /> </form> <a href="register.html">Register</a> </body> <html>



* * *



定位方法的使用如表9-6所示。

表9-6　定位方法示例





9.4.4　页面操作


以如下HTML文档为例介绍页面操作，login.html代码如下：



* * *





<html> <head> <meta http-equiv="content-type" content="text/html;charset=gbk"> </head> <body> <h1>Welcome</h1> <p class="content">用户登录</p> <form id="loginForm"> <select name="loginways"> <option value="email">邮箱</option> <option value="mobile">手机号</option> <option value="name">用户名</option> </select> <br/> <input name="username" type="text" /> <br/> 密码 <br/> <input name="password" type="password" /> <br/><br/> <input name="continue" type="submit" value="Login" /> <input name="continue" type="button" value="Clear" /> </form> <a href="register.html">Register</a> </body> </html>




* * *



效果如图9-14所示。



图9-14　登录页面

1.页面交互与填充表单

第一步： 初始化Firefox驱动，打开html文件，由于是本地文件，可以使用下面方式打开。



* * *



driver = webdriver.Firefox() driver.get("file:// /e:/login.html")



* * *



第二步： 获取用户名和密码的输入框，和登录按钮。



* * *



username = driver.find_element_by_name('username') password = driver.find_element_by_xpath(".// *[@id='loginForm']/input[2]") login_button = driver.find_element_by_xpath("// input[@type='submit']")



* * *



第三步： 使用send_keys方法输入用户名和密码，使用click方法模拟点击登录。



* * *



username.send_keys("qiye") password.send_keys("qiye_pass") login_button.click()



* * *



如果想清除username和password输入框的内容，可以使用clear方法。



* * *



username.clear() password.clear()



* * *



上面还有一个问题没解决，如何操作下拉选项卡选择登录方式呢？第一种方法代码如下：



* * *



select = driver.find_element_by_xpath("// form/select") all_options = select.find_elements_by_tag_name("option") for option in all_options: print("Value is: %s" % option.get_attribute("value")) option.click()



* * *



在代码中首先获取select元素，也就是下拉选项卡。然后轮流设置了select选项卡中的每一个option选项。这并不是一个非常好的办法。官方提供了更好的实现方式，在WebDriver中提供了一个叫Select方法，也就是第二种操作方式。代码如下：



* * *



from selenium.webdriver.support.ui import Select select = Select(driver.find_element_by_xpath('// form/select ')) select.select_by_index(index) select.select_by_visible_text("text") select.select_by_value(value)



* * *



它可以根据索引、文字、value值来选择选项卡中的某一项。

如果select标记中multiple=“multiple”，也就是说这个select标记支持多选，Select对象提供了支持此功能的方法和属性。示例如下：

·取消所有的选项：select.deselect_all（）

·获取所有的选项：select.options

·获取已选中的选项：select.all_selected_options

2.元素拖拽

元素的拖拽即将一个元素拖到另一个元素的位置，类似于拼图。首先要找到源元素和目的元素，然后使用ActionChains类可以实现。代码如下：



* * *



element = driver.find_element_by_name("source") target = driver.find_element_by_name("target") from selenium.webdriver import ActionChains action_chains = ActionChains(driver) action_chains.drag_and_drop(element, target).perform()



* * *



3.窗口和页面frame的切换

一个浏览器一般都会开多个窗口，我们可以switch_to_window方法实现指定窗口的切换。示例如下：



* * *



driver.switch_to_window("windowName")



* * *



也可以通过window handle来获取每个窗口的操作对象。示例如下：



* * *



for handle in driver.window_handles: driver.switch_to_window(handle)



* * *



如需切换页面frame，可以使用switch_to_frame方法，示例如下：



* * *



driver.switch_to_frame("frameName") driver.switch_to_frame("frameName.0.child")



* * *



4.弹窗处理

如果你在处理页面的过程中，触发了某个事件，跳出弹框。可以使用switch_to_alert获取弹框对象，从而进行关闭弹框、获取弹框信息等操作。示例如下：



* * *



alert = driver.switch_to_alert() alert.dismiss()



* * *



5.历史记录

操作页面的前进和后退功能，示例如下：



* * *



driver.forward() driver.back()



* * *



6.Cookie处理

可以使用get_cookies方法获取cookie，也可以使用add_cookie方法添加cookie信息。示例如下：



* * *



driver.get("http://www.baidu.com") cookie = {'name': 'foo', 'value' : 'bar'} driver.add_cookie(cookie) driver.get_cookies()



* * *



7.设置phantomJS请求头中User-Agent

这个功能在爬虫中非常有用，一般针对phantomJS的反爬虫措施都会检测这个字段，默认的User-Agent中含有phantomJS内容，可以通过代码进行修改。代码如下：



* * *



dcap = dict(DesiredCapabilities.PHANTOMJS) dcap["phantomjs.page.settings.userAgent"] = ( "Mozilla/5.0 (Linux; Android 5.1.1; Nexus 6 Build/LYZ28E) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.23 Mobile Safari/537.36" ) driver = webdriver.PhantomJS()# desired_capabilities=dcap) driver.get("http://www.google.com") driver.quit()



* * *





9.4.5　等待


由于现在很多网站采用Ajax技术，不确定网页元素什么时候能被完全加载，所以网页元素的选取会比较困难，这时候就需要等待。Selenium有两种等待方式，一种是显式等待，一种是隐式等待。

1.显式等待

显式等待是一种条件触发式的等待方式，指定某一条件直到这个条件成立时才会继续执行，可以设置超时时间，如果超过这个时间元素依然没被加载，就会抛出异常。示例如下：



* * *





from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC driver = webdriver.Firefox() driver.get("http://somedomain/url_that_delays_loading") try: element = WebDriverWait(driver, 10).until( EC.presence_of_element_located((By.ID, "myDynamicElement")) ) finally: driver.quit()




* * *



以上代码加载http://somedomain/url_that_delays_loading 页面，并定位id为myDynamic Element的元素，设置超时时间为10s。WebDriverWait默认会500ms检测一下元素是否存在。

Selenium提供了一些内置的用于显式等待的方法，位于expected_conditions类中，方法名称如表9-7所示：

表9-7　内置方法





2.隐式等待

隐式等待是在尝试发现某个元素的时候，如果没能立刻发现，就等待固定长度的时间，类似于socket超时，默认设置是0秒。一旦设置了隐式等待时间，它的作用范围是Webdriver对象实例的整个生命周期，也就是说Webdriver执行每条命令的超时时间都是如此。如果大家感觉设置的时间过长，可以进行不断地修改。使用方法示例如下：



* * *



from selenium import webdriver driver = webdriver.Firefox() driver.implicitly_wait(10) # seconds driver.get("http://somedomain/url_that_delays_loading") myDynamicElement = driver.find_element_by_id("myDynamicElement")



* * *



3.线程休眠

time.sleep（time），这是使用线程休眠延时的办法，也是比较常用的。





9.5　动态爬虫2：爬取去哪网


讲解完了Selenium，接下来编写一个爬取去哪网酒店信息的简单动态爬虫。目标是爬取上海今天的酒店信息，并将这些信息存成文本文件。下面将整个目标进行功能分解：

1）搜索功能，在搜索框输出地点和入住时间，点击搜索按钮

2）获取一页完整的数据。由于去哪网一个页面数据分为两次加载，第一次加载15条数据，这时候需要将页面拉到底部，完成第二次数据加载。

3）获取一页完整且渲染过的HTML文档后，使用BeautifulSoup将其中的酒店信息提取出来进行存储。

4）解析完成，点击下一页，继续抽取数据。

第一步： 找到酒店信息的搜索页面，如图9-15所示。

使用Firebug查看Html结果，可以通过selenium获取目的地框、入住日期、离店日期和搜索按钮的元素位置，输入内容，并点击搜索按钮。



* * *



ele_toCity = driver.find_element_by_name('toCity') ele_fromDate = driver.find_element_by_id('fromDate') ele_toDate = driver.find_element_by_id('toDate') ele_search = driver.find_element_by_class_name('search-btn') ele_toCity.clear() ele_toCity.send_keys(to_city) ele_toCity.click() ele_fromDate.clear() ele_fromDate.send_keys(fromdate) ele_toDate.clear() ele_toDate.send_keys(todate) ele_search.click()



* * *





图9-15　搜索页面

第二步： 分两次获取一页完整的数据，第二次让driver执行js脚本，把网页拉到底部。



* * *



try: WebDriverWait(driver, 10).until( EC.title_contains(unicode(to_city)) ) except Exception,e: print e break time.sleep(5) js = "window.scrollTo(0, document.body.scrollHeight);" driver.execute_script(js) time.sleep(5) htm_const = driver.page_source



* * *



第三步： 使用BeautifulSoup解析酒店信息，并将数据进行清洗和存储。



* * *



soup = BeautifulSoup(htm_const,'html.parser', from_encoding='utf-8') infos = soup.find_all(class_="item_hotel_info") f = codecs.open(unicode(to_city)+unicode(fromdate)+u'.html', 'a', 'utf-8') for info in infos: f.write(str(page_num)+'--'*50) content = info.get_text().replace(" ","").replace("\t","").strip() for line in [ln for ln in content.splitlines() if ln.strip()]: f.write(line) f.write('\r\n') f.close()



* * *



第四步： 点击下一页，继续重复这一个过程。



* * *



next_page = WebDriverWait(driver, 10).until( EC.visibility_of(driver.find_element_by_css_selector(".item.next")) ) next_page.click()



* * *



这个小例子只是简单实现了功能，完整代码如下：



* * *



class QunaSpider(object): def get_hotel(self,driver, to_city,fromdate,todate): ele_toCity = driver.find_element_by_name('toCity') ele_fromDate = driver.find_element_by_id('fromDate') ele_toDate = driver.find_element_by_id('toDate') ele_search = driver.find_element_by_class_name('search-btn') ele_toCity.clear() ele_toCity.send_keys(to_city) ele_toCity.click() ele_fromDate.clear() ele_fromDate.send_keys(fromdate) ele_toDate.clear() ele_toDate.send_keys(todate) ele_search.click() page_num=0 while True: try: WebDriverWait(driver, 10).until( EC.title_contains(unicode(to_city)) ) except Exception,e: print e break time.sleep(5) js = "window.scrollTo(0, document.body.scrollHeight);" driver.execute_script(js) time.sleep(5) htm_const = driver.page_source soup = BeautifulSoup(htm_const,'html.parser', from_encoding='utf-8') infos = soup.find_all(class_="item_hotel_info") f = codecs.open(unicode(to_city)+unicode(fromdate)+u'.html', 'a', 'utf-8') for info in infos: f.write(str(page_num)+'--'*50) content = info.get_text().replace(" ","").replace("\t","").strip() for line in [ln for ln in content.splitlines() if ln.strip()]: f.write(line) f.write('\r\n') f.close() try: next_page = WebDriverWait(driver, 10).until( EC.visibility_of(driver.find_element_by_css_selector(".item.next")) ) next_page.click() page_num+=1 time.sleep(10) except Exception,e: print e break def crawl(self,root_url,to_city): today = datetime.date.today().strftime('%Y-%m-%d') tomorrow=datetime.date.today() + datetime.timedelta(days=1) tomorrow = tomorrow.strftime('%Y-%m-%d') driver = webdriver.Firefox(executable_path='D:\geckodriver_win32\gecko- driver.exe') driver.set_page_load_timeout(50) driver.get(root_url) driver.maximize_window() # 将浏览器最大化显示 driver.implicitly_wait(10) # 控制间隔时间，等待浏览器反映 self.get_hotel(driver,to_city,today,tomorrow) if __name__=='__main__': spider = QunaSpider() spider.crawl('http://hotel.qunar.com/',u"上海")



* * *





9.6　小结


本章讲解了两种动态网站抓取的方法，两种方法有利有弊。直接从JavaScript中提取数据远比使用Selenium+PhantomJS速度快，占用系统内存小，但是碰到参数加密的情况，分析起来就较为复杂，而Selenium+PhantomJS恰恰避免了这个问题，反爬虫能力很强，基本上可以躲过大部分的检测。两种方法都要掌握，针对不同的网站使用不同的策略。





第10章　Web端协议分析


本章主要讲解进行深层次网页爬取时遇到的两个最主要的问题，其中一个是网页登录POST分析，另一个是验证码问题。通过解决项目中出现的实际问题，帮助大家开拓思路，熟练掌握解决方法。





10.1　网页登录POST分析


本节探讨的是那些需要登录之后才能进行页面爬取的情况，属于深层次的网页爬取。我们将讲一些大家熟悉的例子，比如爬取论坛或者贴吧的内容，这种网站对权限的管理非常严格，不同的角色权限，对应的网页内容是不同的。假如你没有登录该论坛或贴吧，相当于游客权限，基本上爬取不到任何有价值的数据。本节要做的就是完成登录获取Cookie这一步，现在的网页登录基本上都是使用表单提交POST请求来完成验证。接下来就讲解登录POST请求中需要注意的情况。





10.1.1　隐藏表单分析


大家在分析POST请求时经常碰到这种情况，通过FireBug截获POST请求，发现POST出去的数据比我们在表单中填写的数据多，而且这些数据的内容每次还变化，这非常影响我们使用Python发送POST请求进行模拟登录。下面以知乎（https://www.zhihu.com/#signin ）为例，如图10-1所示。



图10-1　登录知乎

打开Firebug，打开网络监听，输入账号和密码进行登录。截获的请求如图10-2所示。

POST内容如下：



* * *



_xsrf=03be292fc21b83fa6ddb48760af4f4c2 password=XXXXXXXX phone_num=XXXXXXXX remember_me=true



* * *



我使用的是手机号登录，账号密码使用XXXXXXXX代替。大家发现phone_num、password、remember_me这三个字段是我们在表单中输入或者选中的，除了这三个还多了一个_xsrf参数，做过Web前端的朋友肯定认识这个字段，这是用来防跨站请求伪造的。那这个参数在哪呢？我们需要使用_xsrf这个参数模拟登录。

这就需要Firebug强大的搜索功能，将_xsrf后面的值03be292fc21b83fa6ddb48760af4f4c2填入搜索框中并回车，如图10-3所示。



图10-2　POST请求



图10-3　搜索框

很快就在当前页面的响应中找到了_xsrf的值，可以确定位置是在表单提交的隐藏<input>标记中，如图10-4所示。



图10-4　_xsrf位置

知道了_xsrf的位置，既可以使用Beautiful Soup提取其中的值，也可以直接使用正则表达式提取。这次使用正则表达式进行提取，然后使用Requests提交POST请求。代码如下：



* * *



# coding:utf-8 # 构造 Request headers import re import requests def get_xsrf(session): '''_xsrf 是一个动态变化的参数,从网页中提取''' index_url = 'http://www.zhihu.com' # 获取登录时需要用到的_xsrf index_page = session.get(index_url, headers=headers) html = index_page.text pattern = r'name="_xsrf" value="(.*)"' # 这里的_xsrf 返回的是一个list _xsrf = re.findall(pattern, html) return _xsrf[0] agent = 'Mozilla/5.0 (Windows NT 5.1; rv:33.0) Gecko/20100101 Firefox/33.0' headers = { 'User-Agent': agent } session = requests.session() _xsrf = get_xsrf(session) post_url = 'http://www.zhihu.com/login/phone_num' postdata = { '_xsrf': _xsrf, 'password': 'xxxxxxxx', 'remember_me': 'true', 'phone_num': 'xxxxxxx', } login_page = session.post(post_url, data=postdata, headers=headers) login_code = login_page.text print(login_page.status_code) print(login_code)



* * *



登录成功的输出结果为：



* * *



200 {"r":0, "msg": "\u767b\u5f55\u6210\u529f" }



* * *





10.1.2　加密数据分析


上面看到的知乎账号和密码都是使用明文进行发送，但是为了安全，很多网站都会将密码进行加密，然后添加一系列附加的参数到POST请求中，而且还有验证码，分析难度和知乎登录完全不是一个量级。下面我们就进行一下挑战，分析百度POST登录方式，强化大家的分析能力。由于百度登录使用的是同一套加密规则，所以这次就以百度云盘的登录为例进行分析，整个分析过程分为三个部分。

第一部分

首先打开FireBug，访问http://yun.baidu.com/ ，监听网络数据，如图10-5所示。



图10-5　百度网盘

操作流程：

1）输入账号和密码。

2）点击登录。（第一次POST登录。）

3）这时候会出现验证码，输入验证码。

4）最后点击登录成功上线。（第二次POST登录成功。）

在一次成功的登录过程中，我们需要点击两次登录按钮，也就出现了两次POST请求，如图10-6所示。



图10-6　两次POST请求

将上面两次的POST请求记录下来，记录完成之后，清空cookie，再进行一次成功的登录，用于比较POST请求字段中那些是会变化的，那些是不会变化的。两次登录四次POST请求，我们将这四次POST请求命名为post1_1、post1_2、post2_1、post2_2，以便区分是哪一次登录的哪一个POST请求。

现在先关注post2_2和post1_2，这是两次登录最后成功的POST请求，如图10-7所示：



图10-7　post2_2参数

通过比较post2_2和post1_2，我们可以发现一些字段是变化的，一些是不变的，如表10-1所示。

表10-1　POST参数值状态表



通过表10-1，我们可以了解到那些变化的字段，这也是我们着重要分析的地方。接着分析一下变化的参数，看哪些是可以轻易获取的。

·callback：不清楚是什么，不知道怎么获取。

·codestring：不清楚是什么，不知道怎么获取。

·gid：一个生成的ID号，不知道怎么获取。

·password：加密后的密码，不知道怎么获取。

·ppui_logintime：时间，不知道怎么获取。

·rsakey：RSA加密的密钥（可以推断出密码肯定是经过了RSA加密），不知道怎么获取。

·token：访问令牌，不知道怎么获取。





·tt：时间戳，可以使用Python的time模块生成。


·verifycode：验证码，可以轻易获取验证码图片并获取验证码值。

通过上面的分析，又确定了tt、verifycode参数的提取方式，现在只剩下callback、codestring、gid、password、ppui_logintime、rsakey、token等参数的分析。

第二部分

既然已经知道了需要确定的参数，接下来要做的是确定callback、codestring、gid、password、ppui_logintime、rsakey、token这些参数是在哪一次登录过程的哪一个post请求中产生的。将post2_1和post2_2的请求参数进行比较，如图10-8是post2_1请求的内容，可以和图10-7进行比较，以发现参数的变化。



图10-8　post2_1参数

通过比较，参数变化如表10-2所示。

表10-2　post2_1和post2_2参数值对比



通过上表我们看到出现明显变化的是codestring，从无到有。可以基本上确定codestring是在post2_1之后产生的，所以codestring这个字段应该是在post2_1的响应中找到。果不其然，如图10-9所示：



图10-9　codestring参数

codestring这个字段的获取位置已经确定。

接着分析post2_1已经产生，post2_1内容没有发生变化的参数：gid、rsakey、token。这些参数可以确定是在post2_1请求发送之前就已经产生，根据网络响应的顺序，从下到上，看看能不能发现一些敏感命名的链接。在post2_1的不远处，发现了一个敏感链接：https://passport.baidu.com/v2/getpublickeytoken=69a056f475fc955dc16215ab66a985af&tpl=netdisk&subpro=netdisk_web&apiver=v3&tt=1469844359188&gid=58DDBCC-672F-423D-9A02-688ACB9EB252&callback=bd__cbs__rn85cf ，如图10-10所示。



图10-10　敏感链接

通过查看响应我们找到rsakey，虽然在响应中变成了key，可是值是一样的。通过之前的信息，我们知道密码是通过RSA加密的，所以响应中的publickey可能是公钥，这个要重点注意，如图10-11所示：



图10-11　敏感链接响应

还可以发现callback参数，参数中出现callback字段，之后响应中也出现了callback字段的值将响应包裹，由此可以推断callback字段可能只是进行标识作用，不参与实际的参数校验。

通过对这个敏感链接的请求参数可以得出以下结论：gid和token可以得到rsakey参数。

接着分析gid参数和token参数。直接在FireBug的搜索框中输入token，进行搜索。搜索两到三次，可以发现token的出处位于https://passport.baidu.com/v2/api/getapi&tpl=netdisk&subpro=netdisk_web&apiver=v3&tt=1469844296412&class=login&gid=58DDBCC-672F-423D-9A02-688ACB9EB252&logintype=basicLogin&callback=bd__cbs__cmkxjj ，如图10-12所示：



图10-12　token出处

通过这个链接的get参数，我们可以得到如下的结论：通过gid可以得出Token。

最后分析一下gid参数。依旧是通过搜索的办法，很快在http://passport.bdimg.com/passApi/js/login_tangram_a829ef5 .js中找到了gid的出处，如图10-13所示：



图10-13　gid位置

格式化脚本之后，咱们看一下这个gid是怎么产生的。通过gid：e.guideRandom，我们可以知道gid是由guideRandom这个函数产生的，接着在脚本中搜索这个函数，如图10-14所示：

最后找到这个函数的原型，通过代码可以看到，这是随机生成的字符串，这就好办了。函数原型如下：



* * *



gid = this.guideRandom = function () { return 'xxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function (e) { var t = 16 * Math.random() | 0, n='x'==et:3&t|8; return n.toString(16) }).toUpperCase() }()



* * *





图10-14　guideRandom函数

最后将第二部分进行一下总结：

·codestring：从第一次POST之后的响应中提取出来

·gid：由一个已知函数guideRandom随机产生，可以通过调用函数获取

·token：https://passport.baidu.com/v2/api/getapi&tpl=netdisk&subpro=netdisk_web&apiver=v3&tt=1469844296412&class=login&gid=58DDBCC-672F-423D-9A02-688ACB9EB252&logintype=basicLogin&callback=bd__cbs__cmkxjj ，将gid带入链接，获取响应中的token

·rsakey：https://passport.baidu.com/v2/getpublickeytoken=69a056f475fc955dc16215ab66a985af&tpl=netdisk&subpro=netdisk_web&apiver=v3&tt=1469844359188&gid=58DDBCC-672F-423D-9A02-688ACB9EB252&callback=bd__cbs__rn85c ，将获取的gid和token带入链接，从响应中可以提取出rsakey

第三部分

最后还剩callback、password和ppui_logintime参数。通过之前的分析，可以了解到callback可能没啥用，所以放到后面再分析。一般来说password是最难分析的，所以也放到后面分析。

接下来分析ppui_logintime，搜索ppui_logintime，在下面的链接中找到了ppui_logintime的出处：http://passport.bdimg.com/passApi/js/login_tangram_a829ef5.js ，如图10-15所示。





找到了timeSpan：’ppui_logintime‘，接着搜索timeSpan，如图10-16所示。


找到了r.timeSpan=（new Date）.getTime（）-e.initTime，接着搜索initTime，如图10-17所示。

通过上面的代码我们可以知道ppui_logintime可能是从输入登录信息，一直到点击登录按钮提交的这段时间，可以直接使用之前的POST请求所发送的数据，没有什么影响。



图10-15　ppui_logintime参数



图10-16　timeSpan



图10-17　initTime

接着分析callback参数，搜索callback，我们将可以找到callback的生成方式，如图10-18所示。

callback生成方式为：



图10-18　callback

最后分析password的加密方式，搜索password，发现敏感内容，在http://passport.bdimg.com/passApi/js/login_tangram_a829ef5.js 链接中，如图10-19所示。



图10-19　password

通过设置断点，动态调试可以知道，password是通过公钥pubkey对密码进行加密，最后对输出进行base64编码，即为最后的加密密码。

通过以上三部分的分析，基本上将POST所有参数的产生方式都确定了。最后我们进行模拟登录，其中使用到了pyv8引擎，可以直接运行JavaScript代码，这样生成gid和callback的JavaScript函数可以直接使用，不用转化为Python语言，不过转化也是非常简单的。完整的登录代码如下，每一部分我都进行了详细的注释，大家也可以从我的GitHub上进行下载：https://github.com/qiyeboy/baidulogin.git 。



* * *



# coding:utf-8 import base64 import json import re from Crypto.Cipher import PKCS1_v1_5 from Crypto.PublicKey import RSA import PyV8 from urllib import quote import requests import time if __name__=='__main__': s = requests.Session() s.get('http://yun.baidu.com') js=''' function callback(){ return 'bd__cbs__'+Math.floor(2147483648 * Math.random()).toString(36) } function gid(){ return 'xxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function (e) { var t = 16 * Math.random() | 0, n = 'x' == e t : 3 & t | 8; return n.toString(16) }).toUpperCase() } ''' ctxt = PyV8.JSContext() ctxt.enter() ctxt.eval(js) ########### 获取gid############################# 3 gid = ctxt.locals.gid() ########### 获取callback############################# 3 callback1 = ctxt.locals.callback() ########### 获取token############################# 3 tokenUrl="https:// passport.baidu.com/v2/api/getapi&tpl=netdisk&subpro=net disk_web&apiver=v3" \ "&tt=%d&class=login&gid=%s&logintype=basicLogin&callback=%s"%(time. time()*1000,gid,callback1) token_response = s.get(tokenUrl) pattern = re.compile(r'"token"\s*:\s*"(\w+)"') match = pattern.search(token_response.text) if match: token = match.group(1) else: raise Exception ########### 获取callback############################# 3 callback2 = ctxt.locals.callback() ########### 获取rsakey和pubkey############################# 3 rsaUrl = "https:// passport.baidu.com/v2/getpublickeytoken=%s&" \ "tpl=netdisk&subpro=netdisk_web&apiver=v3&tt=%d&gid=%s&callback= %s"%(token,time.time()*1000,gid,callback2) rsaResponse = s.get(rsaUrl) pattern = re.compile("\"key\"\s*:\s*'(\w+)'") match = pattern.search(rsaResponse.text) if match: key = match.group(1) print key else: raise Exception pattern = re.compile("\"pubkey\":'(.+)'") match = pattern.search(rsaResponse.text) if match: pubkey = match.group(1) print pubkey else: raise Exception ################ 加密password######################## 3 password = 'xxxxxxx'# 填上自己的密码 pubkey = pubkey.replace('\\n','\n').replace('\\','') rsakey = RSA.importKey(pubkey) cipher = PKCS1_v1_5.new(rsakey) password = base64.b64encode(cipher.encrypt(password)) print password ########### 获取callback############################# 3 callback3 = ctxt.locals.callback() data={ 'apiver':'v3', 'charset':'utf-8', 'countrycode':'', 'crypttype':12, 'detect':1, 'foreignusername':'', 'idc':'', 'isPhone':'', 'logLoginType':'pc_loginBasic', 'loginmerge':True, 'logintype':'basicLogin', 'mem_pass':'on', 'quick_user':0, 'safeflg':0, 'staticpage':'http://yun.baidu.com/res/static/thirdparty/pass_v3_jump.html', 'subpro':'netdisk_web', 'tpl':'netdisk', 'u':'http://yun.baidu.com/', 'username':'xxxxxxxxx',# 填上自己的用户名 'callback':'parent.'+callback3, 'gid':gid,'ppui_logintime':71755, 'rsakey':key, 'token':token, 'password':password, 'tt':'%d'%(time.time()*1000), } ########### 第一次post############################# 3 post1_response = s.post('https:// passport.baidu.com/v2/api/login',data=data) pattern = re.compile("codeString=(\w+)&") match = pattern.search(post1_response.text) if match: ########### 获取codeString############################# 3 codeString = match.group(1) print codeString else: raise Exception data['codestring']= codeString ############# 获取验证码################################### verifyFail = True while verifyFail: genimage_param = '' if len(genimage_param)==0: genimage_param = codeString verifycodeUrl="https:// passport.baidu.com/cgi-bin/genimage%s"%genimage_param verifycode = s.get(verifycodeUrl) ############# 下载验证码################################### with open('verifycode.png','wb') as codeWriter: codeWriter.write(verifycode.content) codeWriter.close() ############# 输入验证码################################### verifycode = raw_input("Enter your input verifycode: "); callback4 = ctxt.locals.callback() ############# 检验验证码################################### checkVerifycodeUrl='https:// passport.baidu.com/v2/' \ 'checkvcode&token=%s' \ '&tpl=netdisk&subpro=netdisk_web&apiver=v3&tt=%d' \ '&verifycode=%s&codestring=%s' \ '&callback=%s'%(token,time.time()*1000,quote(verifycode), codeString,callback4) print checkVerifycodeUrl state = s.get(checkVerifycodeUrl) print state.text if state.text.find(u'验证码错误')!=-1: print '验证码输入错误...已经自动更换...' callback5 = ctxt.locals.callback() changeVerifyCodeUrl = "https:// passport.baidu.com/v2/reggetcodestr" \ "&token=%s" \ "&tpl=netdisk&subpro=netdisk_web&apiver=v3" \ "&tt=%d&fr=login&" \ "vcodetype=de94eTRcVz1GvhJFsiK5G+ni2k2Z78PYR xUaRJLEmxdJO5ftPhviQ3/ JiT9vezbFtwCyqdkNWSP29oeOvYE0SYPocOGL+iTafSv8pw" \ "&callback=%s"%(token,time.time()*1000,callb ack5) print changeVerifyCodeUrl verifyString = s.get(changeVerifyCodeUrl) pattern = re.compile('"verifyStr"\s*:\s*"(\w+)"') match = pattern.search(verifyString.text) if match: ########### 获取verifyString############################# 3 verifyString = match.group(1) genimage_param = verifyString print verifyString else: verifyFail = False raise Exception else: verifyFail = False data['verifycode']= verifycode ########### 第二次post############################# 3 data['ppui_logintime']=81755 #################################################### # 特地说明，大家会发现第二次的post出去的密码是改变的，为什么我这里没有变化呢？ # 是因为RSA加密，加密密钥和密码原文即使不变，每次加密后的密





码都是改变的，RSA有随机因子 的关系 # 所以我这里不需要在对密码原文进行第二次加密了，直接使用上次加密后的密码即可，是没有问题的。 ########################################################################## post2_response = s.post('https:// passport.baidu.com/v2/api/login',data=data) if post2_response.text.find('err_no=0')!=-1: print '登录成功' else: print '登录失败'


* * *



注意 　以上百度登录分析过程仅限于当时的加密情况，如果之后换了登录方式，以上代码可能会失效，但是分析方法不变





10.2　验证码问题


对于爬虫来说，一个比较大的阻碍就是验证码，验证码也是反爬虫的有效措施之一。接下来针对验证码出现的方式，就如何突破验证码进行进一步的探讨。





10.2.1　IP代理


当你使用同一个IP频繁访问网页时，这时候网站服务器就极有可能将你判定为爬虫，此时会在网页中出现验证码，输入正确才能正常访问，类似淘宝的这种情况，如图10-20所示。



图10-20　访问验证

这种验证码的产生原因是由于同一IP的频繁访问，当然你可以加大爬虫的延时，做到和人访问速率一样，不过这样效率稍微低一些，好的做法是换IP进行访问。

之前在第3章中对于urllib2和Requests如何配置代理IP的方法已经进行了讲解，这里不再进行赘述。大家可能更关心的是如何获取更多的代理IP，主要有以下几种方式：

·首先是VPN：国内和国外很多厂商提供VPN服务，可以分配不同的网络线路，并可以自动更换IP，实时性很高，速度很快。稳定可靠的VPN的价格一般都不低，适合商用。

·IP代理池：一些厂商将很多IP做成代理池，提供API接口，允许用户使用程序调用。稳定的IP代理池也是很贵的，不适合个人学习使用。

·ADSL宽带拨号：大家肯定都用过拨号上网的方式，ADSL有个特点是断开再重新连接时分配的IP会变化，爬虫可以利用这个原理更换IP。由于更换IP需要断开再重连，使用这种方式的效率并不高，适合实时性不高的场景。

VPN和IP代理池都有厂商各自提供的更换IP接口，大家可以根据自己选择的厂商进行配置。

ADSL宽带拨号，可以使用Python实现拨号和断开，比如Windows提供了一个用于操作拨号的命令rasdial，接下来用Python操作这个命令实现上网，代码如下：



* * *



# coding:utf-8 import os import time g_adsl_account = {"name": "adsl", "username": "xxxxxxx", "password": "xxxxxxx"} class Adsl(object): ##################################################### # __init__ : name: adsl名称 ##################################################### def __init__(self): self.name = g_adsl_account["name"] self.username = g_adsl_account["username"] self.password = g_adsl_account["password"] ##################################################### # set_adsl : 修改adsl设置 ##################################################### def set_adsl(self, account): self.name = account["name"] self.username = account["username"] self.password = account["password"] ##################################################### # connect : 宽带拨号 ##################################################### def connect(self): cmd_str = "rasdial %s %s %s" % (self.name, self.username, self.password) os.system(cmd_str) time.sleep(5) #################################################### # disconnect : 断开宽带连接 ################################################## def disconnect(self): cmd_str = "rasdial %s /disconnect" % self.name os.system(cmd_str) time.sleep(5) ##################################################### # reconnect : 重新进行拨号 ##################################################### def reconnect(self): self.disconnect() self.connect() if __name__=='__main__': adsl = Adsl() adsl.connect()



* * *



现在有很多提供宽带拨号的服务商，提供专门的VPS拨号主机，例如无极网络等，价格不是很贵，也相对比较稳定，大家可以尝试去用一下，IP基本上是秒切换。

最后提供一个适合个人使用的代理方式IPProxyPool，这是本人用Python写的一个开源项目，放置于GitHub：https://github.com/qiyeboy/IPProxyPool 。原理：通过爬取各大IP代理网站的免费IP，将这些IP进行去重、检测代理有效性等操作，最后存储到SQLite数据库中，并提供一个API接口，方便大家调用。以Windows使用进行讲解：

1）使用git将代码clone到本地，或在GitHub上下载IPProxyPool压缩包解压即可

2）进入IPProxyPool目录，在命令行窗口运行Python IPProxys.py

这个时候IPProxyPool就开始工作，爬取免费IP了。每半个小时进行一次IP的爬取和校验，防止IP失效，效果如图10-21所示。



图10-21　IPProxys运行

那我们自己的爬虫如何获取IPProxyPool提供的IP呢？非常简单，IPProxyPool提供了一个HTTP请求接口，假如我们的爬虫程序和IPProxys在同一台主机上，可以向127.0.0.1：8000发送一个GET请求，请求参数如表10-3所示。

表10-3　GET参数



假如发送的GET请求为http://127.0.0.1：8000/types=0&count=5&county=中国 ，这个请求的意思是返回5个IP所在地在中国，类型为高匿的IP地址，响应格式为JSON，按照响应速度由高到低，返回数据，类似[{“ip”：“220.160.22.115”，“port”：80}，{“ip”：“183.129.151.130”，“port”：80}，{“ip”：“59.52.243.88”，“port”：80}，{“ip”：“112.228.35.24”，“port”：8888}，{“ip”：“106.75.176.4”，“port”：80}]，大家在爬虫程序中只要将响应进行解析即可。示例代码如下：



* * *



import requests import json r = requests.get('http://127.0.0.1:8000/types=0&count=5&country=中国') ip_ports = json.loads(r.text) print ip_ports ip = ip_ports[0]['ip'] port = ip_ports[0]['port'] proxies={ 'http':'http://%s:%s'%(ip,port), 'https':'http://%s:%s'%(ip,port) } r = requests.get('http://ip.chinaz.com/',proxies=proxies) r.encoding='utf-8' print r.text



* * *



根据我的统计，一般有用的代理IP在70个左右，完全满足个人的需要。





10.2.2　Cookie登录


当我们在登录的时候遇到了验证码，这时候我们需要人工识别之后才能登录上去，其实这是个非常繁琐的过程，每次登录都要我们手动输入验证码，很不可取。但是大部分的网站当你登录上去之后，Cookie都会保持较长的一段时间，避免因用户频繁输入账号和密码造成的不便。我们可以利用这个特性，当我们登录成功一次之后，可以将Cookie信息保存到本地，下次登录时直接使用Cookie登录。以10.1节的知乎登录为例，我们可以加入两个函数：save_session和load_session。代码如下：




* * *



def save_session(session): # 将session写入文件: session.txt with open('session.txt', 'wb') as f: cPickle.dump(session.headers, f) cPickle.dump(session.cookies.get_dict(), f) print '[+] 将session写入文件: session.txt' def load_session(): # 加载session with open('session.txt', 'rb') as f: headers = cPickle.load(f) cookies = cPickle.load(f) return headers,cookies



* * *





10.2.3　传统验证码识别


当我们识别并手动输入验证码，成功登录，并保存Cookie信息以便下次使用，这些操作做完之后仅仅可以暂时松一口气，因为Cooke总有失效的时候，下次还是要重复这个过程，尤其是爬取的网站很多时，将是一个很繁重的工作。如果能使用Python程序自动识别验证码，这将是一件省时省力的事情，这就涉及传统验证码的识别。

为什么限定为传统验证码呢？传统验证码即传统的输入型验证码，可以是数字、字母和汉字，这类验证码不涉及验证码含义的分析，仅仅识别验证码的内容，识别相对简单，进行验证码识别需要使用到tesseract-ocr。下面讲解一下Python如何使用tesseract-ocr进行验证码识别。

Python识别验证码需要安装tesseract-ocr、pytesseract和Pillow。

Ubuntu：

·tesseract-ocr：sudo apt-get install tesseract-ocr

·pytesseract：sudo pip install pytesseract

·Pillow：sudo pip install pillow

Windows：

·tesseract-ocr：下载链接为http://digi.bib.uni-mannheim.de/tesseract/ ，下载后直接安装，建议使用安装过程中的默认选项，安装目录默认为C：\Program Files（x86）\Tesseract-OCR。

·pytesseract：pip install pytesseract

·Pillow：pip install pillow

安装完成后开始进行识别验证码，以 这个验证码为例，识别代码如下：



* * *



# coding:utf-8 import pytesseract from PIL import Image image = Image.open('code.png') # 设置tesseract的安装路径 pytesseract.tesseract_cmd = 'c:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe' code = pytesseract.image_to_string(image) print code



* * *



输出结果为：



* * *



0376



* * *



我们只是简单介绍了tesseract-ocr的使用，对验证码的识别涉及图像处理方面的知识，提高识别率还要学习训练样本，本节不进行扩展讲解，感兴趣的话大家可以自行研究。





10.2.4　人工打码


当传统验证码识别难度加大，识别程序很难保证较高的准确率，例如这种情况 ，验证码粘连扭曲非常严重，识别起来比较困难，这时候人工打码就产生了。

人工打码采用自动识别+人工识别的组合方式。本人之前用过人工打码的平台，主要有两个，分别为打码兔和QQ超人打码，都提供了各种编程语言的接入方式，包括Python，当然人工打码是需要收费的。以QQ超人打码为例，首先要去注册开发者账号，在识别程序中需要填写个人账号进行认证计费，如图10-22所示。



图10-22　注册界面

注册完成后，官方提供了各种编程语言接入方式的示例，其中就有Python的，如图10-23所示。

大家可以根据提供的API示例，开发自己的识别程序。



图10-23　Python示例





10.2.5　滑动验证码


滑动验证码是最近比较流行的验证方式，是一种基于行为的验证方式，如图10-24所示。



图10-24　滑动验证码

滑动验证码虽然验证方式比较特别，不过依然有办法突破，一种通用的办法是使用selenium来进行处理。主要的技术要点有：

·在浏览器上模拟鼠标拖动的操作。

·计算图片中缺口的偏移量。

·模拟人类拖动鼠标的轨迹。

由于涉及图像拼接方面的知识，此处不再深入讲解。如果遇到这种情况，大家可以采取多账号登录后，保存cookie信息，组建cookie池的方法绕过。如果大家对滑动验证码的识别比较感兴趣，推荐大家看一篇文章：http://www.w2bc.com/article/170660 。





10.3　www>m>wap


本节标题中所说的www是PC浏览器看到的网站，m和wap是移动端，现在智能手机一般用的是m站，部分旧手机用的还是wap。从微博和QQ空间的界面最容易发现这三者的不同。以微博为例子，图10-25为www站点，图10-26为m站点，图10-27为wap站点。

现在越来越多的网站使用Ajax技术，而且反爬虫手段层出不穷，但是像wap这种结构简单的移动网站，不会使用复杂的技术，页面结构简单，非常利于我们提取数据，因此如果网站有m或者wap站点，优先选择作为爬取对象。如何伪装成不同的平台去访问呢？当然是修改User-Agent头，网站服务器会根据你的浏览器表头判断你是从哪个平台发送的请求，因此在爬取的时候将User-Agent头修改一下。可能大家不知道如何修改User-Agent使其符合识别要求，可以这样做。Firefox或者Chrome浏览器都有修改User-Agent的插件User-agent Switcher，通过网络请求监控，就可以查看这些插件手机发送的是什么类型的User-Agent。



图10-25　www站



图10-26　m站



图10-27　wap站





10.4　小结


本章讲解了深层次网页爬取的登录问题和验证码问题，尤其是对于登录POST加密请求的分析十分重要，关键在于分析思路。熟练掌握处理验证码的方法，会让爬虫变得更加强大和灵活。





第11章　终端协议分析


本章主要讲解的是PC客户端和移动端APP协议分析，为什么会出现这个专题？当Web端反爬虫方式越来越多，JavaScript调试越来越复杂，是不是感觉爬虫比较难写了呢？这时候，我们需要转变思路，让爬虫在PC端和移动端活跃起来，继续爬取数据。例如虾米音乐，这类比较大型的产品，不仅有Web端，还有PC客户端和移动客户端。通过网页我们可以在线听音乐，获取歌曲和专辑的信息。同样，PC客户端和移动客户端也可以听取音乐，获取歌曲和专辑的信息，只要我们能把爬虫伪装成PC客户端或者移动客户端，模拟它们的请求方式，就可以进行数据爬取。这就是本章终端协议分析的由来和意义，接下来开启分析之旅。





11.1　PC客户端抓包分析


要想将爬虫伪装成PC客户端，我们需要对PC客户端进行抓包分析。PC上的抓包软件本人比较喜欢的有Wireshark、Http Analyzer等。Wireshark擅长各类网络协议的分析，比较重型。Http Analyzer则更专注于对HTTP/HTTPS协议的分析。Http Analyzer可以针对某一个进程进行抓包，对特定软件的分析更加快捷，所以PC抓包工具选用的是Http Analyzer。





11.1.1　HTTP Analyzer简介


HTTP Analyzer是一款实时捕捉分析HTTP/HTTPS协议数据的工具，可以显示许多信息（包括文件头、内容、Cookie、查询字符串、提交的数据、重定向的URL地址），可以提供缓冲区信息、清理对话内容、HTTP状态信息和其他过滤选项。同时还是一个非常有用的分析、调试和诊断的开发工具。下载链接：http://www.ieinspector.com/httpanalyzer/download.html 。

Http Analyzer安装很简单，从上述链接下载的安装包，双击即可。接下来介绍一下Http Analyzer的基本功能。

1）打开HTTP Analyzer，我们可以看到如图11-1的界面。

点击上图中所示的“Start Logging”按钮，即可开始记录当前处于会话状态的所有应用程序的HTTP流量。如果你当前有应用程序正在进行网络会话，即可看到中间网格部分会显示一条或多条详细的HTTP流量信息。如果你没有正在进行网络会话的应用程序，你也可以在按下“Start Logging”按钮后，使用浏览器随意打开一个网页，即可看到相应的HTTP流量信息。



图11-1　HTTP Analyzer主界面

2）如图11-2所示，HTTP Analyzer会根据进程进行区分，将捕获到的HTTP连接信息显示到中间的网格中。点击任意的HTTP连接，即可在下方查看该连接对应的详细信息。

整个面板主要分为三个部分：

·窗口1显示所有的HTTP流量信息，并根据进程和时间进行归类排序。

·窗口2以选项卡的方式显示HTTP连接的详细信息。其中包括HTTP头信息、响应内容、POST表单数据、请求计时、查询字符串、Cookies、原始数据流、提示信息、注释、响应状态码的解释信息。

·窗口3显示的是当前连接所属进程的相关信息。

3）HTTP Analyzer为我们提供了数据过滤器，方便我们快速地定位到符合条件的数据。我们可以按照进程来过滤，也可以按照数据的类型来过滤。比如要让图11-2中窗口1只显示虾米音乐播放器进程的信息，可以如图11-3这样设置。



图11-2　面板划分

也可以根据数据类型进行过滤，如图11-4所示：



图11-3　过滤进程



图11-4　过滤数据类型





11.1.2　虾米音乐PC端API实战分析


以虾米音乐PC客户端为例，使用HTTP Analyzer分析获取歌手信息、歌曲信息和专辑信息等API接口。打开虾米音乐PC客户端，如图11-5所示：



图11-5　虾米音乐客户端

启动HTTP Analyzer，设置为仅显示虾米客户端进程，并将过滤类型设置为text/html。此时在客户端搜索框中搜索陈奕迅，我们观察一下抓包结果，如图11-6所示。



图11-6　抓包效果

主要关注搜索请求的类型头和响应。搜索使用的是GET请求，请求链接为http://www.xiami.com/app/xiating/searchav=XMusic_2.0.2.1618&uid=0&key=%E9%99%88%E5%A5%95%E8%BF%85 ，key参数为陈奕迅的URL编码，这就是一个搜索API。这时查看一下响应，如图11-7所示：



图11-7　抓包效果

响应是一个标准的HTML文档，也就是我们可以使用Beautiful Soup4或者lxml对它进行解析和提取数据。同时在响应中发现了歌手个人信息、歌曲信息、专辑信息和精选集的API接口。

以上就是使用HTTP Analyzer分析API接口的操作流程，大家可以根据这种方式分析出所有的虾米音乐接口，然后启动爬虫发送请求进行解析就可以了。此次分析比较简单，一些客户端例如网易云音乐的链接请求都是进行加密的，这样的分析就会变得很困难。如果没有逆向PC客户端软件和分析算法的能力，还是放弃通过PC客户端分析API接口的想法，接着往下看吧。





11.2　App抓包分析


说完PC客户端的抓包分析，接下来进入移动端App进行分析。App抓包分析以Android App为例，iOS的抓包分析大家可以自行探索，原理都是一样的。对Android应用进行抓包分析，使用的策略是在电脑上安装一个Android模拟器，将应用安装到模拟器中，这个时候Wireshark的作用就体现了。下面对Wireshark的功能进行简要介绍。





11.2.1　Wireshark简介


Wireshark是世界上最流行的网络分析工具，这个强大的工具可以捕捉网络中的数据，并为用户提供关于网络和上层协议的各种信息。官方下载网址为https://www.wireshark.org/download.html 。安装完成后，启动Wireshark，可以看到图11-8的界面。



图11-8　Wireshark启动界面

启动完Wireshark后，接下来讲解一下Wireshark的基本功能和操作。Wireshark是捕获机器上的某一块网卡的网络包，当你的机器上有多块网卡的时候，你需要选择一个网卡。点击/Caputre→Interfaces../，出现如图11-9所示的对话框，选择正确的网卡。然后点击Start按钮，开始抓包。



图11-9　选择网卡

抓包开始后，效果如图11-10所示，下面介绍一下Wireshark窗口面板和功能。

Wireshark面板主要分为以下几个部分：



图11-10　Wireshark窗口面板

·1号框：Display Filter（显示过滤器），用于过滤数据，这个非常有用。

·2号框：Packet List Pane（封包列表），显示捕获到的封包，有源地址和目标地址、端口号。

·3号框：Packet Details Pane（封包详细信息），用于显示封包中的字段，可以查看封包的详细信息。

·4号框：Dissector Pane（16进制数据），使用16进制显示数据包的内容。

其实使用Wireshark抓包分析协议相对简单，但是作为初学者，使用Wireshark时最常见的问题，是当你使用默认设置时，会得到大量冗余信息，以至于很难找到自己需要的部分。这就是为什么过滤器会如此重要。它们可以帮助我们在庞杂的结果中迅速找到我们需要的信息。Wireshark主要支持两种过滤器：

·捕捉过滤器：用于决定将什么样的信息记录在捕捉结果中，需要在开始捕捉前设置。

·显示过滤器：在捕捉结果中进行详细查找，可以在得到捕捉结果后随意过滤。

·这两种过滤器我们该使用哪一种呢？一般采取的措施是两种结合起来使用，两种过滤器的目的是不同的。

·捕捉过滤器是数据经过的第一层过滤器，它用于控制捕捉数据的数量，以避免产生过大的数据包文件。

·显示过滤器是一种更为强大的过滤器，它允许你在数据包文件中迅速准确地找到所需要的记录。

这两种过滤器的语法完全不同，接下来依次对过滤器的语法进行讲解。

1.捕捉过滤器

捕捉过滤器的语法与其他使用Libpcap（Linux）或者Winpcap（Windows）库开发的软件一样。捕捉过滤器必须在开始捕捉前设置完毕。设置捕捉过滤器的主要分为三步：

1）在Wireshark的菜单栏中选择Capture->Options。

2）将过滤表达式填写到Capture filter输入框或者点击Capture filter按钮为过滤器表达式命名，以便下次使用。

3）点击Start进行捕捉。

如图11-11所示，用于将目的或来源IP地址为10.1.2.3的封包进行捕获显示。



图11-11　设置捕捉过滤器

捕捉过滤器的语法如下：



* * *



Protocol Direction Host(s) Value Logical Operations Other expression



* * *



一个过滤表达式可以包括Protocol、Direction、Host（s）、Value、Logical Operations和Other expression等6个部分，中间以空格相连。

·Protocol代表着协议，可能的值：ether、fddi、ip、arp、rarp、decnet、lat、sca、moprc、mopdl、tcp and udp。如果没有特别指明是什么协议，则默认使用所有支持的协议。

·Direction代表着数据包方向，可能的值：src、dst、src and dst、src or dst。如果没有特别指明来源或目的地，可以不用添加，Wireshark默认使用“src or dst”作为关键字。

·Host（s）可能的值：net、port、host、portrange。如果没有指定此值，则默认使用host关键字。例如，src 10.11.12.1与src host 10.11.12.1含义相同。

·Logical Operations为逻辑运算符，可能的值：not、and、or。not具有最高的优先级，or和and具有相同的优先级，运算时从左至右进行。如果想改变运算范围，可以使用（）进行包裹。例如表达式not tcp port 80and tcp port 21与（not tcp port 80）and tcp port 21含义相同，而not tcp port 80and tcp port 21与not（tcp port 80and tcp port 21）不同。

·Other expression意思是其他表达式，也就是说可以使用逻辑运算符将两个过滤表达式进行连接。

下面使用一个非常完整的例子和上面的语法顺序进行一一对应：



* * *



tcp dst 10.11.1.1 80 and tcp dst 10.22.2.2 801



* * *



这个表达式的意思就是捕获目的IP为10.11.1.1，端口为80和目的IP为10.22.2.2，端口为801的tcp数据包。

2.显示过滤器

有时候经过捕捉过滤器后，数据包的数量还是很大，这个时候就要用到显示过滤器。它的功能比捕捉过滤器更为强大，可以更加细致地查找，而且修改过滤器条件时，并不需要重新捕捉一次，可以做到即时生效。显示过滤器表达式的位置如图11-12所示，使用线框圈起来，输入表达式之后回车，过滤会立即生效。



图11-12　设置显示过滤器

显示过滤器的语法如下：



* * *



Protocol.String 1.String2 Comparison operator Value Logical Operations Other expression



* * *



下面对每个字段进行讲解：

Protocol（协议）：可以使用大量位于OSI模型第2至7层的协议。如图11-13所示，点击“Expression...”按钮后，可以看到它们的取值。比如：IP、TCP、DNS。





图11-13　显示过滤器支持协议


String1、String2：这两个字段是可选项，这个是对协议子类的表述，可以对协议中的某个字段进行过滤。如图11-14所示，点开相关父类旁的+号，然后选择其子类。这就是为什么String1，String2和Protocol是使用“.”相连接。



图11-14　协议子类

Comparison operators：代表着比较运算符，有两种不同写法，可以取表11-1所示的值。

表11-1　比较运算符



Logical expressions：代表逻辑运算符，同样有两种写法，如表11-2所示。

表11-2　逻辑运算符



由于显示表达式相对比较复杂，下面通过表11-3来展示一些常用的例子帮助大家学习。

表11-3　常用例子





如果过滤器的语法是正确的，显示过滤器框的背景呈绿色。如果呈红色，说明表达式有误。





11.2.2　酷我听书App端API实战分析


讲解完Wireshark的使用，开始对App进行实战分析。首先需要在电脑上装一个Android模拟器。Android模拟器有很多种，比如天天模拟器，Bluestacks模拟器和Windroye模拟器。我使用的是天天模拟器，将酷我听书App安装到模拟器中，如图11-15所示。

下面就以已经安装的酷我听书App为例子，进行API分析。启动Wireshark，并在模拟器中启动酷我听书App，开始抓包。图11-16为启动酷我听书后的界面。



图11-15　安装酷我听书



图11-16　酷我听书的界面

这时候已经抓获了很多数据包，在显示过滤器中填入：http.host contains“kuwo.cn”，将域名中包含kuwo.cn的请求过滤出来，如图11-17所示。

大家可以从封包信息框中查看封包中的详细信息，包括请求的链接、参数和相应信息。请注意Response inframe：621这句话，这部分是请求响应的位置，右键点击链接，选择show Packet Reference in New Window，就可以查看相应数据包的信息，如图11-18所示。



图11-17　过滤kuwo.cn的请求



图11-18　查看响应请求

通过查看响应内容，发现酷我听书使用JSON进行传输，爬虫解析器的编写方式基本可以确定。通过以上方法就可以找到自己所需的API接口。





11.3　API爬虫：爬取mp3资源信息


通过Wireshark对酷我听书的抓包，找到了四个酷我听书的API接口，如表11-4所示。

表11-4　API接口



通过上面可以看到，这几个API接口是逐层递进的关系，从前一个API接口的JSON响应中可以获取下一个API接口所需的ID。知道这层关系，下面我们写一个简单的API爬虫程序，功能是提取相声分类下郭德纲相声的曲目详细信息。

1.爬虫下载器

爬虫下载器代码和第6章的一样，不过需要将UserAgent改成手机浏览器的信息，下载下来的是JSON格式。代码如下：



* * *



# coding:utf-8 import requests class SpiderDownloader(object): def download(self,url): if url is None: return None user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.93 Safari/537.36' headers={'User-Agent':user_agent} r = requests.get(url,headers=headers) if r.status_code==200: r.encoding='utf-8' return r.text return None



* * *



2.爬虫解析器

由于是爬取所有郭德纲的曲目信息，提前通过API接口知道了ID，所以直接爬取http://ts.kuwo.cn/service/getlist.v31.phpact=cat&id=50 这个链接下的数据即可。数据格式如图11-19所示。

解析器需要将JSON文件中list下所有的Name和ID信息提取出来，将提取出来的ID，再代入http://ts.kuwo.cn/service/getlist.v31.phpact=detail&id={id} 链接获取详细信息，并将其中曲目的ID、Name和Path提取出来，存储为HTML。解析器代码如下：



图11-19　数据格式



* * *



# coding:utf-8 import json class SpiderParser(object): def get_kw_cat(self, response): ''' 获取分类下的曲目 :param response: :return: ''' try: kw_json = json.loads(response, encoding="utf-8") cat_info = [] if kw_json["sign"] is not None: if kw_json["list"] is not None: for data in kw_json["list"]: id = data["Id"] name= data["Name"] cat_info.append({'id':id,'cat_name':name}) return cat_info except Exception,e: print e def get_kw_detail(self,response): ''' 获取某一曲目的详细信息 :param response: :return: ''' detail_json = json.loads(response, encoding="utf-8") details=[] for data in detail_json["Chapters"]: if data is None: return else: try: file_path = data["Path"] name =data["Name"] file_id =str(data["Id"]) details.append({'file_id':file_id,'name':name,'file_path': file_path}) except Exception,e: print e return details



* * *



3.数据存储器

数据存储器的代码和第7章分布式的代码差别不大，程序如下：



* * *



# coding:utf-8 import codecs class SpiderDataOutput(object): def __init__(self): self.filepath='kuwo.html' self.output_head(self.filepath) def output_head(self,path): ''' 将HTML头写进去 :return: ''' fout=codecs.open(path,'w',encoding='utf-8') fout.write("<html>") fout.write("<body>") fout.write("<table>") fout.close() def output_html(self,path,datas): ''' 将数据写入HTML文件中 :param path: 文件路径 :return: ''' if datas==None: return fout=codecs.open(path,'a',encoding='utf-8') for data in datas: fout.write("<tr>") fout.write("<td>%s</td>"%data['file_id']) fout.write("<td>%s</td>"%data['name']) fout.write("<td>%s</td>"%data['file_path']) fout.write("</tr>") fout.close() def ouput_end(self,path): ''' 输出HTML结束 :param path: 文件存储路径 :return: ''' fout=codecs.open(path,'a',encoding='utf-8') fout.write("</table>") fout.write("</body>") fout.write("</html>") fout.close()



* * *



4.爬虫调度器

爬虫调度器和第6章的调度器内容差不多。代码如下：



* * *



class SpiderMan(object): def __init__(self): self.downloader = SpiderDownloader() self.parser = SpiderParser() self.output = SpiderDataOutput() def crawl(self,root_url): content = self.downloader.download(root_url) for info in self.parser.get_kw_cat(content): print info cat_name = info['cat_name'] detail_url = 'http://ts.kuwo.cn/service/getlist.v31.phpact=detail&id= %s'%info['id'] content = self.downloader.download(detail_url) details = self.parser.get_kw_detail(content) print detail_url self.output.output_html(self.output.filepath,details) self.output.ouput_end(self.output.filepath) if __name__ =="__main__": spider = SpiderMan() spider.crawl('http://ts.kuwo.cn/service/getlist.v31.phpact=cat&id=50')



* * *



以上就是API爬虫的所有内容，启动爬虫，数据就开始存储了，效果如图11-20所示。



图11-20　最终数据





11.4　小结


如果Web端的爬虫不是很容易做的话，可以将思路向终端进行转化，但是工具的使用和协议的分析能力依然是必不可少的。





第12章　初窥Scrapy爬虫框架


从本章开始，我们将正式接触Scrapy爬虫框架，学习如何通过成熟的框架来实现定向爬虫。Scrapy是一个非常优秀的框架，操作简单，拓展方便，是比较流行的爬虫解决方案。本章的内容建立在官方文档（https://doc.scrapy.org/en/latest/ ）的基础上，再加上实际的项目开发，从爬虫的创建开始，由浅及深，讲解Scrapy的用法和特性。





12.1　Scrapy爬虫架构


Scrapy是一个用Python写的Crawler Framework，简单轻巧，并且非常方便。Scrapy使用Twisted这个异步网络库来处理网络通信，架构清晰，并且包含了各种中间件接口，可以灵活地完成各种需求。Scrapy整体架构如图12-1所示。

根据架构图介绍一下Scrapy中的各大组件及其功能：

·Scrapy引擎（Engine）。引擎负责控制数据流在系统的所有组件中流动，并在相应动作发生时触发事件。

·调度器（Scheduler）。调度器从引擎接收Request并将它们入队，以便之后引擎请求request时提供给引擎。

·下载器（Downloader）。下载器负责获取页面数据并提供给引擎，而后提供给Spider。

·Spider。Spider是Scrapy用户编写用于分析Response并提取Item（即获取到的Item）或额外跟进的URL的类。每个Spider负责处理一个特定（或一些）网站。

·Item Pipeline。Item Pipeline负责处理被Spider提取出来的Item。典型的处理有清理验证及持久化（例如存储到数据库中）。



图12-1　Scrapy架构

·下载器中间件（Downloader middlewares）。下载器中间件是在引擎及下载器之间的特定钩子（specific hook），处理Downloader传递给引擎的Response。其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。

·Spider中间件（Spider middlewares）。Spider中间件是在引擎及Spider之间的特定钩子（specific hook），处理Spider的输入（response）和输出（Items及Requests）。其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。

大家有没有发现一个成熟的爬虫框架包含的也是基础篇所讲的简单爬虫的各个模块。

讲完组件，接着看一下数据流，通过数据流的流向，可以清楚地看到Scrapy的工作流程。Scrapy中的数据流由执行引擎控制，其过程如下：

1）引擎打开一个网站（open a domain），找到处理该网站的Spider并向该Spider请求第一个要爬取的URL。

2）引擎从Spider中获取到第一个要爬取的URL并通过调度器（Scheduler）以Request进行调度。

3）引擎向调度器请求下一个要爬取的URL。

4）调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件（请求（request）方向）转发给下载器（Downloader）。

5）一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件（返回（response）方向）发送给引擎。

6）引擎从下载器中接收到Response并通过Spider中间件（输入方向）发送给Spider处理。

7）Spider处理Response并返回爬取到的Item及（跟进的）新的Request给引擎。

8）引擎将（Spider返回的）爬取到的Item给Item Pipeline，将（Spider返回的）Request给调度器。

9）（从第二步）重复直到调度器中没有更多的Request，引擎关闭该网站。





12.2　安装Scrapy


1.Windows

Scrapy的安装以在Windows平台下最为复杂，因为很多东西没有预安装。Python2.7.X的安装前面已经讲过，除此之外，安装Scrapy还需要4步：

1）安装pywin32，安装地址为http://sourceforge.net/projects/pywin32/ ，下载对应版本的pywin32，直接双击安装即可。安装完毕之后，在Python命令行下输入import win32com，如果没有提示错误，则证明安装成功。

2）安装pyOpenSSL，源码下载地址为https://github.com/pyca/pyopenssl 。下载完成后，运行python setup.py install安装即可。

3）安装lxml，使用pip install lxml。如果提示Microsoft Visual C++库没安装，则可以从http://www.microsoft.com/en-us/download/details.aspxid=44266 下载支持的库。

4）安装Scrapy，使用pip install Scrapy。安装完成后，在命令行中输入scrapy，如图12-2所示，如果不报错，则证明安装成功。



图12-2　Windows下安装Scrapy

2.Linux Ubuntu

Ubuntu下的安装比较简单。Python安装不再多说，Linux下绝大部分版本都预安装了Python环境，而且还预装了lxml和OpenSSL，所以直接就可以使用sudo pip install Scrapy进行安装，安装完成后，在shell中输入scrapy，出现如图12-3所示的效果即为安装成功。



图12-3　Ubuntu下安装Scrapy

有一点希望大家注意，下面的讲解所使用的Scrapy版本是1.0.5。





12.3　创建cnblogs项目


从本节开始正式学习Scrapy框架，下面以爬取我的博客（http://www.cnblogs.com/qiyeboy/ ）为例进行介绍，提取所有文章的链接、时间、标题和摘要，如图12-4所示。



图12-4　博客文章信息

在开始爬取之前，必须创建一个新的Scrapy项目。在命令行中切换到要存储的位置，比如D：\cnblogs文件夹，运行命令scrapy startproject cnblogSpider，即可创建一个名为cnblogSpider的项目，如图12-5所示。



图12-5　创建cnblogSpider项目

该命令将会在D：\cnblogs下创建包含下列内容的cnblogSpider目录：



* * *



cnblogSpider │ scrapy.cfg │ └─cnblogSpider │ items.py │ pipelines.py │ settings.py │ __init__.py │ └─spiders __init__.py



* * *



cnblogSpider目录下的文件分别是：

·scrapy.cfg：项目部署文件。

·cnblogSpider/：该项目的Python模块，之后可以在此加入代码。

·cnblogSpider/items.py：项目中的Item文件。

·cnblogSpider/pipelines.py：项目中的Pipelines文件。

·cnblogSpider/settings.py：项目的配置文件。

·cnblogSpider/spiders/：放置Spider代码的目录。





12.4　创建爬虫模块


首先编写爬虫模块，爬虫模块的代码都放置于spiders文件夹中。爬虫模块是用于从单个网站或者多个网站爬取数据的类，其应该包含初始页面的URL，以及跟进网页链接、分析页面内容和提取数据函数。创建一个Spider类，需要继承scrapy.Spider类，并且定义以下三个属性：

1）name：用于区别Spider。该名字必须是唯一的，不能为不同的Spider设定相同的名字。

2）start_urls：它是Spider在启动时进行爬取的入口URL列表。因此，第一个被获取到的页面的URL将是其中之一，后续的URL则从初始的URL的响应中主动提取。

3）parse（）：它是Spider的一个方法。被调用时，每个初始URL响应后返回的Response对象，将会作为唯一的参数传递给该方法。该方法负责解析返回的数据（response data）、提取数据（生成item）以及生成需要进一步处理的URL的Request对象。

现在创建CnblogsSpider类，保存于cnblogSpider/spiders目录下的cnblogs_spider.py文件中，代码如下：



* * *



# coding:utf-8 import scrapy class CnblogsSpider(scrapy.Spider): name = "cnblogs"# 爬虫的名称 allowed_domains = ["cnblogs.com"]# 允许的域名 start_urls = [ "http://www.cnblogs.com/qiyeboy/default.htmlpage=1" ] def parse(self, response): # 实现网页的解析 pass



* * *



这时候一个爬虫模块的基本结构搭建起来了，现在的代码只是实现了类似网页下载的功能。在命令行中切换到项目根目录下，如D：\cnblogs\cnblogSpider，在此目录下执行下列命令启动spider：



* * *



scrapy crawl cnblogs



* * *



效果如图12-6所示。crawl cnblogs的含义就是启动名称为cnblogs的爬虫。



图12-6　运行cnblogs爬虫

图中线框的位置是爬取起始URL时的打印信息。





12.5　选择器


爬虫模块创建完成后，仅仅拥有了网页下载功能，下面进行网页数据的提取。Scrapy有自己的一套数据提取机制，称为选择器（selector），因为它们通过特定的XPath或者CSS表达式来选择HTML文件中的某个部分。Scrapy选择器构建于lxml库之上，这意味着它们在速度和解析准确性上非常相似，用法也和之前讲的lxml解析基本类似。当然也可以脱离这套机制，使用BeautifulSoup包进行解析。





12.5.1　Selector的用法


之前的章节我们已经讲过XPath和CSS，这里不再赘述。下面主要说一下Selector的用法，Selector对象有四个基本的方法：

1）xpath（query）：传入XPath表达式query，返回该表达式所对应的所有节点的selector list列表。

2）css（query）：传入CSS表达式query，返回该表达式所对应的所有节点的selector list列表。

3）extract（）：序列化该节点为Unicode字符串并返回list列表。

4）re（regex）：根据传入的正则表达式对数据进行提取，返回Unicode字符串列表。regex可以是一个已编译的正则表达式，也可以是一个将被re.compile（regex）编译为正则表达式的字符串。

在CnblogsSpider类的parse（）方法中，其中一个参数是response，将response传入Selector（response）中就可以构造出一个Selector对象，进而调用以上的四个方法。还有简写的方式，传入的Response直接可以调用xpath和css方法，形如response.xpath（）或者response.css（）。

方法的调用很简单，更多的时间是花费在XPath和CSS表达式的构造。Scrapy提供了一种简便的方式来查看表达式是否正确、是否真的起作用。另起一个命令行窗口，在其中输入scrapy shell“http://www.cnblogs.com/qiyeboy/default.htmlpage=1 ”，记得后面的URL加上双引号，效果如图12-7所示。



图12-7　scrapy shell

其中方框圈出来的response就是访问以上网址获得响应，直接在scrapy shell中输入response.xpath（“.//*[@class=’postTitle‘]/a/text（）”）.extract（），就可以抽取出当前网址的所有文章的标题，返回的是Unicode格式，如图12-8所示。

大家可以用这种方法来测试自己写的XPath或者CSS表达式。

可能有人会说，直接使用Firepath测试不是更简单对，使用Firepath来获取和测试表达式更直观和方便。但是大家有没有想过动态网站，HTML网页都是经过渲染的，很多时候浏览器上显示的网页结构和程序访问获取的结构是不一样的，就会导致在Firepath获取和调试成功的表达式在程序中不起作用，这种情况的解决办法是将scrapy shell和Firepath结合起来使用。在scrapy shell中有一个功能是查看响应，在shell中输入view（response），可以将获取的响应在浏览器中打开，然后就可以使用Firepath获取和调试真实的表达式了。如图12-9所示，响应被存成了一个本地的html文件。



图12-8　测试表达式



图12-9　scrapy shell和Firepath结合处理





12.5.2　HTML解析实现


讲解完Selector的用法，下面抽取博客文章的数据。通过分析网页的结构，XPath表达式如下：

·所有文章：.//*[@class=’day‘]

·文章发表时间：.//*[@class=’dayTitle‘]/a/text（）

·文章标题内容：.//*[@class=’postTitle‘]/a/text（）

·文章摘要内容：.//*[@class=’postCon‘]/div/text（）

·文章链接：.//*[@class=’postTitle‘]/a/@href

parse（）方法代码如下：



* * *



def parse(self, response): # 实现网页的解析 # 首先抽取所有的文章 papers = response.xpath(".// *[@class='day']") # 从每篇文章中抽取数据 for paper in papers: url = paper.xpath(".// *[@class='postTitle']/a/@href").extract()[0] title = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] time = paper.xpath(".// *[@class='dayTitle']/a/text()").extract()[0] content = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] print url, title,time,content



* * *



代码先抽取出当前网页中的所有文章，然后从每篇文章中抽取数据，抽取的数据以列表的格式返回，直接取第一个即可。在命令中输入scrapy crawl cnblogs再次启动爬虫就可以看到抽取出的数据，如图12-10所示。



图12-10　抽取效果





12.6　命令行工具


既然用到了scrapy shell，本节就讲解一下Scrapy的命令行功能。Scrapy提供了两种类型的命令。一种必须在Scrapy项目中运行，是针对项目的命令；另外一种则不需要，属于全局命令。全局命令在项目中运行时的表现可能会与在非项目中的运行表现有些许差别，因为可能会使用项目的设定。

全局命令如下：

·startproject

·settings

·runspider

·shell

·fetch

·view

·version

·bench

startproject命令，语法：scrapy startproject<project_name>。用于在project_name文件夹下创建一个名为project_name的Scrapy项目。示例如下：



* * *



$ scrapy startproject myproject



* * *



settings命令，语法：scrapy settings[options]。在项目中运行时，该命令将会输出项目的设定值，否则输出Scrapy默认设定。示例如下：



* * *



$ scrapy settings --get BOT_NAME scrapybot $ scrapy settings --get DOWNLOAD_DELAY 0



* * *



runspider命令，语法：scrapy runspider<spider_file.py>。在未创建项目的情况下，运行一个编写好的spider模块。示例如下：



* * *



$ scrapy runspider cnblogs_spider.py



* * *



shell命令，语法：scrapy shell[url]。用来启动Scrapy shell，url为可选。示例如下：



* * *



$ scrapy shell "http://www.cnblogs.com/qiyeboy/default.htmlpage=1"



* * *



fetch命令，语法：scrapy fetch<url>。使用Scrapy下载器（downloader）下载给定的URL，并将获取到的内容送到标准输出。该命令以spider下载页面的方式获取页面，如果是在项目中运行，fetch将会使用项目中spider的属性访问。如果在非项目中运行，则会使用默认Scrapy downloader设定。示例如下：



* * *



$ scrapy fetch --nolog "http://www.cnblogs.com/qiyeboy/default.htmlpage=1" $ scrapy fetch --nolog --headers "http://www.cnblogs.com/qiyeboy/default.htmlpage=1"



* * *



view命令，语法：scrapy view<url>。在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现，和之前讲的view（response）效果一样。示例如下：



* * *



$ scrapy view "http://www.cnblogs.com/qiyeboy/default.htmlpage=1"



* * *



version命令，语法：scrapy version[-v]。输出Scrapy版本。配合-v运行时，该命令同时输出Python、Twisted以及平台的信息，方便bug提交。

bench命令，语法：scrapy bench。用于运行benchmark测试，测试Scrapy在硬件上的效率。

项目命令如下：

·crawl

·check

·list

·edit

·parse

·genspider

·deploy

crawl命令，语法：scrapy crawl<spider>。用来使用spider进行爬取，示例如下：



* * *



$ scrapy crawl cnblogs



* * *



check命令，语法：scrapy check[-l]<spider>。运行contract检查，示例如下：



* * *



$ scrapy check -l



* * *



list命令，语法：scrapy list。列出当前项目中所有可用的spider，每行输出一个spider。示例如下：



* * *



$ scrapy list



* * *



edit命令，语法：scrapy edit<spider>。使用设定的编辑器编辑给定的spider。该命令仅仅是提供一个快捷方式，开发者可以自由选择其他工具或者IDE来编写调试spider。



* * *



$ scrapy edit cnblogs



* * *



parse命令，语法：scrapy parse<url>[options]。获取给定的URL并使用相应的spider分析处理。如果提供--callback选项，则使用spider中的解析方法进行处理。支持的选项：

·--spider=SPIDER：跳过自动检测spider并强制使用特定的spider

·--a NAME=VALUE：设置spider的参数（可能被重复）

·--callback or-c：spider中用于解析response的回调函数

·--pipelines：在pipeline中处理item

·--rules or-r：使用CrawlSpider规则来发现用于解析response的回调函数

·--noitems：不显示爬取到的item

·--nolinks：不显示提取到的链接

·--nocolour：避免使用pygments对输出着色

·--depth or-d：指定跟进链接请求的层次数（默认：1）

·--verbose or-v：显示每个请求的详细信息

示例如下：



* * *



$ scrapy parse "http://www.cnblogs.com/qiyeboy/default.htmlpage=1" -c parse



* * *



genspider命令，语法：scrapy genspider[-t template]<name><domain>。可以在当前项目中创建spider。这仅仅是创建spider的一种快捷方法，该方法可以使用提前定义好的模板来生成spider，也可以自己创建spider的源码文件。示例如下：



* * *



$ scrapy genspider -l Available templates: basic crawl csvfeed xmlfeed $ scrapy genspider -d basic import scrapy class $classname(scrapy.Spider): name = "$name" allowed_domains = ["$domain"] start_urls = ( 'http://www.$domain/', ) def parse(self, response): pass $ scrapy genspider -t basic example example.com Created spider 'example' using template 'basic' in module: mybot.spiders.example



* * *



deploy命令，语法：scrapy deploy[<target：project>|-l<target>|-L]。将项目部署到Scrapyd服务，之后会用到。





12.7　定义Item


爬取的主要目标就是从非结构性的数据源提取结构性数据。CnblogsSpider类的parse（）方法中解析出了url、title、time、content等数据，但是如何将这些数据包装为结构化数据呢？scrapy提供Item类来满足这样的需求。Item对象是一种简单的容器，用来保存爬取到的数据，提供了类似于词典的API以及用于声明可用字段的简单语法。Item使用简单的class定义语法以及Field对象来声明。在新建的cnblogSpider项目中，有一个items.py文件，用来定义存储数据的Item类，这个类需要继承scrapy.Item。代码如下：



* * *



class CnblogspiderItem(scrapy.Item): # define the fields for your item here like: url = scrapy.Field() time = scrapy.Field() title = scrapy.Field() content = scrapy.Field()



* * *



我们对已经声明好的CnblogspiderItem进行操作，发现Item的操作方式和字典的操作方式非常相似。

·创建CnblogspiderItem对象



* * *



item = CnblogspiderItem(title="Python爬虫",content='爬虫开发')



* * *



·获取字段的值



* * *



print item['title'] print item.get('title')



* * *



·设置字段的值



* * *



item['title']="爬虫"



* * *



·获取所有的键和值



* * *



print item.keys() print item.items()



* * *



·Item的复制



* * *



item2 = CnblogspiderItem (item) item3 = item.copy()



* * *



·dict与item的转化



* * *



dict_item = dict(item) item = CnblogspiderItem({'title':'爬虫','content':'开发'})



* * *



除了以上的操作，还可以对Item进行扩展。通过继承原始的Item来扩展Item，用来添加更多的字段。例如拓展一下CnblogspiderItem，添加一个body字段：



* * *



class newCnblogItem(CnblogspiderItem): body = scrapy.Field()



* * *



也可以通过使用原字段的元数据，添加新的值或修改原来的值来扩展字段的元数据：



* * *



class newCnblogItem(CnblogspiderItem): title = scrapy.Field(CnblogspiderItem.fields['title'], serializer=my_serializer)



* * *



这段代码在保留所有原来的元数据值的情况下添加了title字段的serializer。

讲解完Item的用法，需要将parse（）中提取出的url、title、time、content封装成Item对象，parse（）方法的代码如下：



* * *



def parse(self, response): # 实现网页的解析 # 首先抽取所有的文章 papers = response.xpath(".// *[@class='day']") # 从每篇文章中抽取数据 for paper in papers: url = paper.xpath(".// *[@class='postTitle']/a/@href").extract()[0] title = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] time = paper.xpath(".// *[@class='dayTitle']/a/text()").extract()[0] content = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] item = CnblogspiderItem(url=url,title=title,time=time,content=content) yield item



* * *



代码最后使用yield关键字提交item，将parse方法打造成一个生成器，这是parse方法中最精彩的地方。





12.8　翻页功能


以上实现了当前网页数据的抽取，但是我们要抽取博客所有页面的文章，这就需要实现翻页功能。翻页功能的实现，本质上是构造Request并提交给Scrapy引擎的过程。

首先抽取下一页的链接，我们使用Selector中的re（）方法进行抽取，正则表达式为



* * *



<a href="(\S*)">下一页</a>



* * *



构造请求使用scrapy.Request对象。parse（）方法代码如下：



* * *



def parse(self, response): # 实现网页的解析 # 首先抽取所有的文章 papers = response.xpath(".// *[@class='day']") # 从每篇文章中抽取数据 for paper in papers: url = paper.xpath(".// *[@class='postTitle']/a/@href").extract()[0] title = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] time = paper.xpath(".// *[@class='dayTitle']/a/text()").extract()[0] content = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] item = CnblogspiderItem(url=url,title=title,time=time,content=content) yield item next_page = Selector(response).re(u'<a href="(\S*)">下一页</a>') if next_page: yield scrapy.Request(url=next_page[0],callback=self.parse)



* * *



Request对象的构造方法中URL为请求链接，callback为回调方法，回调方法用来指定由谁来解析此项Request请求的响应。





12.9　构建Item Pipeline


以上几节已经实现了cnblogs爬虫中网页的下载、解析和数据Item，下面我们需要将爬取到的数据进行持久化存储，这就要说到Item Pipeline。当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。

Item Pipeline主要有以下典型应用：

·清理HTML数据。

·验证爬取的数据的合法性，检查Item是否包含某些字段。

·查重并丢弃。

·将爬取结果保存到文件或者数据库中。





12.9.1　定制Item Pipeline


定制Item Pipeline的方法其实很简单，每个Item Pipeline组件是一个独立的Python类，必须实现process_item方法，方法原型如下：



* * *



process_item(self, item, spider)



* * *



每个Item Pipeline组件都需要调用该方法，这个方法必须返回一个Item（或任何继承类）对象，或者抛出DropItem异常，被丢弃的Item将不会被之后的Pipeline组件所处理。

参数说明：

·Item对象是被爬取的Item。

·Spider对象代表着爬取该Item的Spider。

我们需要将cnblogs爬虫爬取的Item存储到本地。定制的Item Pipeline代码位于cnblogSpider/pipelines.py，声明为CnblogspiderPipeline类，完整内容如下：



* * *



import json from scrapy.exceptions import DropItem class CnblogspiderPipeline(object): def __init__(self): self.file = open('papers.json', 'wb') def process_item(self, item, spider): if item['title']: line = json.dumps(dict(item)) + "\n" self.file.write(line) return item else: raise DropItem("Missing title in %s" % item)



* * *



process_item方法中，先对item中的title进行判断，如果不存在就抛出DropItem异常，进行丢弃，如果存在就将item存入JSON文件中，你可以定制自己想存储的方式，比如存到数据库中等。





12.9.2　激活Item Pipeline


定制完Item Pipeline，它是无法工作的，需要进行激活。要启用一个Item Pipeline组件，必须将它的类添加到settings.py中的ITEM_PIPELINES变量中。代码如下：



* * *



ITEM_PIPELINES = { 'cnblogSpider.pipelines.CnblogspiderPipeline': 300, }



* * *



ITEM_PIPELINES变量中可以配置很多个Item Pipeline组件，分配给每个类的整型值确定了它们运行的顺序，item按数字从低到高的顺序通过Item Pipeline，通常将这些数字定义在0～1000范围内。

激活完成后，将命令行切换到项目目录下，执行scrapy crawl cnblogs命令，就可以将数据存储到papers.json文件中，效果如图12-11所示。



图12-11　papers.json





12.10　内置数据存储


除了使用Item Pipeline实现存储功能，Scrapy内置了一些简单的存储方式，生成一个带有爬取数据的输出文件，通常叫做输出（feed），支持多种序列化格式。其自带支持的类型有：

·JSON

FEED_FORMAT：json

所用的内置输出类：JsonItemExporter

·JSON lines

FEED_FORMAT：jsonlines

所用的内置输出类：JsonLinesItemExporter

·CSV

FEED_FORMAT：csv

所用的内置输出类：CsvItemExporter

·XML

FEED_FORMAT：xml

所用的内置输出类：XmlItemExporter

·Pickle

FEED_FORMAT：pickle

所用的内置输出类：PickleItemExporter

·Marshal

FEED_FORMAT：marshal

所用的内置输出类：MarshalItemExporter

使用方式：将命令行切换到项目目录，比如想保存为CSV格式，输入命令scrapy crawl cnblogs-o papers.csv，效果如图12-12所示。



图12-12　papers.csv





12.11　内置图片和文件下载方式


有时在爬取产品的同时也想保存对应的图片，Scrapy为下载Item中包含的文件提供了一个可重用的Item Pipeline。这些Pipeline有些共同的方法和结构，我们称之为MediaPipeline。一般来说你会使用FilesPipeline或者ImagesPipeline。这两种Pipeline都实现了以下特性：

·避免重新下载最近已经下载过的数据。

·指定存储的位置和方式。

此外，ImagesPipeline还提供了额外特性：

·将所有下载的图片转换成通用的格式（JPG）和模式（RGB）。

·缩略图生成。

·检测图像的宽/高，确保它们满足最小限制。

这个管道也会为那些当前安排好要下载的图片保留一个内部队列，并将那些到达的包含相同图片的项目连接到该队列中，这可以避免多次下载几个Item共享的同一个图片。

当使用FilesPipeline时，典型的工作流程如下所示：

1）在一个爬虫里，抓取一个Item，把其中文件的URL放入file_urls组内。

2）Item从爬虫内返回，进入Item Pipeline。

3）当Item进入FilesPipeline，file_urls组内的URL将被Scrapy的调度器和下载器（这意味着调度器和下载器的中间件可以复用）安排下载，如果优先级更高，会在其他页面被抓取前处理。Item会在这个特定的管道阶段保持“locker”的状态，直到完成文件的下载（或者由于某些原因未完成下载）。

4）当文件下载完后，另一个字段（files）将被更新到结构中。这个组将包含一个字典列表，其中包括下载文件的信息，比如下载路径、源抓取地址（从file_urls组获得）和图片的校验码（checksum）。files列表中的文件顺序将和源file_urls组保持一致。如果某个图片下载失败，将会记录下错误信息，图片也不会出现在files组中。

当使用ImagesPipeline时，典型的工作流程如下所示：

1）在一个爬虫里，抓取一个Item，把其中图片的URL放入images_urls组内。

2）项目从爬虫内返回，进入Item Pipeline。

3）当Item进入ImagesPipeline，images_urls组内的URL将被Scrapy的调度器和下载器（这意味着调度器和下载器的中间件可以复用）安排下载，如果优先级更高，会在其他页面被抓取前处理。项目会在这个特定的管道阶段保持“locker”的状态，直到完成文件的下载（或者由于某些原因未完成下载）。

4）当文件下载完后，另一个字段（images）将被更新到结构中。这个组将包含一个字典列表，其中包括下载文件的信息，比如下载路径、源抓取地址（从images_urls组获得）和图片的校验码（checksum）。images列表中的文件顺序将和源images_urls组保持一致。如果某个图片下载失败，将会记录下错误信息，图片也不会出现在images组中。

Pillow用来生成缩略图，并将图片归一化为JPEG/RGB格式，因此为了使用ImagesPipeline，你需要安装这个库。Python Imaging Library（PIL）在大多数情况下是有效的，但众所周知，在一些设置里会出现问题，因此我们推荐使用Pillow而不是PIL，使用pip install pillow可以安装这个模块。

1.使用FilesPipeline

使用FilesPipeline非常简单，只需要三个步骤即可：

1）在settings.py文件的ITEM_PIPELINES中添加一条’scrapy.pipelines.files.FilesPipeline‘：1。

2）在item中添加两个字段，比如：



* * *



file_urls = scrapy.Field() files = scrapy.Field()



* * *



3）在settings.py文件中添加下载路径FILES_STORE、文件url所在的item字段FILES_URLS_FIELD和文件结果信息所在的item字段FILES_RESULT_FIELD，比如



* * *



FILES_STORE = 'D:\\cnblogs' FILES_URLS_FIELD = 'file_urls' FILES_RESULT_FIELD = 'files'



* * *



使用FILES_EXPIRES设置文件过期时间，示例如下：



* * *



FILES_EXPIRES = 30# 30天过期



* * *



2.使用ImagesPipeline

使用ImagesPipeline的基本步骤和FilesPipeline一样，不过针对的是图片下载，又添加了一些新的特性。基本步骤如下：

1）在settings.py文件的ITEM_PIPELINES中添加一条’scrapy.pipelines.images.Images Pipeline‘：1。

2）在item中添加两个字段，比如：



* * *



image_urls = scrapy.Field() images = scrapy.Field()



* * *



3）在settings.py文件中添加下载路径IMAGES_STORE、文件url所在的item字段IMAGES_URLS_FIELD和文件结果信息所在的item字段IMAGES_RESULT_FIELD，比如



* * *



IMAGES_STORE = 'D:\\cnblogs' IMAGES_URLS_FIELD= 'image_urls' IMAGES_RESULT_FIELD = 'images'



* * *



可以在settings.py中使用IMAGES_THUMBS制作缩略图，并设置缩略图尺寸大小。使用IMAGES_EXPIRES设置文件过期时间，示例如下：



* * *



IMAGES_THUMBS = { 'small': (50, 50), 'big': (270, 270), } IMAGES_EXPIRES = 30# 30天过期



* * *



如果想过滤特别小的图片可以使用IMAGES_MIN_HEIGHT和IMAGES_MIN_WIDTH来设置图片的最小高和宽。

通过上面讲解的内容，我们可以给cnblogs爬虫添加下载每篇文章中图片的功能。首先按照ImagesPipeline基本步骤进行配置。

settings.py文件中的设置如下：



* * *



ITEM_PIPELINES = { 'cnblogSpider.pipelines.CnblogspiderPipeline': 300, 'scrapy.pipelines.images.ImagesPipeline':1 } IMAGES_STORE = 'D:\\cnblogs' IMAGES_URLS_FIELD = 'cimage_urls' IMAGES_RESULT_FIELD = 'cimages' IMAGES_EXPIRES = 30 IMAGES_THUMBS = { 'small': (50, 50), 'big': (270, 270), }



* * *



items.py文件代码如下，添加了cimage_url和cimages字段：



* * *



class CnblogspiderItem(scrapy.Item): # define the fields for your item here like: url = scrapy.Field() time = scrapy.Field() title = scrapy.Field() content = scrapy.Field() cimage_urls = scrapy.Field() cimages = scrapy.Field()



* * *



cnblogs_spider.py文件代码如下，添加了parse_body方法，用于提取文章正文中的图片链接，同时还用到了Request的meta属性，用来将Item示例进行暂存，统一提交：



* * *



def parse(self, response): # 实现网页的解析 # 首先抽取所有的文章 papers = response.xpath(".// *[@class='day']") # 从每篇文章中抽取数据 for paper in papers: url = paper.xpath(".// *[@class='postTitle']/a/@href").extract()[0] title = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] time = paper.xpath(".// *[@class='dayTitle']/a/text()").extract()[0] content = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] item = CnblogspiderItem(url=url,title=title,time=time,content=content) request = scrapy.Request(url=url,callback=self.parse_body) request.meta['item'] = item# 将item暂存 yield request next_page = Selector(response).re(u'<a href="(\S*)">下一页</a>') if next_page: yield scrapy.Request(url=next_page[0],callback=self.parse) def parse_body(self,response): item = response.meta['item'] body = response.xpath(".// *[@class='postBody']") item['cimage_urls'] = body.xpath('.// img// @src').extract()# 提取图片链接 yield item



* * *



最后在命令行中切换到项目目录下，运行scrapy crawl cnblogs，开始爬取数据。爬取到的图片所在的目录结构如下：



* * *



cnblogs ├─full │ 0215c0dfe13fa7da8ed07467e94bc62d7f6e7583.jpg │ 059a17af502b604ea825d164d799d59406e9e573.jpg │ 09b23ca063bee1dc713cc37e023c7fd9709effac.jpg └─thumbs ├─big │ 0215c0dfe13fa7da8ed07467e94bc62d7f6e7583.jpg │ 059a17af502b604ea825d164d799d59406e9e573.jpg │ 09b23ca063bee1dc713cc37e023c7fd9709effac.jpg └─small 0215c0dfe13fa7da8ed07467e94bc62d7f6e7583.jpg 059a17af502b604ea825d164d799d59406e9e573.jpg 09b23ca063bee1dc713cc37e023c7fd9709effac.jpg



* * *



大家肯定会发现图片的名称很奇怪，图片名称是图片下载链接经过SHA1哈希后的值，由Scrapy自行处理。

以上讲解了Scrapy内置的FilesPipeline和ImagesPipeline，那么如何定制我们自己的FilesPipeline或者ImagesPipeline呢？我们需要继承FilesPipeline或者ImagesPipeline，重写get_media_requests和item_completed（）方法。下面以ImagesPipeline为例进行讲解。

1.get_media_requests（item，info）方法

在工作流程中可以看到，管道会得到图片的URL并从项目中下载。需要重写get_media_requests（）方法，并对各个图片URL返回一个Request：



* * *



def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url)



* * *



这些请求将由管道处理，当它们完成下载后，结果results将以2-元素的元组列表形式传送到item_completed（）方法，结果类似如下的形式：



* * *



[(True, {'checksum': '2b00042f7481c7b056c4b410d28f33cf', 'path': 'full/7d97e98f8af710c7e7fe703abc8f639e0ee507c4.jpg', 'url': 'http://www.example.com/images/product1.jpg'}), (True, {'checksum': 'b9628c4ab9b595f72f280b90c4fd093d', 'path': 'full/1ca5879492b8fd606df1964ea3c1e2f4520f076f.jpg', 'url': 'http://www.example.com/images/product2.jpg'}), (False, Failure(...))]



* * *



返回结果的格式解释如下：

·success是一个布尔值，当图片成功下载时为True，因为某个原因下载失败为False。

·如果success为True，image_info_or_error是一个包含下列关键字的字典，如果出问题时则为Twisted Failure。

·url是图片下载的url。这是从get_media_requests（）方法返回的请求的url。

·path是图片存储的路径（类似IMAGES_STORE）。

·checksum是图片内容的MD5hash。

2.item_completed（results，items，info）方法

当一个单独项目中的所有图片请求完成时（要么完成下载，要么因为某种原因下载失败），ImagesPipeline.item_completed（）方法将被调用。其中results参数就是get_media_requests下载完成之后返回的结果。item_completed（）方法需要返回一个输出，其将被送到随后的ItemPipelines，因此你需要返回或者丢弃项目，这和之前在ItemPipelines中的操作一样。

以下是item_completed（）方法的例子，其中我们将下载的图片路径存储到item中的image_paths字段里，如果其中没有图片，我们将丢弃项目：



* * *



from scrapy.exceptions import DropItem def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem("Item contains no images") item['image_paths'] = image_paths return item



* * *



以下是一个定制ImagesPipeline的完整例子，代码如下：



* * *



import scrapy from scrapy.contrib.pipeline.images import ImagesPipeline from scrapy.exceptions import DropItem class MyImagesPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem("Item contains no images") item['image_paths'] = image_paths return item



* * *





12.12　启动爬虫


本节主要讲解爬虫的启动，我们之前运行爬虫采取的都是命令行的方式，输入命令为scrapy crawl spider_name。除了这种方式，Scrapy还提供了API可以让我们在程序中启动爬虫。

由于Scrapy是在Twisted异步网络库上构建的，因此必须在Twisted reactor里运行。

第一种通用的做法是使用CrawlerProcess类，这个类内部将会开启Twisted reactor、配置log和设置Twisted reactor自动关闭。下面给我们的cnblogs爬虫添加启动脚本，在cnblogs_spider.py下面添加如下代码：



* * *



if __name__=='__main__': process = CrawlerProcess({ 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)' }) process.crawl(CnblogsSpider) process.start()



* * *



可以在CrawlerProcess初始化时传入设置的参数，使用crawl方式运行指定的爬虫类。

也可以在CrawlerProcess初始化时传入项目的settings信息，在crawl方法中传入爬虫的名字。代码如下：



* * *



if __name__=='__main__': process = CrawlerProcess(get_project_settings()) process.crawl('cnblogs') process.start()



* * *



另一种启动的办法是使用CrawlerRunner，这种方法稍微复杂一些。在spider运行结束后，必须自行关闭Twisted reactor，需要在CrawlerRunner.crawl所返回的对象中添加回调函数。代码如下：



* * *



if __name__=='__main__': configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'}) runner = CrawlerRunner() d = runner.crawl(CnblogsSpider) d.addBoth(lambda _: reactor.stop()) reactor.run()



* * *



代码中使用configure_logging配置了日志信息的打印格式，通过CrawlerRunner的crawl方法添加爬虫，并通过addBoth添加关闭Twisted reactor的回调函数。

以上的两种方式都是在一个进程中启动了一个爬虫，其实我们可以在一个进程中启动多个爬虫。第一种实现方式的示例代码如下：



* * *



import scrapy from scrapy.crawler import CrawlerProcess class MySpider1(scrapy.Spider): # Your first spider definition ... class MySpider2(scrapy.Spider): # Your second spider definition ... process = CrawlerProcess() process.crawl(MySpider1) process.crawl(MySpider2) process.start()



* * *



第二种实现方式的示例代码如下：



* * *



import scrapy from twisted.internet import reactor from scrapy.crawler import CrawlerRunner from scrapy.utils.log import configure_logging class MySpider1(scrapy.Spider): # Your first spider definition ... class MySpider2(scrapy.Spider): # Your second spider definition ... configure_logging() runner = CrawlerRunner() runner.crawl(MySpider1) runner.crawl(MySpider2) d = runner.join() d.addBoth(lambda _: reactor.stop()) reactor.run()



* * *



第三种实现方式的示例代码如下：



* * *



from twisted.internet import reactor, defer from scrapy.crawler import CrawlerRunner from scrapy.utils.log import configure_logging class MySpider1(scrapy.Spider): # Your first spider definition ... class MySpider2(scrapy.Spider): # Your second spider definition ... configure_logging() runner = CrawlerRunner() @defer.inlineCallbacks def crawl(): yield runner.crawl(MySpider1) yield runner.crawl(MySpider2) reactor.stop() crawl() reactor.run()



* * *





12.13　强化爬虫


本节讲解一下Scrapy中的调试方法、异常和控制运行状态等内容，可以帮助我们更好地使用Scrapy编写爬虫。





12.13.1　调试方法


Scrapy中共有三种比较常用的调试技术：Parse命令、Scrapy shell和logging。下面以cnblogs爬虫为例讲解以上三种技术。

1.Parse命令

检查spider输出的最基本方法是使用Parse命令。这能让你在函数层上检查spider各个部分的效果，其十分灵活并且易用，不过不能在代码中调试。

查看特定url爬取到的item，命令格式为scrapy parse--spider=<spidename>-c<parse_item>-d 2<item_url>。在命令行中切换到项目目录下，输入scrapy parse--spider=cnblogs-c parse-d 2“http://www.cnblogs.com/qiyeboy/default.html?page=1 ”，效果如图12-13所示。



图12-13　parse命令

配合使用--verbose或-v选项，可以查看各个层次的详细状态。

2.Scrapy shell

尽管Parse命令对检查spider的效果十分有用，但除了显示收到的response及输出外，其对检查回调函数内部的过程并没有提供什么便利。这个时候可以通过scrapy.shell.inspect_response方法来查看spider的某个位置中被处理的response，以确认期望的response是否到达特定位置。在CnblogsSpider类中parse方法里添加两句代码：



* * *



def parse(self, response): # 实现网页的解析 # 首先抽取所有的文章 papers = response.xpath(".// *[@class='day']") # 从每篇文章中抽取数据 from scrapy.shell import inspect_response inspect_response(response, self) for paper in papers: url = paper.xpath(".// *[@class='postTitle']/a/@href").extract()[0] title = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] time = paper.xpath(".// *[@class='dayTitle']/a/text()").extract()[0] content = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] item = CnblogspiderItem(url=url,title=title,time=time,content=content) request = scrapy.Request(url=url,callback=self.parse_body) request.meta['item'] = item yield request next_page = Selector(response).re(u'<a href="(\S*)">下一页</a>') if next_page: yield scrapy.Request(url=next_page[0],callback=self.parse)



* * *



我们使用命令行执行程序时，当程序运行到inspect_response方法时会暂停，并切换进shell中，可以方便我们对当前的response进行调试，效果如图12-14所示。



图12-14　inspect_response方法使用

这时可以在shell中调试Xpath，或者查看当前响应内容。

如果调试完了，可以点击Ctrl-D来退出终端，恢复爬取，当程序再次运行到inspect_response方法时再次暂停，这样可以帮助我们了解每一个响应的细节。

3.logging

记录（logging）是另一个获取spider运行信息的方法。虽然不是那么方便，但好处是日志的内容在以后的运行中也可以看到。

以上就是Scrapy调试的三种方式，其实还有一种我比较喜欢的调试方式。首先将爬虫改写成API启动的方式，然后使用Pycharm打开整个爬虫项目，设置断点进行Debug调试，效果如图12-15所示。



图12-15　Debug调试





12.13.2　异常


下面是Scrapy提供的异常及其用法，如表12-1所示。

表12-1　Scrapy提供的异常及其用法





12.13.3　控制运行状态


Scrapy提供了内置的telnet终端，以供检查、控制Scrapy运行的进程。telnet终端是一个自带的Scrapy扩展。该扩展默认为启用，不过也可以关闭。

1.访问telnet终端

telnet终端监听设置中定义的TELNETCONSOLE_PORT默认为6023。Windows及大多数Linux发行版都自带了所需的telnet程序，所以访问本地Scrapy直接在命令行中输入：



* * *



telnet localhost 6023



* * *



2.telnet终端中可用的变量

为了方便，Scrapy telnet提供了一些默认定义的变量，如表12-2所示。

表12-2　telnet变量



3.使用示例

在终端中可以使用Scrapy引擎的est（）方法来快速查看状态，示例如下：



* * *



>>> est() Execution engine status time()-engine.start_time : 424.530999899 engine.has_capacity() : False len(engine.downloader.active) : 16 engine.scraper.is_idle() : False engine.spider.name : jiandan engine.spider_is_idle(engine.spider) : False engine.slot.closing : False len(engine.slot.inprogress) : 18 len(engine.slot.scheduler.dqs or []) : 0 len(engine.slot.scheduler.mqs) : 1 len(engine.scraper.slot.queue) : 0 len(engine.scraper.slot.active) : 2 engine.scraper.slot.active_size : 160265 engine.scraper.slot.itemproc_size : 2 engine.scraper.slot.needs_backout() : False



* * *



暂停、恢复和停止Scrapy引擎：

·暂停：



* * *



>>> engine.pause() >>>



* * *



·恢复：



* * *



>>> engine.unpause() >>>



* * *



·停止：



* * *



>>> engine.stop() Connection closed by foreign host.



* * *



4.配置telnet

在Settings.py中配置IP和端口：

·TELNETCONSOLE_PORT：默认为[6023，6073]，telnet终端使用的端口范围。如果设为None或0，则动态分配端口。

·TELNETCONSOLE_HOST：默认为’127.0.0.1‘，telnet终端监听的接口。





12.14　小结


本章通过cnblogs项目讲解了Scrapy的基本用法，从爬虫项目的创建到爬虫项目的最终运行，已经将Scrapy的基本功能都涵盖了。通过本章的学习，基本上可以将之前的项目换成Scrapy来实现。





第13章　深入Scrapy爬虫框架


本章将深入讲解Scrapy框架，不仅对上一章讲的内容进行了深入拓展，还讲解了一些新的知识点，包括中间件的重写，框架的扩展等，尽可能让大家对Scrapy有深入的理解。本章的内容建立在官方文档（https://doc.scrapy.org/en/latest/intro/install.html ）的基础上，并根据本人使用Scrapy的项目经验，对常用的重要知识点进行讲解，但肯定还有没涉及到的内容，大家可以学完本章之后阅读官方文档。





13.1　再看Spider


上一章讲解了Spider模块的基本用法，本节继续讲解Spider的其他用法。Spider类定义了如何爬取某个或某些网站，包括了爬取的动作（例如是否跟进链接），以及如何从网页的内容中提取结构化数据item。换句话说，Spider是定义爬取的动作及分析网页结构的地方。

对Spider来说，爬取的循环流程如下：

1）以入口URL初始化Request，并设置回调函数。此Request下载完毕返回Response，并作为参数传给回调函数。spider中初始的Request是通过调用start_requests（）方法来获取的，start_requests（）读取start_urls中的URL，并以parse为回调函数生成Request。

2）在回调函数内分析Response，返回Item对象、dict、Request或者一个包括三者的可迭代容器。其中返回的Request对象之后会经过Scrapy处理，下载相应的内容，并调用设置的回调函数，可以是parse（）或者其他函数。

3）在回调函数内，可以使用选择器（Selectors）或者其他第三方解析器来分析response，并根据分析的数据生成item。

4）最后，由spider返回的item可以经由Item Pipeline被存到数据库或使用Feed exports存入到文件中。

1.scrapy.Spider

上一章创建Spider模块是通过继承scrapy.Spider类来实现，这是一种最简单的spider。每个spider必须继承自scrapy.Spider类（包括Scrapy自带的spider以及自定义的spider）。Spider并没有提供什么特殊的功能，仅仅提供了start_requests（）的默认实现，读取并请求spider属性中的start_urls，并根据返回的response调用spider的parse方法。

Spider类常用的属性为：

·name。定义spider名字的字符串，Scrapy使用spider的名字来定位和初始化spider，所以它必须是唯一的。不过可以生成多个相同的spider实例，这没有任何限制。name是spider最重要的属性，而且是必需的。一个常见的做法是以该网站的域名来命名spider。例如，如果spider爬取cnblogs.com，该spider通常会被命名为cnblogs。

·allowed_domains。可选。包含了spider允许爬取的域名列表。当OffsiteMiddleware组件启用时，域名不在列表中的URL不会被跟进。

·start_urls为URL列表。当没有使用start_requests（）方法配置Requests时，Spider将从该列表中开始进行爬取，因此第一个被获取到的页面的URL将是该列表之一。后续的URL将会从获取到的数据中提取。

·custom_settings。该设置是一个dict。当启动spider时，该设置将会覆盖项目级的设置。由于设置必须在初始化前被更新，所以该属性必须定义为class属性。

·crawler。该属性在初始化class后，由类方法from_crawler（）设置，并且链接了本spider实例对应的Crawler对象。Crawler包含了很多项目中的组件，作为单一的入口点（例如插件、中间件、信号管理器等）。

常用的方法如下：

·start_requests（）方法。该方法必须返回一个可迭代对象，该对象包含了spider用于爬取的第一个Request。当spider启动爬取并且未制定URL时，该方法被调用。当指定了URL时，make_requests_from_url将被调用来创建Request对象。该方法仅仅会被Scrapy调用一次，因此可以将其实现为生成器。该方法的默认实现是使用start_urls的url生成Request。如果想要修改最初爬取某个网站的Request对象，可以重写该方法。例如在进行深层次爬取时，在启动阶段需要POST登录某个网站，获取用户权限，代码如下：



* * *



class MySpider(scrapy.Spider): name = 'myspider' def start_requests(self): return [scrapy.FormRequest("http://www.example.com/login", formdata={'user': 'john', 'pass': ' secret'},callback=self.login)] def login(self, response): pass



* * *



make_requests_from_url（url）方法。该方法接受一个URL并返回用于爬取的Request对象。该方法在初始化request时被start_requests（）调用，也用于转化URL为Request。默认未被复写（overridden）的情况下，该方法返回的Request对象中，parse作为回调函数

·parse（response）方法。response参数即用于分析的response。当response没有指定回调函数时，该方法是Scrapy处理下载的response的默认方法。parse负责处理response并返回处理的数据以及跟进的URL，该方法及其他的Request回调函数必须返回一个包含Request、dict或Item的可迭代的对象。

·closed（reason）方法。当spider关闭时，该函数被调用。可以用来在spider关闭时释放占用的资源。

Scrapy除了提供了Spider类作为基类进行拓展，还提供了CrawlSpider、XMLFeedSpider、CSVFeedSpider和SitemapSpider等类来实现不同的爬虫任务。下面主要讲解CrawlSpider和XMLFeedSpider的用法，其他用法类似。

2.CrawlSpider

CrawlSpider类常用于爬取一般网站，其定义了一些规则（rule）来提供跟进链接功能，使用非常方便。除了从Spider继承过来的属性外，还提供了一个新的属性rules，该属性是一个包含一个或多个Rule对象的集合，每个Rule对爬取网站的动作定义了特定的规则。如果多个Rule匹配了相同的链接，则根据它们在rules属性中被定义的顺序，第一个会被使用。CrawlSpider也提供了一个可复写的方法parse_start_url（response），当start_urls的请求返回时，该方法被调用。该方法分析最初的响应，并返回一个Item对象或者一个Request对象或者一个可迭代的包含二者的对象。

Rule类的原型为：



* * *



scrapy.contrib.spiders.Rule(link_extractor,callback=None,cb_kwargs=None,follow=None, process_links=None, process_request=None)



* * *



构造参数说明：

·link_extractor是一个LinkExtractor对象，其定义了如何从爬取到的页面提取链接。

·callback是一个callable或string，该spider中与string同名的函数将会被调用。每次从link_extractor中获取到链接时将会调用该函数。该回调函数接受一个response作为第一个参数，并返回Item或Request对象。当编写爬虫规则时，应避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了parse方法，CrawlSpider将会运行失败。

·cb_kwargs包含传递给回调函数的参数的字典。

·follow是一个布尔值，指定了根据该规则从response提取的链接是否需要跟进。如果callback为None，follow默认设置为True，否则默认为False。

·process_links是一个callable或string，该spider中与string同名的函数将会被调用。从link_extractor中获取到链接列表时将会调用该函数，主要用来过滤链接。

·process_request是一个callable或string，该spider中与string同名的函数将会被调用。该规则提取到每个Request时都会调用该函数，该函数必须返回一个Request或者None，用来过滤Request。

下面将CnblogsSpider类进行一下改造，继承CrawlSpider来实现同样的功能。代码如下：



* * *



class CnblogsSpider(CrawlSpider): name = 'cnblogs' allowed_domains = ["cnblogs.com"]# 允许的域名 start_urls = [ "http://www.cnblogs.com/qiyeboy/default.htmlpage=1" ] rules = ( Rule(LinkExtractor(allow=("/qiyeboy/default.html\page=\d{1,}",)), follow=True, callback='parse_item' ), ) def parse_item(self,response): papers = response.xpath(".// *[@class='day']") # 从每篇文章中抽取数据 for paper in papers: url = paper.xpath(".// *[@class='postTitle']/a/@href").extract()[0] title = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] time = paper.xpath(".// *[@class='dayTitle']/a/text()").extract()[0] content = paper.xpath(".// *[@class='postTitle']/a/text()").extract()[0] item = CnblogspiderItem(url=url,title=title,time=time,content=content) request = scrapy.Request(url=url,callback=self.parse_body) request.meta['item'] = item yield request def parse_body(self,response): item = response.meta['item'] body = response.xpath(".// *[@class='postBody']") item['cimage_urls'] = body.xpath('.// img// @src').extract()# 提取图片链接 yield item



* * *



在以上代码中，重点说一下Rule的定义，将其中的Rule改成下面的情况：



* * *



Rule(LinkExtractor(allow=("/qiyeboy/default.html\page=\d{1,}",)),)



* * *



这句话中不带callback，说明只是“跳板”，即只下载网页并根据allow中匹配的链接，继续遍历下一步的页面。在很多情况下，我们并不是只抓取某个页面，而需要“顺藤摸瓜”，从几个种子页面，依次递进，最终定位到我们想要的页面。LinkExtractor对象的构造也很重要，用来产生过滤规则。LinkExtractor常用的参数有：

·allow：提取满足正则表达式的链接。

·deny：排除正则表达式匹配的链接，优先级高于allow。

·allow_domains：允许的域名，可以是str或list。

·deny_domains：排除的域名，可以是str或list。

·restrict_xpaths：提取满足XPath选择条件的链接，可以是str或list。

·restrict_css：提取满足CSS选择条件的链接，可以是str或list。

·tags：提取指定标记下的链接，默认从a和area中提取，可以是str或list。

·attrs：提取满足拥有属性的链接，默认为href，类型为list。

·unique：链接是否去重，类型为Boolean。

·process_value：值处理函数，优先级大于allow。

注意 　rules属性中即使只有一个Rule实例，后面也要用逗号“，”分隔。

3.XMLFeedSpider

XMLFeedSpider被设计用于通过迭代各个节点来分析XML源。迭代器可以从Iternodes、XML、HTML中选择，但是XML以及HTML迭代器需要先读取所有DOM再分析，性能较低，一般推荐使用Iternodes。不过使用HTML作为迭代器能有效应对不标准的XML。在XMLFeedSpider中，需要定义下列类属性来设置迭代器以及标记名称：

·iterator用于确定使用哪个迭代器的string，默认值为iternodes。可选项有：

iternodes：一个高性能的基于正则表达式的迭代器

html：基于Selector的迭代器。

xml：基于Selector的迭代器。

·itertag为一个包含开始迭代的节点名的string。

·namespaces称为命名空间，一个由（prefix，url）元组（tuple）所组成的list，其定义了在该文档中会被Spider处理的可用的namespace。prefix和url会被自动调用，由register_namespace（）方法生成namespace。示例如下：



* * *



class YourSpider(XMLFeedSpider): namespaces = [('n', 'http://www.sitemaps.org/schemas/sitemap/0.9')] itertag = 'n:url'



* * *



除了提供了以上的属性，XMLFeedSpider还提供了下列可以被重写的方法：

·adapt_response（response）。该方法在Spider分析Response前被调用，可以在Response被分析之前使用该函数来修改Response内容。该方法接受一个Response并返回一个Response，返回的Response可以是修改过的，也可以是原来的。

·parse_node（response，selector）。当节点符合提供的itertag时，该方法被调用。接收到的Response以及相应的Selector作为参数传递给该方法，需要返回一个Item对象或者Request对象或者一个包含二者的可迭代对象。

·process_results（response，results）。当Spider返回Item或Request时，该方法被调用。该方法的目的是在结果返回给框架核心之前做最后的处理例如修改Item的内容。一个结果列表Results及对应的Response作为参数传递给该方法，其结果必须返回一个结果列表，可以包含Item或者Request对象。

现在很多博客都有RSS订阅功能，输出的是XML格式。下面使用XMLFeedSpider来解析订阅内容，以http://feed.cnblogs.com/blog/u/269038/rss 为例，如图13-1所示。



图13-1　RSS订阅

代码如下：



* * *



class XMLSpider(XMLFeedSpider): name = 'xmlspider' allowed_domains = ['cnblogs.com'] start_urls = ['http://feed.cnblogs.com/blog/u/269038/rss'] iterator = 'html' # This is actually unnecessary, since it's the default value itertag = 'entry' def adapt_response(self,response): return response def parse_node(self, response, node): print node.xpath('id/text()').extract()[0] print node.xpath('title/text()').extract()[0] print node.xpath('summary/text()').extract()[0]



* * *



解析效果如图13-2所示。



图13-2　RSS订阅解析





13.2　Item Loader


在讲Item Loader之前，首先回顾一下Item的用法。Item对象是种简单的容器，保存了爬取到的数据，提供了类似于词典的API以及用于声明可用字段的简单语法。Item使用简单的class定义语法以及Field对象来声明，例如定义一个Product类：



* * *



import scrapy class Product(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field(serializer=str)



* * *



以下都以Product类为例，讲解Item与Item Loaders的关系。





13.2.1　Item与Item Loader


Item Loader提供了一种便捷的方式填充抓取到的Items。虽然Items可以使用自带的类字典形式API填充，但是Items Loader提供了更便捷的API，可以分析原始数据并对Item进行赋值。换句话说，Items提供保存抓取数据的容器，而Item Loader提供的是填充容器的机制。

Item Loader提供的是一种灵活、高效的机制，可以更方便地被spider或HTML、XML等文件扩展，重写不同字段的解析规则，这对大型的爬虫项目的后期维护非常有利，拓展新的功能更加方便。下面是Item Loader在Spider中的典型应用方式，代码如下：



* * *



def parse(self, response): l = ItemLoader(item=Product(), response=response) l.add_xpath('name', '// div[@class="product_name"]') l.add_xpath('name', '// div[@class="product_title"]') l.add_xpath('price', '// p[@id="price"]') l.add_css('stock', 'p# stock]') l.add_value('last_updated', 'today') return l.load_item()



* * *



我们可以发现name字段被从页面中两个不同的XPath位置提取：



* * *



l.add_xpath('name', '// div[@class="product_name"]') l.add_xpath('name', '// div[@class="product_title"]')



* * *



也就是说使用add_xpath方法，将数据从两处XPath位置上收集起来，之后会分配给name字段。

类似操作被应用于price、stock和last_updated，pirce的字段数据通过XPath方式收集，stock字段数据通过CSS选择器方式收集，而last_updated字段数据直接被填充字符串today。当所有的数据被收集起来后，使用l.load_item（）将数据实际填充到Item中。





13.2.2　输入与输出处理器


从上面的分析可以看到，Item Loader负责了数据的收集、处理和填充，Item仅仅承载了数据本身而已。数据的收集、处理和填充，归功于Item Loader中两个重要的组件：输入处理器（input processors）和输出处理器（output processors）。下面说一下Item Loaders这两个处理器的职能：

·首先Item Loader在每个字段中都包含了一个输入处理器和一个输出处理器

·输入处理器收到response后时立刻通过add_xpath（）、add_css（）或者add_value（）等方法提取数据，经过输入处理器的结果被收集起来并且保存在ItemLoader内，这个时候数据还没有给Item。

·收集到所有的数据后，调用ItemLoader.load_item（）方法来填充并返回Item对象。load_item（）方法内部先调用输出处理器来处理收集到的数据，处理后的结果最终存入Item中。

说完了输入和输出处理器的职能，下面声明一个Item Loader。Item Loader的声明类似于Items，以class的语法来声明，代码如下：



* * *



from scrapy.contrib.loader import ItemLoader from scrapy.contrib.loader.processor import TakeFirst, MapCompose, Join class ProductLoader(ItemLoader): default_output_processor = TakeFirst() name_in = MapCompose(unicode.title) name_out = Join() price_in = MapCompose(unicode.strip) # ...



* * *



代码中输入处理器以_in为后缀来声明，输出处理器以_out为后缀来声明，也可以用ItemLoader.default_input_processor和ItemLoader.default_output_processor属性来声明默认的输入/输出处理器。

除了可以在ItemLoader类中声明输入输出处理器，也可以在Item中声明。示例如下：



* * *



import scrapy from scrapy.contrib.loader.processor import Join, MapCompose, TakeFirst from w3lib.html import remove_tags def filter_price(value): if value.isdigit(): return value class ProductItem(scrapy.Item): name = scrapy.Field( input_processor=MapCompose(remove_tags), output_processor=Join(), ) price = scrapy.Field( input_processor=MapCompose(remove_tags, filter_price), output_processor=TakeFirst(), )



* * *



以上说了三种输入输出处理器的声明方式：

·ItemLoader类中声明类似field_in和field_out的属性。

·Item的字段中声明。

·Item Loader默认处理器：ItemLoader.default_input_processor（）和ItemLoader.default_output_processor（）。

这种三种方式的响应优先级是从上到下依次递减。





13.2.3　Item Loader Context


Item Loader Context是一个任意的键值对字典，能被Item Loader中的输入输出处理器所共享。它在Item Loader声明、实例化、使用的时候传入，可以调整输入输出处理器的行为。举个例子，假设有个parse_length方法用于接收text值并且获取其长度：



* * *



def parse_length(text, loader_context): unit = loader_context.get('unit', 'm') # ... length parsing code goes here ... return parsed_length



* * *



通过接收一个loader_context参数，这个方法告诉Item Loader它能够接收Item Loader context，因此当这个方法被调用时，Item Loader能将当前的active Context传递给它。

有以下几种方式可以修改Item Loader Context的值：

·通过context属性修改当前active Item Loader Context：



* * *



loader = ItemLoader(product) loader.context[‘unit’] = ‘cm’



* * *



·在Item Loader实例化的时候：



* * *



loader = ItemLoader(product, unit=’cm’)



* * *



·对于那些支持Item Loader Context实例化的输入输出处理器（例如MapCompose），可以在Item Loader定义时修改Context：



* * *



class ProductLoader(ItemLoader): length_out = MapCompose(parse_length, unit=’cm’)



* * *





13.2.4　重用和扩展Item Loader


当爬虫项目越来越大，使用的Spider越来越多时，项目的维护将成为一个要紧的问题。特别是维护每一个Spider中许多而且不同的解析规则时，会出现很多异常，这个时候需要考虑重用和拓展的问题了。

Item Loader本身的设计就是为了减轻维护解析规则的负担，而且提供了方便的接口，用来重写和拓展他们。Item Loader支持通过传统Python类继承的方式来处理不同Spider解析的差异。

比如你之前写了一个ProductLoader来提取和解析某家公司网站的产品名称Plasma TV，但是一段时间之后公司网站更新，产品用三个短线封装起来，如---Plasma TV---。现在的需求是去掉这些短线，提取其中的产品名。示例代码如下：



* * *



def strip_dashes(x): return x.strip('-') class SiteSpecificLoader(ProductLoader): name_in = MapCompose(strip_dashes, ProductLoader.name_in)



* * *



通过继承ProductLoader类，通过strip_dashes方法将name中的短线去掉，这便是拓展输入处理器的方法。

对于输出处理器，更常用的方式是在Item字段元数据里声明，因为通常它们依赖于具体的字段而不是网站，这个可以参考13.2.2节在Item的字段中声明输入和输出处理器。





13.2.5　内置的处理器


除了可以使用可调用的函数作为输入输出处理器，Scrapy提供了一些常用的处理器。例如MapCompose，通常用于输入处理器，能把多个方法执行的结果按顺序组合起来产生最终的输出。

下面是一些内置的处理器。

1.Identity

类原型：class scrapy.loader.processors.Identity

最简单的处理器，不进行任何处理，直接返回原来的数据，无参数。

2.TakeFirst

类原型：class scrapy.loader.processors.TakeFirst

返回第一个非空值，常用于单值字段的输出处理器，无参数。

在Scrapy shell运行示例如下：



* * *



>>> from scrapy.loader.processors import TakeFirst >>> proc = TakeFirst() >>> proc(['', 'one', 'two', 'three']) 'one'



* * *



3.Join

类原型：class scrapy.loader.processors.Join（separator=u’‘）

返回用分隔符separator连接后的值，分隔符separator默认为空格。不接受Loader contexts。当使用默认分隔符的时候，这个处理器等同于Python字符串对象中的join方法：’‘.join。

在Scrapy shell运行示例如下：



* * *



>>> from scrapy.loader.processors import Join >>> proc = Join() >>> proc(['one', 'two', 'three']) u'one two three' >>> proc = Join('<br>') >>> proc(['one', 'two', 'three']) u'one<br>two<br>three'



* * *



4.Compose

类原型：class scrapy.loader.processors.Compose（*functions，**default_loader_context）

用给定的多个方法的组合来构造处理器，每个输入值被传递到第一个方法，然后其输出再传递到第二个方法，诸如此类，直到最后一个方法返回整个处理器的输出。默认情况下，当遇到None值的时候停止处理，可以通过传递参数stop_on_none=False改变这种设定。

在Scrapy shell运行示例如下：



* * *



>>> from scrapy.loader.processors import Compose >>> proc = Compose(lambda v: v[0], str.upper) >>> proc(['hello', 'world']) 'HELLO'



* * *



每个方法可以选择接收一个loader_context参数。

5.MapCompose

类原型：class scrapy.loader.processors.MapCompose（*functions，**default_loader_context）

和Compose类似，也是用给定的多个方法的组合来构造处理器，不同的是内部结果在方法间传递的方式：

·处理器的输入值是被迭代处理的，每一个元素被单独传入第一个函数进行处理，处理的结果被串连起来形成一个新的迭代器，并被传入第二个函数，以此类推，直到最后一个函数。最后一个函数的输出被连接起来形成处理器的输出。

·每个函数能返回一个值或者一个值列表，也能返回None。如果返回值是None，此值会被下一个函数所忽略。

·这个处理器提供了方便的方式来组合多个处理单值的函数。因此它常用于输入处理器，因为通过selectors中的extract（）函数提取出来的值是一个unicode strings列表。

在Scrapy shell运行示例如下：



* * *



>>> def filter_world(x): ... return None if x == 'world' else x ... >>> from scrapy.loader.processors import MapCompose >>> proc = MapCompose(filter_world, unicode.upper) >>> proc([u'hello', u'world', u'this', u'is', u'scrapy']) [u'HELLO, u'THIS', u'IS', u'SCRAPY']



* * *



与Compose处理器类似，它也能接受Loader context。

6.SelectJmes

类原型：class scrapy.loader.processors.SelectJmes（json_path）

使用指定的json_path查询并返回值。需要jmespath的支持，而且每次只接受一个输入。jmespath：https://github.com/jmespath/jmespath.py 。

在Scrapy shell运行示例如下：



* * *



>>> from scrapy.loader.processors import SelectJmes, Compose, MapCompose >>> proc = SelectJmes("foo") # for direct use on lists and dictionaries >>> proc({'foo': 'bar'}) 'bar' >>> proc({'foo': {'bar': 'baz'}}) {'bar': 'baz'}



* * *



和JSON一起使用：



* * *



>>> import json >>> proc_single_json_str = Compose(json.loads, SelectJmes("foo")) >>> proc_single_json_str('{"foo": "bar"}') u'bar' >>> proc_json_list = Compose(json.loads, MapCompose(SelectJmes('foo'))) >>> proc_json_list('[{"foo":"bar"}, {"baz":"tar"}]') [u'bar']



* * *





13.3　再看Item Pipeline


之前讲解了Item Pipeline的创建和激活，Item Pipeline还有三个方法非常常用，也很重要。

·open_spider（self，spider）

参数：spider是一个Spider对象，代表被开启的Spider。

当spider被开启时，这个方法被调用。

·close_spider（self，spider）

参数：spider是一个Spider对象，代表被关闭的Spider。

当spider被关闭时，这个方法被调用。

·from_crawler（cls，crawler）

参数：crawler是一个Crawler对象。

这个类方法从Crawler属性中创建一个pipeline实例。Crawler对象能够接触所有Scrapy的核心组件，比如settings和signals。

下面看一个例子，就可以明白每个方法的使用，该例子的功能使用MongoDB进行数据存储：



* * *



import pymongo class MongoPipeline(object): collection_name = 'scrapy_items' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE', 'items') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].insert(dict(item)) return item



* * *



代码中首先通过from_crawler方法，获取setting中的MongoDB的URL和数据库名称，从而创建一个MongoPipeline实例。在Spider开始运行时，在open_spider方法中建立数据库连接。当Spider关闭时，在close_spider方法中关闭数据库连接。

之前激活Item Pipeline时，是在settings.py里面配置Pipeline，这里配置的Pipeline会作用于所有的Spider。假如项目中有很多Spider在运行，Item Pipeline的处理就会很麻烦，你可以通过process_item（self，item，spider）中的Spider参数判断是来自哪个爬虫，但是这种方法很冗余，更好的做法是配置Spider类中的custom_settings对象，为每一个Spider配置不同的Pipeline。示例如下：



* * *



class MySpider(CrawlSpider): ... # 自定义配置 custom_settings = { 'ITEM_PIPELINES': { 'test.pipelines.TestPipeline.TestPipeline': 1, } }



* * *





13.4　请求与响应


在编写Spider模块中接触最紧密的是请求和响应。上一章我们讲解了如何简单地构造Request对象和解析Response对象，下面对这两个对象进行详细分析。





13.4.1　Request对象


一个Request对象代表着一个HTTP请求，通常在Spider类中产生，然后传递给下载器，最后返回一个响应。

类原型：class scrapy.http.Request（url[，callback，method=’GET‘，headers，body，cookies，meta，encoding=’utf-8‘，priority=0，dont_filter=False，errback]）

构造参数说明：

·url（string）：请求的链接。

·callback（callable）：指定用于解析请求响应的方法。如果没有指定，默认使用spider的parse（）方法。

·method（string）：HTTP请求方式，默认为GET。

·meta（dict）：可以用来初始化Request.meta属性。

·body（str or unicode）：请求的body。

·headers（dict）：请求头。如果传入的为None，请求头不会被发送。

·cookies（dict or list）：请求的cookie信息。

·encoding（string）：请求的编码，默认为UTF-8。

·priority（int）：请求的优先级，默认为0。优先级被调度器用来安排处理请求的顺序。

·dont_filter（boolean）：表明该请求不应由调度器过滤。适用场景为你想多次执行相同请求的时候。小心使用它，否则你会进入爬行循环。默认为False。

·errback（callable）：如果在处理请求的过程中出现异常，指定的方法将会被调用。

下面介绍一下cookies参数的设置，有两种方式：

·使用字典发送：



* * *



request_with_cookies = Request(url="http://www.example.com", cookies={'currency': 'USD', 'coun try': 'UY'})



* * *



·使用字典列表发送：



* * *



request_with_cookies = Request(url="http://www.example.com", cookies=[{'name': 'currency', 'value': 'USD', 'domain': 'example.com', 'path': '/currency'}])



* * *



后面的这种方式可以定制Cookie中的domain和path属性。如果想把cookie信息保存到之后的请求中，这种方式会很有用。

当一些网站返回cookie信息，它们会被保存到cookie域中，在之后的请求时发送出去，这是一个典型的浏览器行为。但是由于某些原因，如果不想和现有的cookie进行合并，可以设置Request.meta中dont_merge_cookies字段的值。示例如下：



* * *



request_with_cookies = Request(url="http://www.example.com", cookies={'currency': 'USD', 'coun try': 'UY'}, meta={'dont_merge_cookies': True})



* * *



Request对象还提供一些属性和方法，如表13-1所示。

表13-1　常见属性与方法



除了我们能给meta设置任意的元数据，Scrapy还为Request.meta定义了一些特殊的键值，下面介绍其中一些常用的键值，如表13-2所示。

表13-2　常用键值



下面着重介绍一下FormRequest类，这是Request的子类，专门用来处理HTML表单，尤其对隐藏表单的处理非常方便，适合用来完成登录操作。

类原型：



* * *



class scrapy.http.FormRequest(url[, formdata, ...])



* * *



其中构造参数formdata可以是字典，也可以是可迭代的（key，value）元组，代表着需要提交的表单数据。示例如下：



* * *



return FormRequest(url="http://www.example.com/post/action", formdata={'name': 'John Doe', 'age': '27'}, callback=self.after_login)



* * *



通常网站通过<inputtype=“hidden”>实现对某些表单字段（如数据或是登录界面中的认证令牌等）的预填充，例如之前讲过的知乎网站的登录情况，如何处理这种隐藏表单呢？FormRequest类提供了一个类方法from_response。

方法原型如下：



* * *



from_response(response[, formname=None, formnumber=0, formdata=None, formxpath=None, clickdata=None, dont_click=False, ...])



* * *



常用参数说明：

·response：一个包含HTML表单的响应页面。

·formname（string）：如果不为None，表单中的name属性将会被设定为这个值。

·formnumber（int）：当响应页面中包含多个HTML表单时，本参数用来指定使用第几个表单，第一个表单数字为0。

·formdata（dict）：本参数用来填充表单中属性的值。如果其中一个属性的值在响应页面中已经被预填充，但formdata中也指定了这个属性的值，将会把预填充的值覆盖掉。

·formxpath（string）：如果页面中有多个HTML表单，可以用xpath表达式定位页面中的表单，第一个被匹配的将会被操作。

下面使用from_response方法来实现登录功能，示例如下：



* * *



import scrapy class LoginSpider(scrapy.Spider): name = 'example.com' start_urls = ['http://www.example.com/users/login.php'] def parse(self, response): return scrapy.FormRequest.from_response( response, formdata={'username': 'john', 'password': 'secret'}, callback=self.after_login ) def after_login(self, response): # check login succeed before going on if "authentication failed" in response.body: self.logger.error("Login failed") return



* * *





13.4.2　Response对象


Response对象代表着HTTP响应，Response通常是从下载器获取然后交给Spider处理。

类原型：



* * *



scrapy.http.Response(url[, status=200, headers, body, flags])



* * *



构造参数说明：

·url（string）：响应的URL。

·headers（dict）：响应头信息。

·status（integer）：响应码，默认为200。

·body（str）：响应的body。

·meta（dict）：用来初始化Response.meta。

·flags（list）：用来初始化Response.flags。

下面说一下常用的属性和方法，如表13-3所示。

表13-3　常用属性和方法



Response在Response的基础上添加了智能编码的功能。

类原型：



* * *



scrapy.http.TextResponse(url[, encoding[, ...]])



* * *



构造参数encoding是一个包含编码的字符串。如果使用一个unicode编码的body构造出TextResponse实例，那body属性会使用encoding进行编码。

TextResponse类除了具有Response的属性，还拥有自己的属性：

·encoding：包含编码的字符串。为什么说TextResponse具有智能编码的功能呢？编码的优先级由高到低如下所示：

1）首先选用构造器中传入的encoding。

2）选用HTTP头中Content-Type字段的编码。如果编码无效，则被忽略，继续尝试下面的规则。

3）选用响应body中的编码。

4）最后猜测响应的编码，这种方式是比较脆弱的。

·selector：以当前响应为目标的选择器实例。

TextResponse类除了支持Response中的方法，还支持以下方法：

·body_as_unicode（）：返回unicode编码的响应body内容。等价于：



* * *



response.body.decode(response.encoding)



* * *



·xpath（query）：等价于TextResponse.selector.xpath（query）。示例如下：



* * *



response.xpath('// p')



* * *



·css（query）：等价于TextResponse.selector.css（query）。示例如下：



* * *



response.css('p')



* * *



TextResponse还有两个子类HtmlResponse和XmlResponse，用法大同小异，不再赘述。





13.5　下载器中间件


从Scrapy框架图12-1中可以看到，下载器中间件是介于Scrapy的request/response处理的钩子框架，是用于全局修改Scrapy的request和response，可以帮助我们定制自己的爬虫系统。





13.5.1　激活下载器中间件


要激活下载器中间件组件，需要将其加入到DOWNLOADER_MIDDLEWARES设置中。该设置位于Settings.py文件，是一个字典（dict），键为中间件类的路径，值为其中间件的顺序。示例如下：



* * *



DOWNLOADER_MIDDLEWARES = { 'myproject.middlewares.CustomDownloaderMiddleware': 543, }



* * *



在Settings.py中对DOWNLOADER_MIDDLEWARES的设置，会与Scrapy内置的下载器中间件设置DOWNLOADER_MIDDLEWARES_BASE合并，但不会覆盖，而是根据顺序值进行排序，最后得到启用中间件的有序列表：第一个中间件是最靠近引擎的，最后一个中间件是最靠近下载器的。Scrapy内置的中间件设置DOWNLOADER_MIDDLEWARES_BASE为：

·'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware'：100

·'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware'：300

·'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware'：350

·'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'：400

·'scrapy.downloadermiddlewares.retry.RetryMiddleware'：500

·'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware'：550

·'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware'：580

·'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'：590

·'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'：600

·'scrapy.downloadermiddlewares.cookies.CookiesMiddleware'：700

·'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'：750

·'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware'：830

·'scrapy.downloadermiddlewares.stats.DownloaderStats'：850

·'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware'：900

如何分配中间件的位置，首先看一下内置的中间件的位置，然后根据将你想放置的中间件的位置设置一个值。有时候你想放置的中间件可能会依赖前后的中间件的作用，因此设置顺序相当重要。如果想禁用内置的中间件，必须在DOWNLOADER_MIDDLEWARES中定义该中间件，并将值设置为None。例如想关闭User-Agent中间件：



* * *



DOWNLOADER_MIDDLEWARES = { 'myproject.middlewares.CustomDownloaderMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, }



* * *



注意 　DOWNLOADER_MIDDLEWARES_BASE中内置中间件并不是都开启的，有些中间件需要通过特定的设置来启用。





13.5.2　编写下载器中间件


如何定制我们自己的下载器中间件才是我们比较关心的问题，编写下载器中间件非常简单。每个中间件组件是定义了以下一个或多个方法的Python类：

·process_request（request，spider）

·process_response（request，response，spider）

·process_exception（request，exception，spider）

下面分别介绍这三种中间件。

1.process_request（request，spider）

方法说明： 当每个Request通过下载中间件时，该方法被调用，返回值必须为None、Response对象、Request对象中的一个或Raise IgnoreRequest异常。

参数：

Request（Request对象）：处理的Request。

Spider（Spider对象）：该Request对应的Spider。

返回值：

如果返回None，Scrapy将继续处理该Request，执行其他的中间件的相应方法，直到合适的下载器处理函数被调用，该Request被执行（其Response被下载）。

如果返回Response对象，Scrapy不会调用其他的process_request（），process_exception（）方法，或相应的下载方法，将返回该response。已安装的中间件的process_response（）方法则会在每个response返回时被调用。

如果返回Request对象，Scrapy则停止调用process_request方法并重新调度返回的Request。当新返回的Request被执行后，相应地中间件链将会根据下载的Response被调用。

如果是Raise IgnoreRequest异常，则安装的下载中间件的process_exception（）方法会被调用。如果没有任何一个方法处理该异常，则Request的errback方法会被调用。如果没有代码处理抛出的异常，则该异常被忽略且不记录。

2.process_response（request，response，spider）

方法说明： 该方法主要用来处理产生的Response，返回值必须是Response对象、Request对象中的一个或Raise IgnoreRequest异常。

参数：

request（Request对象）：Response对应的Request。

response（Response对象）：处理的Response。

spider（Spider对象）：Response对应的Spider。

返回值：

如果返回Response对象，可以与传入的Response相同，也可以是新的对象，该Response会被链中的其他中间件的process_response（）方法处理。

如果返回Request对象，则中间件链停止，返回的Request会被重新调度下载。处理类似于process_request（）返回Request时所做的那样。

如果抛出IgnoreRequest异常，则调用Request的errback方法。如果没有代码处理抛出的异常，则该异常被忽略且不记录。

3.process_exception（request，exception，spider）

方法说明： 当下载处理器或process_request（）抛出异常，比如IgnoreRequest异常时，Scrapy调用process_exception（）。process_exception（）应该返回None、Response对象或者Request对象其中之一。

参数：

request（Request对象）：产生异常的Request。

exception（Exception对象）：抛出的异常。

spider（Spider对象）：Request对应的Spider。

返回值：

如果返回None，Scrapy将会继续处理该异常，接着调用已安装的其他中间件的process_exception（）方法，直到所有中间件都被调用完毕，则调用默认的异常处理。

如果返回Response对象，则已安装的中间件链的process_response（）方法被调用。Scrapy将不会调用任何其他中间件的process_exception（）方法。

如果返回Request对象，则返回的request将会被重新调用下载，这将停止中间件的process_exception（）方法执行，类似于返回Response对象的处理。

下面通过两个例子帮助大家理解下载器中间件的编写，这两个例子也是在实际项目中经常用到。

一个是动态设置Request的User-Agent字段，主要是为了突破反爬虫对User-Agent字段的检测，实例代码如下：



* * *



''' 这个类主要用于产生随机User-Agent ''' class RandomUserAgent(object): def __init__(self,agents): self.agents = agents @classmethod def from_crawler(cls,crawler): # 从Settings中加载USER_AGENTS的值 return cls(crawler.settings.getlist('USER_AGENTS')) def process_request(self,request,spider): # 在process_request中设置User-Agent的值 request.headers.setdefault('User-Agent', random.choice(self.agents))



* * *



其中USER_AGENTS是写在Settings.py中的User-Agent列表，内容如下：



* * *



USER_AGENTS=[ "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)", "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Tri dent/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)", "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)", "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)", "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6", "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0", "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5", "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1. fc10 Kazehakase/0.5.6", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11", ]



* * *



一个是动态设置Request的代理IP，主要是为了突破反爬虫对IP的检测，实例代码如下：



* * *



''' 这个类主要用于产生随机代理 ''' class RandomProxy(object): def __init__(self,iplist):# 初始化一下数据库连接 self.iplist=iplist @classmethod def from_crawler(cls,crawler): # 从Settings中加载IPLIST的值 return cls(crawler.settings.getlist('IPLIST')) def process_request(self, request, spider): ''' 在请求上添加代理 :param request: :param spider: :return: ''' proxy = random.choice(self.iplist) request.meta['proxy'] =proxy



* * *



其中IPLIST是写在Settings.py中的代理IP列表，内容如下：



* * *



IPLIST=["http://220.160.22.115:80", "http://183.129.151.130:80","http://112.228. 35.24:8888"]



* * *



写完以上两个中间件，如果想使用的话，直接按照上一节讲解的那样，进行激活即可。





13.6　Spider中间件


从Scrapy框架图12-1中可以看到，Spider中间件是介入到Scrapy的Spider处理机制的钩子框架，可以用来处理发送给Spiders的Response及Spider产生的Item和Request。





13.6.1　激活Spider中间件


和下载器中间件一样，Spider也是需要激活才能使用的。要启用Spider中间件，需要将其加入到SPIDER_MIDDLEWARES设置中。该设置和DOWNLOADER_MIDDLEWARES一样，键是中间件的路径，值为中间件的顺序。示例如下：



* * *



SPIDER_MIDDLEWARES = { 'myproject.middlewares.CustomSpiderMiddleware': 543, }



* * *



在Settings.py中对SPIDER_MIDDLEWARES的设置，会与Scrapy内置的Spider中间件设置SPIDER_MIDDLEWARES_BASE合并，但是不会覆盖，而是根据顺序值进行排序，最后得到启用中间件的有序列表：第一个中间件是最靠近引擎的，最后一个中间件是最靠近Spider的。Scrapy内置的spider中间件设置SPIDER_MIDDLEWARES_BASE为：

·'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware'：50

·'scrapy.spidermiddlewares.offsite.OffsiteMiddleware'：500

·'scrapy.spidermiddlewares.referer.RefererMiddleware'：700

·'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware'：800

·'scrapy.spidermiddlewares.depth.DepthMiddleware'：900

如何分配中间件的位置，首先看一下内置的中间件的位置，然后根据将你想放置的中间件的位置设置一个值。有时候你想放置的中间件可能会依赖前后的中间件的作用，因此设置顺序相当重要。如果想禁用内置的中间件，必须在SPIDER_MIDDLEWARES中定义该中间件，并将值设置为None。例如想关闭（off-site）中间件：



* * *



SPIDER_MIDDLEWARES = { 'myproject.middlewares.CustomSpiderMiddleware': 543, 'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': None, }



* * *



注意 　SPIDER_MIDDLEWARES_BASE中内置中间件并不是都开启的，有些中间件需要通过特定的设置来启用。





13.6.2　编写Spider中间件


编写spider中间件十分简单，和下载器中间件非常类似，每个中间件组件是定义了以下一个或多个方法的Python类：

·process_spider_input（response，spider）

·process_spider_output（response，result，spider）

·process_spider_exception（response，exception，spider）

·process_start_requests（start_requests，spider）

下面分别介绍这四种中间件。

1.process_spider_input（response，spider）

方法说明： 当response通过spider中间件时，该方法被调用，处理该response。应该返回None或者抛出一个异常。

参数：

·response（Response对象）：被处理的response

·spider（Spider对象）：该response对应的spider

返回：

如果返回None，Scrapy将会继续处理该response，调用所有其他的中间件直到spider处理该response。

如果产生一个异常，Scrapy将不会调用任何其他中间件的process_spider_input（）方法，并调用request的errback。errback的输出将会以另一个方向被重新输入到中间件链中，使用process_spider_output（）方法来处理，当其抛出异常时，则调用process_spider_exception（）。

2.process_spider_output（response，result，spider）

方法说明： 当Spider处理Response返回Result时，该方法被调用。

参数：

·response（Response对象）：生成该输出的Response。

·result：Spider返回的Result，是包含Request、Dict或Item的可迭代对象。

·spider（Spider对象）：其结果被处理的Spider。

返回： process_spider_output（）必须返回包含Request、dict或Item对象的可迭代对象（iterable）。

3.process_spider_exception（response，exception，spider）

方法说明： 当Spider或（其他Spider中间件的）process_spider_input（）抛出异常时，该方法被调用。该方法必须返回None或者包含Response、Dict或Item对象的可迭代对象中。

参数：

·response（Response对象）：异常被抛出时被处理的Response。

·exception（Exception对象）：被抛出的异常。

·spider（Spider对象）：抛出该异常的Spider。

返回：

·如果其返回None，Scrapy将继续处理该异常，调用中间件链中的其他中间件的process_spider_exception（）方法，直到所有中间件都被调用，该异常到达引擎时被记录，最后被忽略。

·如果返回一个可迭代对象，则中间件链的process_spider_output（）方法被调用，其他的process_spider_exception（）将不会被调用。

4.process_start_requests（start_requests，spider）

方法说明： 该方法以spider启动的request为参数，执行的过程类似于process_spider_output（），其接受一个可迭代的对象（start_requests参数）且必须返回另一个包含Request对象的可迭代对象。

参数：

·start_requests（包含Request的可迭代对象）：起始Requests。

·spider（Spider对象）：起始Requests所属的Spider。

下面通过一个例子帮助大家理解Spider中间件的编写，中间件的功能是实现Request中的URL规范化，示例代码如下：



* * *



from scrapy.http import Request from scrapy.utils.url import canonicalize_url class UrlCanonicalizerMiddleware(object): def process_spider_output(self, response, result, spider): for r in result: if isinstance(r, Request): curl = canonicalize_url(r.url) if curl != r.url: r = r.replace(url=curl) yield r



* * *





13.7　扩展


扩展框架提供了一种机制，你可以将自定义功能绑定到Scrapy中。扩展只是正常的Python类，它们会在Scrapy启动时被实例化、初始化。





13.7.1　配置扩展


扩展需要在settings中进行设置，和中间件的设置类似。扩展在扩展类被实例化时加载和激活，实例化代码必须在类的构造函数（__init__）中执行。要使得扩展可用，需要把它添加到Settings的EXTENSIONS配置中。在EXTENSIONS中，每个扩展都使用一个字符串表示，即扩展类的全Python路径。例如：



* * *



EXTENSIONS = { 'scrapy.extensions.corestats.CoreStats': 500, 'scrapy.telnet.TelnetConsole': 500, }



* * *



EXTENSIONS配置的格式和中间件配置的格式差不多，都是一个字典，键是扩展类的路径，值是顺序，它定义扩展加载的顺序。扩展顺序不像中间件的顺序那么重要，扩展之间一般没有关联。Scrapy中的内置扩展设置EXTENSIONS_BASE如下：

·'scrapy.extensions.corestats.CoreStats'：0

·'scrapy.telnet.TelnetConsole'：0

·'scrapy.extensions.memusage.MemoryUsage'：0

·'scrapy.extensions.memdebug.MemoryDebugger'：0

·'scrapy.extensions.closespider.CloseSpider'：0

·'scrapy.extensions.feedexport.FeedExporter'：0

·'scrapy.extensions.logstats.LogStats'：0

·'scrapy.extensions.spiderstate.SpiderState'：0

·'scrapy.extensions.throttle.AutoThrottle'：0

扩展一般分为三种状态：可用的（Available）、开启的（enabled）和禁用的（disabled）。并不是所有可用的扩展都会被开启。一些扩展经常依赖一些特别的配置，比如HTTP Cache扩展是可用的但默认是禁用的，除非设置了HTTPCACHE_ENABLED配置项。如何禁用一个默认开启的扩展呢？和中间件的禁用一样，需要将其顺序（order）设置为None。比如：



* * *



EXTENSIONS = { 'scrapy.extensions.corestats.CoreStats': None, }



* * *





13.7.2　定制扩展


如何定制我们自己的扩展，强化Scrapy的功能才是我们比较关心的问题。扩展类是一个不同的Python类，但是如果想操作Scrapy的功能，需要一个入口：from_crawler类方法，它接收一个Crawler类的实例，通过这个对象可以访问settings（设置）、signals（信号）、stats（状态），以便控制爬虫的行为。通常来说，扩展需要关联到signals并执行它们触发的任务，如果from_crawler方法抛出NotConfigured异常，扩展会被禁用。否则，扩展会被开启。下面通过一个例子来实现简单扩展，功能是当出现以下事件时，记录一条日志：

·Spider被打开。

·Spider被关闭。

·爬取了特定数量的Item。

扩展代码如下：



* * *



import logging from scrapy import signals from scrapy.exceptions import NotConfigured logger = logging.getLogger(__name__) class SpiderOpenCloseLogging(object): def __init__(self, item_count): self.item_count = item_count self.items_scraped = 0 @classmethod def from_crawler(cls, crawler): # 首先检查一下是否存在相应的配置，如果不存在则抛出NotConfigured异常 if not crawler.settings.getbool('MYEXT_ENABLED'): raise NotConfigured # 从setting中获取MYEXT_ITEMCOUNT的值 item_count = crawler.settings.getint('MYEXT_ITEMCOUNT', 1000) # 初始化扩展实例 ext = cls(item_count) # 将扩展中的spider_opened、spider_closed和item_scraped连接到相应信号处，进行触发。 crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened) crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed) crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped) # 扩展实例返回 return ext def spider_opened(self, spider): logger.info("opened spider %s", spider.name) def spider_closed(self, spider): logger.info("closed spider %s", spider.name) def item_scraped(self, item, spider): self.items_scraped += 1 if self.items_scraped % self.item_count == 0: logger.info("scraped %d items", self.items_scraped)



* * *



编写扩展依赖的Crawler实例，其中信号的设置很重要。下面说一下内置的信号。

1.engine_started

原型：scrapy.signals.engine_started（）

说明：当Scrapy引擎启动爬取时发送该信号。该信号支持返回deferreds。

2.engine_stopped

原型：scrapy.signals.engine_stopped（）

说明：当Scrapy引擎停止时发送该信号，例如爬取结束。该信号支持返回deferreds。

3.item_scraped

原型：scrapy.signals.item_scraped（item，response，spider）

参数：item（dict或Item对象）：爬取到的item

　spider（Spider对象）：爬取item的spider

　response（Response对象）：提取item的response

说明：当item被爬取，并通过所有Item Pipeline后（没有被丢弃（dropped），发送该信号。该信号支持返回deferreds。

4.item_dropped

原型：scrapy.signals.item_dropped（item，exception，spider）

参数：item（dict或Item对象）：Item Pipeline丢弃的item。

　spider（Spider对象）：爬取item的spider。

　exception（DropItem异常）：导致item被丢弃的异常。

说明：当item通过Item Pipeline，有些pipeline抛出DropItem异常，丢弃Item时，该信号被发送。该信号支持返回deferreds。

5.spider_closed

原型：scrapy.signals.spider_closed（spider，reason）

参数：spider（Spider对象）：关闭的spider。

reason（str）：描述Spider被关闭的原因的字符串。如果Spider是由于完成爬取而被关闭，则其为“finished”。否则，如果Spider是被引擎的close_spider方法所关闭，则其为调用该方法时传入的reason参数（默认为“cancelled”）。如果引擎被关闭（例如，输入Ctrl-C），则其为“shutdown”。

说明：当某个Spider被关闭时，该信号被发送。该信号可以用来释放每个Spider在spider_opened时占用的资源。该信号支持返回deferreds。

6.spider_opened

原型：scrapy.signals.spider_opened（spider）

参数：spider（Spider对象）：开启的spider。

说明：当spider开始爬取时发送该信号。该信号一般用来分配Spider的资源，不过它也能做任何事。该信号支持返回deferreds。

7.spider_idle

原型：scrapy.signals.spider_idle（spider）

参数：spider（Spider对象）：空闲的Spider。

说明：当Spider进入空闲（idle）状态时该信号被发送。空闲意味着：

·Requests正在等待被下载。

·Requests被调度。

·Items正在Item Pipeline中被处理。

当该信号的所有处理器（handler）被调用后，如果Spider仍然保持空闲状态，引擎将会关闭该Spider。当Spider被关闭后，spider_closed信号将被发送，可以在spider_idle处理器中调度某些请求来避免spider被关闭。

该信号不支持返回deferreds。

8.spider_error

原型：scrapy.signals.spider_error（failure，response，spider）

参数：failure（Failure对象）：以Twisted Failure对象抛出的异常。

　response（Response对象）：当异常被抛出时被处理的response。

　spider（Spider对象）：抛出异常的Spider。

说明：当Spider的回调函数产生错误时，例如抛出异常，该信号被发送。

9.request_scheduled

原型：scrapy.signals.request_scheduled（request，spider）

参数：request（Request对象）：到达调度器的Request。

　spider（Spider对象）：产生该Request的Spider。

说明：当引擎调度一个Request对象用于下载时，该信号被发送。该信号不支持返回deferreds。

10.response_received

原型：scrapy.signals.response_received（response，request，spider）

参数：response（Response对象）：接收到的response。

　request（Request对象）：生成response的request。

　spider（Spider对象）：response所对应的spider。

说明：当引擎从downloader获取到一个新的Response时发送该信号。该信号不支持返回deferreds。

11.response_downloaded

原型：scrapy.signals.response_downloaded（response，request，spider）

参数：response（Response对象）：下载的response。

　request（Request对象）：生成response的request。

　spider（Spider对象）：response所对应的spider。

说明：当一个HTTPResponse被下载时，由downloader发送该信号。该信号不支持返回deferreds。





13.7.3　内置扩展


下面简要介绍一下Scrapy的内置扩展，方便我们使用，同时也可以参考内置扩展的源码来拓展自己的功能。常见内置扩展如表13-4所示。

表13-4　常见内置扩展





13.8　突破反爬虫


既然要突破反爬虫机制，那我们就需要知道有哪些反爬虫措施。大部分网站的反爬虫措施可以分为以下四类：

·基于验证码的反爬虫。 现在大部分的网站都会有验证码，有传统验证码、逻辑验证码，还有滑动验证码等，例如google会在访问的时候有时候要求输入验证码，能加大爬虫大规模爬取的难度，但是依然有办法突破，解决办法可以参考第10章的验证码问题。

·基于Headers的反爬虫。 从请求头Headers进行反爬虫是比较常见的措施，大部分网站会对Headers中的User-Agent和Referer字段进行检测。突破办法是可以根据浏览器正常访问的请求头对爬虫的请求头进行修改，尽可能和浏览器保持一致。

·基于用户行为的反爬虫。 还有一些网站通过用户的行为进行反爬虫，例如同一IP短时间内多次访问同一页面，同一账户短时间内多次进行相同操作或者访问页面的间隔比较固定，通俗来说就是表现得不像人在访问。大部分都是第一种情况，可以使用大量的IP代理进行绕过。第二种情况可以注册较多的账户登录，构成一个Cookie池，对用户状态进行自动切换。第三种情况可以将访问间隔设置成随机的，尽可能模拟人的访问频率。

·基于动态页面的反爬虫。 现在越来越多的网站采用动态加载技术，无法直接从页面上获取数据，需要分析Ajax请求，然后进行模拟发送获取数据。如果能够直接模拟Ajax请求，这当然是最好的结果，但是有些网站把Ajax请求的所有参数全部加密了，无法构造自己所需要的数据的请求，这就大大增加了爬取的难度。如果能忍受较低的效率和较大的内存消耗，我们可以使用selenium+phantomJS进行突破。

综上所述，突破反爬虫的秘诀是将爬虫模拟得像人在操作浏览器进行访问，而且在爬虫和反爬虫的斗争中，最终必然是爬虫胜利，只不过是付出的代价有多大而已。但是我建议做爬虫的时候，尽可能地减小访问频率，不要给他人的网站服务器造成过大的负担，毕竟我们只是想提取数据而已。下面我们讲解一下Scrapy中推荐的突破反爬虫措施，这些措施不仅能用于Scrapy，也可以用于普通爬虫程序。





13.8.1　UserAgent池


HTTP请求头中User-Agent包含了我们使用的浏览器和操作系统的一些信息，很多网站通过判断User-Agent内容来确定用户，所以要动态设置User-Agent，来伪装存在很多用户。这就需要使用13.5.2小节的下载器中间件RandomUserAgent，设置动态的User-Agent，使用时在settings中将内置的UserAgentMiddleware禁用，并激活RandomUserAgent即可。





13.8.2　禁用Cookies


假如你爬取的网站不需要登录就可以进行爬取，可以尝试将Cookie禁用。因为Cookie会跟踪爬虫的访问过程，容易被发现。Scrapy通过设置settings中的COOKIES_ENABLED=False，即可实现对cookie的禁用。





13.8.3　设置下载延时与自动限速


对一个网站访问过于频繁就会被反爬虫措施所识别和拦截，一个比较好的做法是设置下载延时，在settings中设置DOWNLOAD_DELAY。比如设置延时2秒：



* * *



DOWNLOAD_DELAY=2



* * *



但是存在一个问题，DOWNLOAD_DELAY设置完成之后，不能动态改变，导致访问延时都差不多，也容易被发现。不过可以设置RANDOMIZE_DOWNLOAD_DELAY字段，进行动态调整。



* * *



RANDOMIZE_DOWNLOAD_DELAY=True



* * *



如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值，延迟时间为0.5到1.5之间的一个随机值乘以DOWNLOAD_DELAY。这会大大降低被发现的几率，但是有一些网站会检测访问延迟的相似性，也有发现的可能性。Scrapy提供了一种更智能的方法来解决限速的问题：通过自动限速扩展，该扩展能根据Scrapy服务器及爬取的网站的负载自动限制爬取速度。

Scrapy是如何实现自动限速扩展的呢？在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头（header）之间的时间来测量的，该扩展就是以此为前提进行编写的。使用的限速算法根据以下规则调整下载延迟及并发数：

·spider永远以1并发请求数及AUTOTHROTTLE_START_DELAY中指定的下载延迟启动。

·当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。

·如何配置自动限速扩展呢？通过配置settings中的以下字段：

·AUTOTHROTTLE_ENABLED：默认为False，设置为True可以启用该扩展。

·AUTOTHROTTLE_START_DELAY：初始下载延时，单位为秒，默认为5.0。

·AUTOTHROTTLE_MAX_DELAY：设置在高延迟情况下最大的下载延迟，单位秒，默认为60。

·AUTOTHROTTLE_DEBUG：用于启动Debug模式，默认为False.

·CONCURRENT_REQUESTS_PER_DOMAIN：对单个网站进行并发请求的最大值。默认为8。

·CONCURRENT_REQUESTS_PER_IP：对单个IP进行并发请求的最大值。如果非0，则忽略CONCURRENT_REQUESTS_PER_DOMAIN设定，使用该设定。也就是说，并发限制将针对IP，而不是网站。





13.8.4　代理IP池


突破对IP访问次数的限制，可以使用大量的代理IP，然后选取合适的IP进行请求的访问。可以使用13.5.2节中讲到的RandomProxy中间件，对请求设置代理。如果是个人用户，推荐使用我的IPProxys项目。





13.8.5　Tor代理


首先介绍一下Tor是什么？Tor即“洋葱路由器”，创造这项服务的目的是让人们匿名浏览互联网。它是一个分散式系统，允许用户通过中继网络连接，而无需建立直接连接。这种方法的好处是，可以对访问的网站隐藏IP地址，因为连接是在不同服务器之间随机变换的，无法追踪您的踪迹，其实也相当于代理IP池的作用，但是也有缺点，比如访问速度较慢。下面以Windows和Ubuntu上的配置为例。

首先使用VPN连接到国外的服务器上或者配置Tor代理，因为国内并不容易连接上Tor，如图13-3所示。

接着从官网上（https://www.torproject.org/download/download.html ）下载Tor，如图13-4所示，下载Expert Bundle。



图13-3　连接VPN



图13-4　Tor下载

安装完成后，这个版本并没有一个图形化的操作界面，要修改配置十分麻烦，可以通过下载Vidalia来使用TOR，Vidalia的下载地址：

https://people.torproject.org/~erinn/vidalia-standalone-bundles/



下载其中的vidalia-standalone-0.2.21-win32-1_zh-CN.exe即可，安装完成之后，以管理员权限运行Start Vidalia.exe，进行下面的设定，选择Tor路径，如图13-5所示。



图13-5　配置Tor

设置完成后，点击启动Tor就可以了，显示如图13-6所示，即为启动成功。



图13-6　启动Tor

由于Scrapy暂时只支持HTTP代理，但是Tor使用的sock5代理方式，绑定了9050端口，所以需要一个HTTP转sock的代理软件，例如polipo或者privoxy。我选择的是polipo，下载地址为：https://www.irif.fr/~jch/software/files/polipo/ 。选择polipo-1.1.0-win32.zip，下载并解压，然后编辑解压后的文件config.sample，加入以下配置：



* * *



socksParentProxy = "localhost:9050" socksProxyType = socks5 diskCacheRoot = ""



* * *



在命令行下运行该目录下的程序：polipo.exe-c config.sample，polipo默认监听8123端口，如图13-7所示。



图13-7　启动polipo

当以上工作都完成后，我们只需要在RandomProxy中间件中将process_request方法代码修改为如下所示：



* * *



def process_request(self, request, spider): request.meta['proxy'] ='http://127.0.0.1:8123'



* * *



Ubuntu下的配置也是类似的，一共分为5个步骤：

1）连接上VPN。

2）使用以下四条命令添加软件源：



* * *



sudo add-apt-repository "deb http://deb.torproject.org/torproject.org/ precise main" gpg --keyserver keys.gnupg.net --recv 886DDD89 gpg --export A3C4F0F979CAA22CDBA8F512EE8CBC9E886DDD89 | sudo apt-key add - sudo apt-get update



* * *



3）安装Tor、polipo、vidalia：sudo apt-get install tor polipo vidalia。

4）配置polipo：



* * *



socksParentProxy = "localhost:9050" socksProxyType = socks5 diskCacheRoot = ""



* * *



配置完成后，重启sudo/etc/init.d/polipo restart。

5）启动vidalia，在界面中启动Tor即可。





13.8.6　分布式下载器：Crawlera


Scrapy官方提供了一个分布式下载器Crawlera，用来帮助我们躲避反爬虫的封锁。首先去官网https://app.scrapinghub.com/account/login/next=/account/login ，注册一个账号，如图13-8所示。

注册完成后，会分配给用户API Key，用作访问验证使用，如图13-9所示。

不过这个下载器是收费的，需要充值才能正常工作。下面介绍一下如何在Scrapy中使用这个下载器，只需要两步即可。

1）安装scrapy-crawlera：



* * *



pip install scrapy-crawlera



* * *



2）修改settings.py：



* * *



DOWNLOADER_MIDDLEWARES = {'scrapy_crawlera.CrawleraMiddleware': 300} CRAWLERA_ENABLED = True CRAWLERA_APIKEY = '<API key>'



* * *



这个时候就可以启用爬虫了，具体的配置和管理操作，大家请看官方的使用文档：https://doc.scrapinghub.com/crawlera.html 。



图13-8　注册scrapinghub



图13-9　API Key





13.8.7　Google cache


Google cache指的是Google的网页快照功能。Google是一个强大的搜索引擎，可以将爬取到的网页缓存到服务器中，因此我们可以不用直接访问目的站点，访问缓存也是可以达到提取数据的目的。访问Google cache的方式为：http://webcache.googleusercontent.com/searchq=cache: 要查询的网址，例如我想查看博客的快照，可以在浏览器中输入以下网址：

http://webcache.googleusercontent.com/search?q=cache :http://www.cnblogs.com/qiyeboy/

搜索结果如图13-10所示。

大家只需要写个中间件将Request中的URL替换成Google cache下的URL即可。



图13-10　Google cache





13.9　小结


本章总结了Scrapy的常用和较为重要的知识点，通过本章的学习，大家基本上可以理解Scrapy框架，在此基础上可以进行简单的扩展功能，进行二次开发。如果大家想深度优化扩展Scrapy框架，还需要深入学习一下Twisted框架。





第14章　实战项目：Scrapy爬虫


前两章已经讲解完Scrapy框架，为了帮助大家夯实学过的知识，本章将开始进行实战项目，本次项目的主题是爬取知乎网站上用户的信息以及人际关系等，下面会对整个分析过程和关键代码进行讲解，完整的项目代码在GitHub（https://github.com/qiyeboy/ ）上。





14.1　创建知乎爬虫


在开始编程之前，我们首先需要根据项目需求对知乎网站进行分析。首先确定一下爬取用户的信息以及人际关系的位置，如图14-1所示。



图14-1　用户的信息和人际关系

如上图所示，用户信息主要是提取其中的用户昵称、住址、所在领域、公司、职位和教育经历、用户关注人数和被关注人数。人际关系需要提取用户所关注的人的id和被关注的人的id，并和用户自身id绑定，点击“关注了”链接，可以看到关注的用户信息，如图14-2所示。



图14-2　用户的信息和人际关系

接着发现用户信息和提取人际关系，需要用户登录之后，才能进行操作，所以首先需要模拟登录操作，如图14-3所示。

在查看人际关系的过程中，发现图14-2中关注人信息是动态加载的，每次加载20条，如图14-4所示。也就是说我们需要模拟动态加载的过程，可以抓包分析请求过程。





图14-3　知乎登录



图14-4　动态加载

通过上面的分析，将整个抓取项目的流程进行分析，如图14-5所示。

首先完成登录操作，获取cookie信息，接着从起始url中解析出用户信息，然后进入关注者界面和被关注者界面，提取关系用户ID和新的用户链接，将用户信息和关系用户ID存储到MongoDB中，将新的用户链接交给用户信息解析模块，依次类推，完成循环抓取任务。

以上将整个知乎爬虫项目的流程分析完成，编程可以正式开始了。首先在命令行中切换到用于存储项目的路径，然后输入以下命令创建知乎爬虫项目和爬虫模块：



图14-5　项目流程



* * *



scrapy startproject zhihuCrawl cd zhihuCrawl scrapy genspider -t crawl zhihu.com zhihu.com



* * *





14.2　定义Item


创建完工程后，首先要做的不是编写Spider，而是定义Item，确定我们需要提取的结构化数据。主要定义两个Item，一个负责装载用户信息，一个负责装载用户关系。代码如下：



* * *



class UserInfoItem(scrapy.Item): # define the fields for your item here like: # id user_id = scrapy.Field() # 头像img user_image_url = scrapy.Field() # 姓名 name = scrapy.Field() # 居住地 location = scrapy.Field() # 技术领域 business = scrapy.Field() # 性别 gender = scrapy.Field() # 公司 employment = scrapy.Field() # 职位 position = scrapy.Field() # 教育经历 education = scrapy.Field() # 我关注的人数 followees_num = scrapy.Field() # 关注我的人数 followers_num = scrapy.Field() class RelationItem(scrapy.Item): # 用户id user_id =scrapy.Field() # relation 类型 relation_type =scrapy.Field() # 和我有关系的人的id列表 relations_id = scrapy.Field()



* * *





14.3　创建爬虫模块


14.1节通过genspider命令已经创建了一个基于CrawlSpider类的爬虫模板，类名称为ZhihuComSpider，当然现在什么功能都没有。在爬虫模块中，我们需要完成登录、解析当前用户信息、动态加载和解析关系用户ID的功能。





14.3.1　登录知乎


首先完成登录操作，要在进行爬取之前完成，因此需要重写start_requests方法。首先通过Firebug抓取登录post包，数据包如下：



* * *



POST /login/phone_num HTTP/1.1 Host: www.zhihu.com User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/20100101 Fire fox/50.0 Accept: */* Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3 Accept-Encoding: gzip, deflate, br Content-Type: application/x-www-form-urlencoded; charset=UTF-8 X-Requested-With: XMLHttpRequest Referer: https:// www.zhihu.com/ Content-Length: 226 Cookie:q_c1=e0dd3bc1e3fb43e192d66da7772d2255|1480422237000|1480422237000; _xsrf= 1451c04ef9f408b69a94196b71c64b07; l_cap_id="ZTA5NzQ3MGE0MzY1NGIxY2IzYWU2OGFm YzQwNmI0OWY=|1480422237|35a69271df72e4b588c34ecb427af38e9ad1ffa3"; cap_id="Yz Q2N2YwYzcyNGQ1NDYyYjgwMmE1MjU0OGExZjJmNjU=|1480422237|7bc08fee3c211de4195635e58ed3b1e0ca5e098d";n_c=1;_zap=9c0e1ceb-6476-413e-837c-67b6c1043993; __utma=51854390.1255459250.1480422230.1480422230.1480422230.1; __utmb=51854390.2.10.1480422230;__utmc=51854390; __utmz=51854390.1480422230.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); __utmv=51854390.000--|3=entry_date=20161129=1;__utmt=1; d_c0="AGAC_lUW7AqPTjlRMkFmtScK_2Dnm6X_imE=|1480422238"; r_cap_id="OGE3ZjllOTcwYWE3NDI2NzkxNjBlODc0M2I2MDNlOTI=|1480422239|e47878a0b995e909ceb 878c9ef385a5eb833913f";l_n_c=1; login="ZTY1OTEyNDI0YTNjNDc3N2JmMzYzZjU1MmFiM2I5N2 I=|1480422257|649e6261252f60f825c82e73ad032f1fbab0d331" Connection: keep-alive _xsrf=1451c04ef9f408b69a94196b71c64b07 captcha_type=cn password=xxxxxxxxxxxx phone_num=xxxxxxxxxxx



* * *



登录成功后，返回json数据，内容为登录成功，格式如下：



* * *



{"r":0,"msg": "\u767b\u5f55\u6210\u529f"}



* * *



通过第10章的讲解，我们知道_xsrf属于隐藏表单，需要从页面进行提取，提取出来之后，构造Request请求，进行发送。总共需要3个方法，start_requests用于进入登录页面，start_login用于构造登录请求，after_login用于判断登录状态，如果登录成功则开始爬取起始url。在ZhihuComSpider中代码如下：



* * *



def start_requests(self): # 首先进入登录界面 return [Request('https:// www.zhihu.com/# signin', callback=self.start_login, meta={'cookiejar':1}) ] def start_login(self,response): # 开始登录 self.xsrf = Selector(response).xpath( '// input[@name="_xsrf"]/@value' ).extract_first() return [FormRequest( 'https:// www.zhihu.com/login/phone_num', method='POST', meta={'cookiejar': response.meta['cookiejar']}, formdata={ '_xsrf': self.xsrf, 'phone_num': 'xxxxxxx', 'password': 'xxxxxx', 'captcha_type': 'cn'}, callback=self.after_login )] def after_login(self,response): if json.loads(response.body)['msg'].encode('utf8') == "登录成功": self.logger.info(str(response.meta['cookiejar'])) return [Request( self.start_urls[0], meta={'cookiejar':response.meta['cookiejar']}, callback=self.parse_user_info, errback=self.parse_err, )] else: self.logger.error('登录失败') return



* * *



代码中的phone_num和password字段都用“xxxxxx”代替，大家可以填写自己的账号和密码。

因为要使用到Cookie，需要在Settings中将COOKIES_ENABLED设置为True。同时还需要伪装一下默认请求的User-Agent字段，因为默认的User-Agent字段包含Scrapy关键字，容易被发现，所以在Setting中将USER_AGENT设置为：Mozilla/5.0（Windows NT 6.1；WOW64；rv：50.0）Gecko/20100101Firefox/50.0。





14.3.2　解析功能


登录成功后，我们开始解析数据。首先解析用户信息数据，如图14-1所示。和之前的开发手段一样，通过Firebug分析页面，可以确定用户信息的XPath表达式如下：

·用户头像链接user_image_url：//img[@class='Avatar Avatar--l']/@src

·用户昵称name：//*[@class='title-section']/span/text（）

·居住地location：//*[@class='location item']/@title

·技术领域business：//*[@class='business item']/@title

·性别gender：//*[@class='item gender']/i/@class

·公司employment：//*[@class='employment item']/@title

·职位position：//*[@class='position item']/@title

·关注者和被关注者followees_num，followers_num第一种情况：//div[@class='zm-profile-side-following zg-clear']/a[@class='item']/strong/text（）

·关注者和被关注者followees_num，followers_num第二种情况：//div[@class='Profile-followStatusValue']/text（）

·关注者和被关注者页面跳转链接relations_url第一种情况：//*[@class='zm-profile-side-following zg-clear']/a/@href

·关注者和被关注者页面跳转链接relations_url第二种情况：//a[@class='Profile-followStatus']/@href

解析用户信息的功能在parse_user_info方法中实现，部分代码如下：



* * *



def parse_user_info(self,response): ''' 解析用户信息 :param response: :return: ''' user_id = os.path.split(response.url)[-1] user_image_url = response.xpath("// img[@class='Avatar Avatar--l']/@src").extract_first() name = response.xpath("// *[@class='title-section']/span/text()").extract_first() location = response.xpath("// *[@class='location item']/@title").extract_first() business = response.xpath("// *[@class='business item']/@title").extract_first() gender = response.xpath("// *[@class='item gender']/i/@class").extract_first() if gender and u"female" in gender: gender = u"female" else: gender = u"male" employment = response.xpath("// *[@class='employment item']/@title").extract_first() position = response.xpath("// *[@class='position item']/@title").extract_first() education = response.xpath("// *[@class='education item']/@title").extract_first() try: followees_num,followers_num = tuple(response.xpath("// div[@class='zm- profile-side-following zg-clear']/a[@class='item']/strong/text()").extract()) relations_url = response.xpath("// *[@class='zm-profile-side-following zg-clear']/a/@href").extract() except Exception,e: followees_num,followers_num =tuple(response.xpath ("// div[@class='Profile-followStatusValue']/text()").extract()) relations_url =response.xpath("// a[@class='Profile-followStatus']/@href"). extract() user_info_item = UserInfoItem(user_id=user_id,user_image_url=user_image_url, name=name,location=location,business=business, gender=gender,employment=employment,position=position, education=education,followees_num=int(followees_num), followers_num=int(followers_num)) yield user_info_item



* * *



其中有一点需要说明，user_id可以从响应链接中获取，链接类似于以下这种情况：

https://www.zhihu.com/people/qi-ye-59-20



对于性别的判断，可以判断提取出来的gender字段是否包含female。最后将所有的用户信息提取出来，构造成UserInfoItem返回即可。

接下来我们需要进入关注者界面和被关注者界面，上面的代码已经将relations_url提取出来，需要根据relations_url构造Request，开始进入分析用户关系的阶段。代码如下：



* * *



def parse_user_info(self,response): ...... 省略以上代码..... yield user_info_item for url in relations_url: if u"followees" in url: relation_type = u"followees" else: relation_type = u"followers" yield Request(response.urljoin(url=url), meta={ 'user_id':user_id, 'relation_type':relation_type, 'cookiejar': response.meta['cookiejar'], 'dont_merge_cookies': True }, errback=self.parse_err, callback=self.parse_relation )



* * *



代码中通过url是否包含followees来判断关系类型，然后将关系类型和用户ID添加到Request.meta字段中进行绑定。由于关注者界面和被关注者界面的页面结构一样，所以统一通过parse_relation方法进行解析，但是因为知乎通过动态加载的方式获取用户关注者，因此需要两个方法来负责用户关系的提取，一个负责进入关注者界面已经存在的20个以内的数据，另一个负责发送请求，动态获取数据。parse_relation属于前者。要提取关注者的ID，我们只需要从图14-2中提取关注者的链接即可，链接中包含关注者ID，XPath表达式如下：



* * *



// *[@class='zh-general-list clearfix']/div/a/@href



* * *



提取当前静态页面所有关注者id，代码如下：



* * *



def parse_relation(self,response): ''' 解析和我有关系的用户,只能处理前20条 :param response: :return: ''' user_id = response.meta['user_id'] relation_type = response.meta['relation_type'] relations_url = response.xpath("// *[@class='zh-general-list clearfix']/div/ a/@href").extract() relations_id = [os.path.split(url)[-1] for url in relations_url] yield RelationItem(user_id=user_id, relation_type=relation_type, relations_id=relations_id)



* * *



将user_id、relation_type、relations_id打包成RelationItem，存储并返回即可。

前20条数据提取完成后，下面我们需要构造请求，动态加载剩余的数据内容。首先看一下请求的方式和内容，请求头如下：



* * *



POST /node/ProfileFollowersListV2 HTTP/1.1 Host: www.zhihu.com User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/20100101 Fire fox/50.0 Accept: */* Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3 Accept-Encoding: gzip, deflate, br X-Xsrftoken: 1451c04ef9f408b69a94196b71c64b07 Content-Type: application/x-www-form-urlencoded; charset=UTF-8 X-Requested-With: XMLHttpRequest Referer: https://www.zhihu.com/people/tombkeeper/followers Content-Length: 132 Cookie: q_c1=e0dd3bc1e3fb43e192d66da7772d2255|1480422237000|1480422237000; _xsrf=1451c04ef9f408b69a94196b71c64b07; l_cap_id="ZTA5NzQ3MGE0MzY1NGIxY2IzYWU2OGFmYzQwNmI0OWY=|1480422237|35a69271df72e4b588c34ecb427af38e9ad1ffa3"; cap_id="YzQ2N2YwYzcyNGQ1NDYyYjgwMmE1MjU0OGExZjJmNjU=|1480422237|7bc08fee3c211de4195635e58ed3b1e0ca5e098d"; _zap=9c0e1ceb-6476-413e-837c-67b6c1043993; __utma=51854390.1255459250.1480422230.1480422230.1480422230.1; __utmb=51854390. 16.10.1480422230; __utmc=51854390; __utmz=51854390.1480422230.1.1.utmcsr=(direct)| utmccn=(direct)|utmcmd=(none); __utmv=51854390.100-1|2=registration_date=20160504=1^3=entry_date=20160504=1; d_c0="AGAC_lUW7AqPTjlRMkFmtScK_2Dnm6X_imE=|1480422238"; r_cap_id="OGE3ZjllOTcwYWE3NDI2NzkxNjBlODc0M2I2MDNlOTI=|1480422239|e47878a0b995e909ceb878c9ef385a5eb833913f"; l_n_c=1; login="ZTY1OTEyNDI0YTNjNDc3N2JmMzYzZjU1MmFiM2I5N2I=|1480422257|649e6261252f60f8 25c82e73ad032f1fbab0d331"; a_t="2.0ADBAt9gp3wkXAAAAogVlWAAwQLfYKd8JAGAC_lUW7AoXAAAA YQJVTX8AZVgASZjhTCU3MGhlW3IVuMCb9Hsv-G4zdCSTu9oWQreLGA2bXCQPpxQM-A=="; z_c0=Mi4wQUR CQXQ5Z3Azd2tBWUFMLVZSYnNDaGNBQUFCaEFsVk5md0JsV0FCSm1PRk1KVGN3YUdWYmNoVzR3SnYwZXlfNGJn|1480423586|8fa5513790936d0788752bbedce6dc1c0b1058dd; __utmt=1 Connection: keep-alive



* * *



POST请求的数据内容如下：



* * *



method:next params:{"offset":7,"order_by":"created","hash_id":"40be3c5c5aa1d4b4be674d2f6bebebca"}



* * *



对我们来说比较关键的是请求头中的X-Xsrftoken参数，和POST请求的数据内容从何而来。

首先X-Xsrftoken数据内容是我们的登录时的xsrf参数。POST请求的数据内容可以从网页中提取，如图14-6所示。



图14-6　参数提取

所在的标记为<divclass=“zh-general-list clearfix”data-init=“{”params“：{”offset“：0，”order_by“：“created”，“hash_id”：“40be3c5c5aa1d4b4be674d2f6bebebca”}，“nodename”：“ProfileFolloweesListV2”}“>，XPath表达式为//*[@class='zh-general-list clearfix']/@data-init。

经过以上分析可知，通过动态改变参数中offset的值模拟请求，就可以不断获取加载内容，parse_relation方法动态模拟的代码如下：



* * *



# 提出POST所需的参数和和我有关系的人数 users_num = response.xpath("// *[@class='zm-profile-section-name']/text()").extract_ first() users_num = int(re.search(r'\d+', users_num).group())if users_num else len(rela tions_url) # 提取要POST出去的参数 # data-init="{"params": {"offset": 0, "order_by": "created", "hash_id": # "fbbe3c439118fddec554b03734f9da99"}, "nodename": "ProfileFollowersListV2"}" data_init = response.xpath("// *[@class='zh-general-list clearfix']/@data-init"). extract_first() try: nodename =json.loads(data_init)['nodename'] params = json.loads(data_init)['params'] post_url = 'https:// www.zhihu.com/node/%s'% nodename # 下面获取剩余的数据POST if users_num > 20: params['offset'] = 20 payload = { 'method':'next', 'params':params } post_header={ 'Host': 'www.zhihu.com', 'Connection': 'keep-alive', 'Accept': '*/*', 'X-Requested-With': 'XMLHttpRequest', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/ 20100101Firefox/50.0', 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3', 'X-Xsrftoken':self.xsrf } yield Request(url=post_url,method='POST', headers=post_header, body=urlencode(payload), cookies=self.cookies, meta={'user_id':user_id, 'relation_type':relation_type, 'offset':20, 'payload':payload, 'users_num':users_num, 'cookiejar': response.meta['cookiejar'] }, callback=self.parse_next_relation, errback=self.parse_err, priority=100 ) except Exception,e: self.logger.warning('no second post--'+str(data_init)+'--'+str(e)) for url in relations_url: yield Request(response.urljoin(url=url), meta={'cookiejar': response.meta['cookiejar']}, callback=self.parse_user_info, errback=self.parse_err)



* * *



代码中判断关注者是否大于20，如果大于则模拟POST请求，获取动态数据，动态数据的解析放到了parse_next_relation方法中。

下面看一下POST请求发送后，获取的动态响应格式，然后才知道如何进行解析。响应格式如图14-7所示。



图14-7　返回数据

POST请求的响应为JSON格式，数据内容其实就是HTML代码，我们可以在HTML代码中通过XPath表达式提取用户链接，XPath表达式如下：



* * *



// a[@class="zm-item-link-avatar"]/@href



* * *



以上就是动态加载部分的全部分析，parse_next_relation方法代码如下：



* * *



def parse_next_relation(self,response): ''' 解析和我有关的人的剩余部分 :param response: :return: ''' user_id = response.request.meta['user_id'] relation_type = response.request.meta['relation_type'] payload = response.request.meta['payload'] relations_id=[] offset = response.request.meta['offset'] users_num = response.request.meta['users_num'] body = json.loads(response.body) user_divs = body.get('msg', []) for user_div in user_divs: selector = Selector(text=user_div) user_url = selector.xpath('// a[@class="zm-item-link-avatar"]/@href'). extract_first() relations_id.append(os.path.split(user_url)[-1]) # 发送请求 yield Request(response.urljoin(url=user_url), meta={'cookiejar': response.meta['cookiejar'] }, callback=self.parse_user_info, errback=self.parse_err) # 发送捕获到的关系数据 yield RelationItem(user_id=user_id, relation_type=relation_type, relations_id=relations_id) # 判断是否还有更多的数据 if offset + 20 < users_num: payload['params']['offset'] = offset+20 more_post = response.request.copy() more_post = more_post.replace( body=urlencode(payload), meta={'user_id':user_id, 'relation_type':relation_type, 'offset':offset+20, 'users_num':users_num, 'cookiejar': response.meta['cookiejar']}) yield more_post



* * *



上述代码通过解析动态响应中的HTML代码，提取其中关注者的链接，然后将关注者链接构造Request，交给parse_user_info解析用户信息，而且从链接中提取出用户user_id，构造成RelationItem交给Pipeline处理，最后判断已经获取的数据是否大于总的数据，如果没有，说明还有数据，改变offset偏移量，继续发送动态请求进行循环处理。





14.4　Pipeline


上一节完成了爬虫模块的编写，下面开始编写Pipeline，主要是完成Item到MongoDB的存储，分成两个集合进行存储：UserInfo和Relation。代码如下：



* * *



class ZhihucrawlPipeline(object): def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE', 'zhihu') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): if isinstance(item,UserInfoItem): self._process_user_item(item) else: self._process_relation_item(item) return item def _process_user_item(self,item): self.db.UserInfo.insert(dict(item)) def _process_relation_item(self,item): self.db.Relation.insert(dict(item))



* * *



代码和13.3节的非常类似，首先从Settings中加载MONGO_URI和MONGO_DATABASE，初始化连接URL和数据库名称，在process_item方法中判断item的类型，然后使用_process_user_item和_process_user_item方法存储到不同的集合中。下面在Settings中设置MONGO_URI，MONGO_DATABASE和激活Pipeline：



* * *



MONGO_URI = 'mongodb:// 127.0.0.1:27017/' MONGO_DATABASE='zhihu' ITEM_PIPELINES = { 'zhihuCrawl.pipelines.ZhihucrawlPipeline': 300, }



* * *





14.5　优化措施


以上几节其实已经完成了项目的所有功能，本小节对上面的内容进行部分优化，优化包括登录成功后对Cookie的存储和cookie登录，添加限速功能，防止被发现。

首先要做的是对Cookie的存储，即将Cookie序列化存储到session.txt中，将这段代码放到parse_user_info方法的开始位置。代码如下：



* * *



if not os.path.exists('session.txt'): with open('session.txt','wb') as f: import pickle cookies = response.request.headers['cookie'] cookieDict={} for cookie in cookies.split(';'): key,value = cookie[0:cookie.find('=')], cookie[cookie.find('=')+1:] cookieDict[key]=value pickle.dump(cookieDict,f)



* * *



首先判断文件是否存在，如果不存在，则从request.headers中提取cookie字符串，并将cookie字符串拆分和组装成字段，最后进行序列化存储。存储完毕后，我们需要在start_requests的起始位置加载Cookie，代码如下：



* * *



if os.path.exists('session.txt'): with open('session.txt','rb') as f: import pickle self.cookies = pickle.load(f) self.xsrf = self.cookies['_xsrf'] return [Request( self.start_urls[0], cookies=self.cookies, meta={'cookiejar': 1}, callback=self.parse_user_info, errback=self.parse_err, )]



* * *



首先判断session.txt是否存在，然后读取cookeis的值，直接通过Cookies构造Request请求，进入起始url解析阶段。

最后进行以下限速的操作，在Settings中设置如下：



* * *



DOWNLOAD_DELAY=2 AUTOTHROTTLE_ENABLED=True AUTOTHROTTLE_START_DELAY=5 AUTOTHROTTLE_MAX_DELAY=60



* * *



大家也可以根据自己的情况设置延时。





14.6　部署爬虫


知乎爬虫项目已经完成，下面我们需要将爬虫部署到服务器中。之前我们讲到的使用命令行或者API启动方式，略显粗糙，仅适合个人调试，不适合应用到实际的工程项目中，我们需要一种灵活稳定的方式启动和控制爬虫。





14.6.1　Scrapyd


Scrapy官方为我们提供了一个部署爬虫非常有用的工具Scrapyd。Scrapyd是运行Scrapy爬虫的服务程序，它支持以HTTP命令方式通过JSON API进行发布、删除、启动、停止爬虫程序的操作，而且Scrapyd可以同时管理多个爬虫，每个爬虫还可以有多个版本，也是部署分布式爬虫的有效手段。官方文档：http://scrapyd.readthedocs.io/en/latest/ 。

1.安装Scrapyd

主要有两种安装方式：

·pip install scrapyd，安装的版本可能不是最新版本。

·从https://github.com/scrapy/scrapyd 中下载源码，运行python setup.py install命令进行安装。

2.启动Scrapyd

在命令行中输入scrapyd，即可完成启动，如图14-8所示。默认情况下scrapyd运行后会侦听6800端口。



图14-8　启动Scrapyd

在浏览器中输入：http://127.0.0.1:6800/ ，可以打开Scrapyd界面，如图14-9所示。



图14-9　Scrapyd界面

3.Scrapyd API介绍

Scrapyd主要支持10种操作方式：

·获取Scrapyd状态：http://127.0.0.1：6800/daemonstatus.json 。GET请求方式。响应类似{”status“：“ok”，“running”：“0”，“pending”：“0”，“finished”：“0”，“node_name”：“node-name”} 获取项目列表：http://127.0.0.1:6800/listprojects.json ，GET请求方式。响应类似{“status”：“ok”，“projects”：[“myproject”，“otherproject”]}。

·获取项目下已发布的爬虫列表：http://127.0.0.1：6800/listspiders.jsonproject=myproject 。GET请求方式，参数为项目名称myproject。响应类似{“status”：“ok”，“spiders”：[“spider1”，“spider2”，“spider3”]}。

·获取已发布的爬虫版本列表：http://127.0.0.1：6800/listversions.jsonproject=myproject 。GET请求方式，参数为项目名称myproject。响应类似{“status”：“ok”，“versions”：[“r99”，“r156”]}。

·获取爬虫运行状态：http://127.0.0.1：6800/listjobs.jsonproject=myproject 。GET请求方式，参数为项目名称myproject。响应类似{“status”：“ok”，“pending”：[{“id”：“78391cc0fcaf11e1b0090800272a6d06”，“spider”：“spider1”}]，“running”：[{“id”：“422e608f9f28cef127b3d5ef93fe9399”，“spider”：“spider2”，“start_time”：“2012-09-1210：14：03.594664”}]，“finished”：[{“id”：“2f16646cfcaf11e1b0090800272a6d06”，“spider”：“spider3”，“start_time”：“2012-09-1210：14：03.594664”，“end_time”：“2012-09-1210：24：03.594664”}]}

·启动服务器上某一爬虫：http://127.0.0.1：6800/schedule.json 。POST请求方式，参数为“project”：myproject，“spider”：myspider，myproject为项目名称，myspider为爬虫名称。响应类似：{“status”：“ok”，“jobid”：“6487ec79947edab326d6db28a2d86511e8247444”}

·删除某一版本爬虫：http://127.0.0.1:6800/delversion.json 。POST请求方式，参数为“project”：myproject，“version”：myversion，myproject为项目名称，version为爬虫版本。

·删除某一工程，并将工程下各版本爬虫一起删除：http://127.0.0.1:6800/delproject.json 。POST请求方式，参数为“project”：myproject，myproject为项目名称。响应类似：{“status”：“ok”}

·给工程添加版本，如果工程不存在则创建：http://127.0.0.1:6800/addversion.json 。POST请求方式，参数为“project”：myproject，“version”：myversion，myproject为项目名称，version为项目版本。响应类似{“status”：“ok”，“spiders”：3}。

·取消一个运行的爬虫任务：http://127.0.0.1:6800/cancel.json 。POST请求方式，参数为“project”：myproject，“job”：jobid，myproject为项目名称，jobid为任务的id。响应类似{“status”：“ok”，“prevstate”：“running”}

大家只需要使用request发送请求，解析json响应就可以灵活地控制爬虫。但是上述API中还少了如何发布爬虫程序到Scrapyd服务中的功能，那是因为额外提供了Scrapyd-client发布工具。





14.6.2　Scrapyd-client


Scrapyd-client是一个专门用来发布scrapy爬虫的工具，安装该程序之后会自动在Python安装目录下scripts文件夹中生成scrapyd-deploy工具，其实类似于Python脚本，可以直接使用python scrapyd-deploy的方式运行。

1.安装Scrapyd-client

主要有两种安装方式：

·pip install Scrapyd-client，安装的版本可能不是最新版本。

·从https://github.com/scrapy/scrapyd-client 中下载源码，运行python setup.py install命令进行安装。

2.使用Scrapyd-client

安装完成后，将scrapyd-deploy拷贝到爬虫项目目录下，与scrapy.cfg在同一级目录。下面我们需要修改scrapy.cfg文件，默认生成的scrapy.cfg文件内容如下：



* * *



[settings] default = zhihuCrawl.settings [deploy] # url = http://127.0.0.1:6800/ project = zhihuCrawl



* * *



首先去掉url前的注释符号，url是scrapyd服务器的网址，project=zhihuCrawl为项目名称，可以随意起。修改[deploy]为[deploy：100]，表示把爬虫发布到名为100的爬虫服务器上，一般在需要同时发布爬虫到多个目标服务器时使用。修改如下：



* * *



[settings] default = zhihuCrawl.settings [deploy:100] url = http://127.0.0.1:6800/ project = zhihuCrawl



* * *



配置完成后，就可以使用scrapyd-deploy进行爬虫的发布了，命令如下：



* * *



scrapyd-deploy <target> -p <project> --version <version>



* * *



参数解释：

·target：deploy后面的名称。

·project：自行定义名称，跟爬虫的工程名字无关。

·version：自定义版本号，不写的话默认为当前时间戳。

下面将命令行切换到工程目录下，运行：python scrapyd-deploy 100-p zhihu--version ver2016011，如图14-10所示。

发布完成后根据API发送启动爬虫的请求，爬虫就可以正常工作了。爬取一段时间后，数据存储效果如图14-11所示。



图14-10　发布爬虫



图14-11　数据存储





14.7　小结


以上就是知乎爬虫实战项目的所有内容，着重强调了Request的构造方式，同时对爬虫在工程中的发布进行了描述。在此提出一个新的需求，如果想下载知乎用户的头像图片，上面的代码已经将头像的URL提取出来了，大家可以根据学过的知识，实现这个功能。





深入篇


·第15章　增量式爬虫

·第16章　分布式爬虫与Scrapy

·第17章　实战项目：Scrapy分布式爬虫

·第18章　人性化PySpider爬虫框架





第15章　增量式爬虫


本章我们讲解增量式爬虫，所谓增量式爬虫并不是新型的爬虫架构，而是根据项目需求而产生的一种爬虫类型。例如我们想爬取智联的职位信息，可是我们只想爬取每天更新的职位信息，不想全部都爬取，这就需要增量式爬虫。增量式爬虫的核心在于快速去重，我们必须判断哪些是已经爬取过的，哪些是新产生的。本章将对去重的方式和应用进行深入的讲解。





15.1　去重方案


去重一般的情况是对URL进行去重，也就说我们访问过的页面下次不再访问。但是也有一些情况，例如贴吧和论坛等社交网站，同一个URL，由于用户评论的存在，页面内容是一直变化的，如果想抓取评论内容，那就不能以URL为去重标准，但是本质上都可以看做是针对字符串的去重方式。

对于爬虫来说，由于网络间的链接错综复杂，爬虫在网络间爬行很可能会形成“环”，这对爬虫来说是非常可怕的事情，会一直做无用功。为了避免形成“环”，就需要知道Spider已经访问过哪些URL，基本上有如下几种方案：

1）关系型数据库去重。

2）缓存数据库去重。

3）内存去重。

首先讲解一下前两种方案：

·关系型数据库去重，需要将URL存入到数据库中，每来一个URL就启动一次数据库查询，数据量变得非常庞大后关系型数据库查询的效率会变得很低，不推荐。

·缓存数据库，比如现在比较流行的Redis，去重方式是使用其中的Set数据类型，类似于Python中的Set，也是一种内存去重方式，但是它可以将内存中的数据持久化到硬盘中，应用非常广泛，推荐。

对于第三种“内存去重”方案，还可以细分出三种不同的实现方式：

·将URL直接存储到HashSet中，也就是Python中的Set数据结构中，但是这种方式最明显的缺点是太消耗内存。随着URL的增多，占用的内存会越来越多。大家可以计算一下假如存储了1亿个链接，每个链接平均40个字符，这就占用了4G内存。

·将URL经过MD5或者SHA-1等单向哈希算法生成摘要，再存储到HashSet中。由于字符串经过MD5处理后的信息摘要长度只有128位，SHA-1处理后也只有160位，所以占用的内存将比第一种方式小很多倍。

·采用Bit-Map方法，建立一个BitSet，将每个URL经过一个哈希函数映射到某一位。这种方式消耗内存是最少，但缺点是单一哈希函数发生冲突的概率太高，极易发生误判。

内存去重方案的这三种实现方式各有优缺点，但是对于整个内存去重方案来说，比较致命的是内存大小的制约和掉电易丢失的特性，万一服务器宕机了，所有内存数据将不复存在。

通过对以上去重方式的分析，我们可以确定相对比较好的方式是内存去重方案+缓存数据库，更准确地说是内存去重方案的第二种实现方式+缓存数据库，这种方式基本上可以满足大多数中型爬虫的需要。但是本章要讲的不是针对百万级和千万级数据量的去重方案，而是当数据量上亿甚至几十亿时这种海量数据的去重方案，这就需要用到BloomFilter算法。





15.2　BloomFilter算法


BloomFilter（布隆过滤器）是由Bloom在1970年提出的一种多哈希函数映射的快速查找算法。BloomFilter是一种空间效率很高的随机数据结构，它利用位数组很简洁地表示一个集合，并能判断一个元素是否属于这个集合。BloomFilter的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，BloomFilter不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，BloomFilter通过极少的错误换取了存储空间的极大节省。





15.2.1　BloomFilter原理


上一节内存去重方案中的第三种实现方式Bit-Map方法，已经非常接近BloomFilter算法的思想，可是由于使用单一哈希函数，导致误判率很高，为了降低冲突，BloomFilter使用了多个哈希函数。下面讲解一下BloomFilter的实现原理：创建一个m位的位数组，先将所有位初始化为0。然后选择k个不同的哈希函数。第i个哈希函数对字符串str哈希的结果记为h（i，str），且h（i，str）的范围是0到m-1。如图15-1所示，将一个字符串经过k个哈希函数映射到m位数组中。



图15-1　哈希映射

从图中我们可以看到，字符串经过哈希函数映射成介于0到m-1之间的数字，并将m位位数组中下标等于这个数字的那一位置为1，这样就将字符串映射到位数组中的k个二进制位了。

如何判断字符串是否存在过呢？只需要将新的字符串也经过h（1，str），h（2，str），h（3，str），...，h（k，str）哈希映射，检查每一个映射所对应m位位数组的值是否为1。若其中任何一位不为1则可以判定str一定没有被记录过。但是若一个字符串对应的任何一位全为1，实际上是不能100%的肯定该字符串被BloomFilter记录过，这就是所说的低错误率。

以上即BloomFilter的原理，那如何选择BloomFilter参数呢？

首先哈希函数的选择对性能的影响应该是很大的，一个好的哈希函数要能近似等概率地将字符串映射到各个位。选择k个不同的哈希函数比较麻烦，一种常用的方法是选择一个哈希函数，然后送入k个不同的参数。

接下来我们要选取k、m、n的取值。哈希函数个数k、位数组大小m、加入的字符串数量n的关系，如表15-1所示。

表15-1　m、n、k关系表





表15-1所示的是m、n、k不同的取值所对应的漏失概率，即不存在的字符串有一定概率被误判为已经存在。m表示多少个位，也就是使用内存的大小，n表示去重字符串的数量，k表示哈希函数的个数。例如申请了256M内存，即1<<31，因此m=2^31，约21.5亿。将k设置为7，并查询表中k=7的那一列。当漏失率为8.56e-05时，m/n值为23。所以n=21.5/23=0.93（亿），表示漏失概率为8.56e-05时，256M内存可满足0.93亿条字符串的去重。如果大家想对m、n、k之间的关系有更深入的了解，推荐一篇非常有名的文献：http://pages.cs.wisc.edu/~cao/papers/summary-cache/node8.htm l。





15.2.2　Python实现BloomFilter


了解清楚原理之后，下面开始用Python来实现BloomFilter算法。在GitHub中有一个开源项目python-bloomfilter，这个项目不仅仅实现了BloomFilter，还实现了一个大小可动态扩展的ScalableBloomFilter。

1.安装python-bloomfilter

从https://github.com/qiyeboy/python-bloomfilter 中下载源码，进入源码目录，使用Python setup.py install即可完成安装。

2.示例

使用BloomFilter创建一个容量为1000，漏失率为0.001的布隆过滤器



* * *



from pybloom import BloomFilter f = BloomFilter(capacity=1000, error_rate=0.001) print [f.add(x) for x in range(10)] print 11 in f print 4 in f



* * *



输出结果为：



* * *



[False, False, False, False, False, False, False, False, False, False] False True



* * *



如果你不想静态指定容量，可以使用python-bloomfilter中可动态扩展的ScalableBloomFilter。示例如下：



* * *



from pybloom import ScalableBloomFilter sbf = ScalableBloomFilter(mode=ScalableBloomFilter.SMALL_SET_GROWTH) count = 10000 for i in xrange(0, count): sbf.add(i) print 10001 in sbf print 4 in sbf



* * *



输出结果为：



* * *



False True



* * *





15.3　Scrapy和BloomFilter


Scrapy自带了去重方案，同时支持通过RFPDupeFilter来完成去重。在RFPDupeFilter源码中依然是通过set（）进行去重。部分源码如下：



* * *



class RFPDupeFilter(BaseDupeFilter): """Request Fingerprint duplicates filter""" def __init__(self, path=None, debug=False): self.file = None self.fingerprints = set() self.logdupes = True self.debug = debug self.logger = logging.getLogger(__name__) if path: self.file = open(os.path.join(path, 'requests.seen'), 'a+') self.file.seek(0) self.fingerprints.update(x.rstrip() for x in self.file)



* * *



继续查看源代码，可以了解到Scrapy是根据request_fingerprint方法实现过滤的，将Request指纹添加到set（）中。部分源码如下：



* * *



def request_fingerprint(request, include_headers=None): if include_headers: include_headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers)) cache = _fingerprint_cache.setdefault(request, {}) if include_headers not in cache: fp = hashlib.sha1() fp.update(to_bytes(request.method)) fp.update(to_bytes(canonicalize_url(request.url))) fp.update(request.body or b'') if include_headers: for hdr in include_headers: if hdr in request.headers: fp.update(hdr) for v in request.headers.getlist(hdr): fp.update(v) cache[include_headers] = fp.hexdigest() return cache[include_headers]



* * *



从代码中我们可以看到，去重指纹为sha1（method+url+body+header），对这个整体进行去重，去重比例太小。下面我们根据URL进行去重，定制过滤器。代码如下：



* * *



from scrapy.dupefilter import RFPDupeFilter class URLFilter(RFPDupeFilter): """根据URL过滤""" def __init__(self, path=None): self.urls_seen = set() RFPDupeFilter.__init__(self, path) def request_seen(self, request): if request.url in self.urls_seen: return True else: self.urls_seen.add(request.url)



* * *



但是这样依旧不是很好，因为URL有时候会很长导致内存上升，我们可以将URL经过sha1操作之后再去重，改进如下：



* * *



from scrapy.dupefilter import RFPDupeFilter from w3lib.util.url import canonicalize_url class URLSha1Filter(RFPDupeFilter): """根据urlsha1过滤""" def __init__(self, path=None): self.urls_seen = set() RFPDupeFilter.__init__(self, path) def request_seen(self, request): fp = hashlib.sha1() fp.update(canonicalize_url(request.url)) url_sha1 = fp.hexdigest() if url_sha1 in self.urls_seen: return True else: self.urls_seen.add(url_sha1)



* * *



这样似乎好了一些，但是依然不够，继续优化，加入BloomFilter进行去重。改进如下：



* * *



class URLBloomFilter(RFPDupeFilter): """根据urlhash_bloom过滤""" def __init__(self, path=None): self.urls_sbf = ScalableBloomFilter(mode=ScalableBloomFilter.SMALL_SET_　GROWTH) RFPDupeFilter.__init__(self, path) def request_seen(self, request): fp = hashlib.sha1() fp.update(canonicalize_url(request.url)) url_sha1 = fp.hexdigest() if url_sha1 in self.urls_sbf: return True else: self.urls_sbf.add(url_sha1)



* * *



经过这样的处理，去重能力将得到极大提升，但是稳定性还是不够，因为是内存去重，万一出现服务器宕机的情况，内存数据将全部消失。如果能把Scrapy、BloomFilter、Redis这三者完美地结合起来，才是一个比较稳定的选择。下一章将继续讲解Redis+BloomFilter去重。有一点一定要注意，代码编写完成后，去重组件是无法工作的，需要在settings中设置DUPEFILTER_CLASS字段，指定过滤器类的路径，比如：



* * *



DUPEFILTER_CLASS = "test.test.bloomRedisFilter. URLBloomFilter "



* * *





15.4　小结


本章讲解了各式各样的去重方案，不断改进实现方式。但是选择去重方案还是要根据项目需求和成本因素合理选择，最好的不一定就是最适合的。





第16章　分布式爬虫与Scrapy


分布式是大数据时代比较流行的一个词，比如分布式计算、分布式存储，当然还有分布式爬虫。分布式爬虫，从字面意义上讲是集群爬虫，就是将爬取任务分配给多台机器同时处理，而与之相对应的是单机爬虫，单点部署，单点操作。分布式爬虫相当于将多个单机联系起来形成一个整体来完成工作，有一种“众人拾柴火焰高”的感觉，目的是提高可用性、稳定性和性能，毕竟单机有CPU、IO和带宽等多重限制。

打造分布式爬虫的关键是调度，因为需要将单机关联起来，现在采用的方式是消息队列。前几章讲到的Scrapy框架是基于单机的，但是通过重写调度器的形式可以将Scrapy改造成分布式爬虫，其中用到了Redis作为消息队列。本章的主要内容是如何使用Scrapy打造分布式爬虫。





16.1　Redis基础


使用Scrapy打造分布式爬虫，首先要了解Redis的基础知识，在分布式爬虫中，Redis处于非常关键的地位。





16.1.1　Redis简介


Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。Redis有以下特点：

·支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候再次加载进行使用。

·不仅支持简单的key-value类型的数据，同时还提供list、set、sorted set、hash等数据结构的存储，因此也被称为数据结构服务器。

·支持master-slave模式的数据备份。

Redis是现在非常流行的Key-Value数据库，相比其他的Key-Value数据库，优势明显，主要包括以下方面：

·基于内存，性能极高。Redis读的速度是110000次/秒，写的速度是81000次/秒。

·支持丰富的数据结构。

·具有丰富的特性，比如支持publish/subscribe、通知、key过期等。





16.1.2　Redis的安装和配置


1.Redis的安装

对于Redis安装，主要讲解Linux和Windows平台。Redis官方下载地址为：https://redis.io/download ，如图16-1所示。



图16-1　Redis官网

选择下载Stable下的源码包，本书用的版本是3.2.5。将源码包下载下来进行解压和编译：



* * *



$ wget http://download.redis.io/releases/redis-3.2.5.tar.gz $ tar xzf redis-3.2.5.tar.gz $ cd redis-3.2.5 $ make



* * *



编译效果如图16-2所示。

编译成功后，进入src目录下，执行./redis-server命令，即可启动Redis服务。Redis服务端的默认连接端口是6379，启动效果如图16-3所示。

Redis官方不支持Windows版，Windows版是由Microsoft Open Tech group小组维护，并在GitHub上发布了Win64版，下载地址如下：

https://github.comMSOpenTech/redis/releases



我们直接下载免安装版，如图16-4所示。



图16-2　编译Redis



图16-3　启动Redis



图16-4　Win64版

将压缩包解压，并进入其中，有如下文件：

·EventLog.dll

·Redis on Windows Release Notes.docx

·Redis on Windows.docx

·redis-benchmark.exe

·redis-benchmark.pdb

·redis-check-aof.exe

·redis-check-aof.pdb

·redis-cli.exe

·redis-cli.pdb

·redis-server.exe

·redis-server.pdb

·redis.windows-service.conf

·redis.windows.conf

·Windows Service Documentation.docx

双击redis-server.exe即可以运行Redis服务，效果如图16-5所示。



图16-5　启动redis-server

最好将redis-server.exe添加到Path环境变量中，之后就可以在命令行中快速启动服务。

2.Redis的配置

无论是Linux版还是Windows版，Redis里面都有一个配置文件。Linux下的配置文件为redis.conf，和src在同一级目录下，Windows下的配置文件为redis.windows.conf。在使用redis-server启动服务的时候，可以在命令后面指定配置文件，类似如下的情况：



* * *



redis-server redis.conf



* * *



Redis配置文件的可用参数如表16-1所示。

表16-1　配置参数





16.1.3　Redis数据类型与操作


Redis支持五种数据类型：string（字符串）、hash（哈希）、list（列表）、set（集合）及sorted set（有序集合）。

1.string类型

string是Redis最基本的类型，一个key对应一个value。string类型可以包含任何数据，比如jpg图片或者序列化的对象。从内部实现来看，其实string可以看作byte数组，是二进制安全的，一个键最大能存储512MB。

下面我们在Redis客户端中进行操作，使用redis-cli命令连接Redis服务，语法格式为：redis-cli-h host-p port-a password。如果Redis服务在本机运行，而且无密码，可以直接在命令行中输入：redis-cli，如图16-6所示。



图16-6　连接redis-server

下面使用set和get操作string类型数据，示例如下：



* * *



127.0.0.1:6379> set name qiye OK 127.0.0.1:6379> get name "qiye" 127.0.0.1:6379>



* * *



以上示例使用了Redis的set和get命令。键为name，对应的值为qiye。

2.hash类型

Redis的hash类型是一个string类型的field和value的映射表，特别适合用于存储对象。相较于将对象的每个字段存成单个string类型，将一个对象存储在hash类型中会占用更少的内存，并且可以更方便地存取整个对象。每个hash可以存储232 -1个键值对，大约40多亿条。下面使用hset、hget、hmset、hmget命令进行操作，示例如下：



* * *



127.0.0.1:6379> hset person name qiye (integer) 1 127.0.0.1:6379> hget person name "qiye" 127.0.0.1:6379> hmset student name qiye age 20 country china OK 127.0.0.1:6379> hmget student age 1) "20" 127.0.0.1:6379> hmget student names 1) (nil) 127.0.0.1:6379>



* * *



以上示例中，使用Redis的hset设置了key为person、field为name、value为qiye的hash数据，hmset可以设置多个field的值。

3.list类型

list类型是一个双向键表，其每个子元素都是string类型，最多可存储232 -1个元素，大约40多亿，可以使用push、pop操作从链表的头部或者尾部添加删除元素，操作中key可以理解为链表的名字。下面使用lpush和lrange命令进行操作，示例如下：



* * *



127.0.0.1:6379> lpush country china (integer) 1 127.0.0.1:6379> lpush country USA (integer) 2 127.0.0.1:6379> lpush country UK (integer) 3 127.0.0.1:6379> lrange country 0 10 1) "UK" 2) "USA" 3) "china"



* * *



以上示例中使用lpush往country中添加了china、USA、UK等值，使用lrange从指定起始位置取出country中的值。

4.set类型

set是string类型的无序集合，最大可以包含232 -1个元素，约40多亿。对集合可以添加删除元素，也可以对多个集合求交并差，操作中key可以理解为集合的名字。set通过hash table实现，添加、删除和查找的复杂度都是O（1）。hash table会随着添加或者删除自动调整大小。下面使用sadd和smembers命令进行操作，示例如下：



* * *



127.0.0.1:6379> sadd url www.baidu.com (integer) 1 127.0.0.1:6379> sadd url www.google.com (integer) 1 127.0.0.1:6379> sadd url www.qq.com (integer) 1 127.0.0.1:6379> sadd url www.qq.com (integer) 0 127.0.0.1:6379> smembers url 1) "www.baidu.com" 2) "www.qq.com" 3) "www.google.com"



* * *



以上示例通过sadd添加了四次数据，重复的数据是会被忽略的，最后通过smembers获取url中的值，只有三条数据，这就可以使用set类型进行URL去重。

5.sorted set类型

和set一样，sorted set也是string类型元素的集合，不允许重复的成员。sorted set算是set的升级版本，它在set的基础上增加了一个顺序属性，会关联一个double类型的score。这一属性在添加和修改元素的时候可以指定，每次指定后，sorted set会自动重新按新的值调整顺序。sorted set成员是唯一的，但score却可以重复。下面使用zadd和zrangebyscore命令进行操作，示例如下：



* * *



127.0.0.1:6379> zadd web 0 flask (integer) 1 127.0.0.1:6379> zadd web 1 web.py (integer) 1 127.0.0.1:6379> zadd web 2 django (integer) 1 127.0.0.1:6379> zadd web 3 flask (integer) 0 127.0.0.1:6379> zrangebyscore web 0 5 1) "web.py" 2) "django" 3) "flask"



* * *



以上示例通过zadd添加了四次数据，重复的数据是会被忽略的，最后通过zrangebyscore根据score范围获取web中的值。

通过以上内容的学习，大家已经了解了Redis的基本用法，Redis还有很多命令和用法，大家感兴趣的话，可以去Redis中文网：http://www.redis.net.cn/ 进行学习。





16.2　Python和Redis


了解完Redis的基础知识后，我们最关心的是如何使用Python对Redis进行操作。Redis有很多开源的Python接口，但是比较成熟和稳定的，也是我比较推荐的是redis-py，如图16-7所示。



图16-7　Python接口





16.2.1　Python操作Redis


1.安装Redis

·pip install redis

·从https://github.com/andymccurdy/redis-py 下载源码，执行python setup install。

2.建立连接

首先导入redis模块，通过指定主机和端口和redis建立连接，进行操作，示例如下：



* * *



import redis r = redis.Redis(host='127.0.0.1', port=6379) r.set('name', 'qiye') print r.get('name')



* * *



或者使用连接池管理redis的连接，避免每次建立、释放连接的开销，示例如下：



* * *



pool = redis.ConnectionPool(host='127.0.0.1', port=6379) r = redis.Redis(connection_pool=pool) r.set('name', 'qiye') print r.get('name')



* * *



3.操作string类型

下面讲解一下常用的操作string类型的方法。

1）set（name，value，ex=None，px=None，nx=False，xx=False）

说明：用于设置键值对。

参数：

·name：键。

·value：值。

·ex：过期时间，单位秒。

·px：过期时间，单位毫秒。

·nx：如果设置为True，则只有name不存在时，当前set操作才执行。

·xx：如果设置为True，则只有name存在时，当前set操作才执行。

示例如下：



* * *



import redis pool = redis.ConnectionPool(host='127.0.0.1', port=6379) r = redis.Redis(connection_pool=pool) r.set('name', 'qiye',ex=3) print r.get('name')



* * *



3秒之后name键所对应的值为None。

2）setnx（name，value）

说明：只有当name不存在时，才能进行设置操作。

参数：

·name：键。

·value：值。

示例如下：



* * *



r.setnx('name','hah')



* * *



3）setex（name，value，time）

说明：用于设置键值对。

参数：

·name：键。

·value：值。

·time：过期时间，可以是timedelta对象或者是数字秒。

示例如下：



* * *



r.setex("name","qiye",5) print r.get('name')



* * *



5秒后，name的值变为None。

4）psetex（name，time_ms，value）

说明：用于设置键值对。

参数：

·name：键。

·time_ms：过期时间，可以是timedelta对象或者是数字毫秒。

·value：值。

示例如下：



* * *



r. psetex ("name",5000，"qiye") print r.get('name')



* * *



5秒后，name的值变为None。

5）mset（*args，**kwargs）

说明：用于批量设置键值对。

示例如下：



* * *



r.mset(age=20,country='china')



* * *



6）mget（keys，*args）

说明：用于批量获取键值。

参数：

·keys：多个键。

示例如下：



* * *



print r.mget('age','country') print r.mget(['age','country'])



* * *



7）getset（name，value）

说明：用于设置新值并获取原来的值。

参数：

·name：键。

·value：值。

示例如下：



* * *



print r.getset('name','hello')



* * *



打印出来的值为qiye，也就是之前设置的值。

8）getrange（key，start，end）

说明：根据字节获取子字符串。

参数：

·key：键。

·start：起始位置，单位字节。

·end：结束位置，单位字节。

示例如下：



* * *



r.set('name','qiye安全博客') print r.getrange('name',4,9)



* * *



输出结果为：安全，因为汉字是3个字节，字母是1个字节。

9）setrange（name，offset，value）

说明：从指定字符串索引开始向后修改字符串内容。

参数：

·name：键。

·offset：索引，单位字节。

·value：值。

示例如下：



* * *



r.set('name','qiye安全博客') r.setrange("name",1,"python") print r.get('name')



* * *



输出结果为：qpython全博客。

10）setbit（name，offset，value）

说明：对name对应值的二进制形式进行位操作。

参数：

·name：键。

·offset：索引，单位为位。

·value：0或1。

示例如下：



* * *



from binascii import hexlify r.set('name','qiye') print bin(int(hexlify('qiye'),16)) r.setbit('name',2,0) print r.get('name') print bin(int(hexlify(r.get('name')),16))



* * *



输出结果为：



* * *



01110001011010010111100101100101 Qiye 01010001011010010111100101100101



* * *



11）getbit（name，offset）

说明：获取name对应值的二进制形式中某位的值。

参数：

·name：键。

·offset：索引，单位为位。

示例如下：



* * *



print r.getbit('name',2)



* * *



12）bitcount（key，start=None，end=None）

说明：获取name对应值的二进制形式中1的个数。

参数：

·key：Redis的name。

·start：字节起始位置。

·end：字节结束位置。

示例如下：



* * *



print r.bitcount('name',0,1)



* * *



13）strlen（name）

说明：返回name对应值的长度。

参数：

·name：键。

示例如下：



* * *



print r.strlen('name')



* * *



14）append（key，value）

说明：在name对应值之后追加内容。

参数：

·key：键。

·value：要追加的字符串。

示例如下：



* * *



r.append('name','python')



* * *



4.操作hash类型

下面讲解一下常用的操作hash类型的方法。

1）hset（name，key，value）

说明：设置name对应hash中的一个键值对，如果不存在，则创建；否则，进行修改。

参数：

·name：hash的name。

·key：hash中的key。

·value：hash中的value。

示例如下：



* * *



r.hset('student','name','qiye')



* * *



2）hmset（name，mapping）

说明：在name对应的hash中批量设置键值对。

参数：

·name：hash的name。

·mapping：字典。

示例如下：



* * *



r.hmset('student', {'name':'qiye', 'age': 20})



* * *



3）hget（name，key）

说明：获取name对应的hash中key的值。

参数：

·name：hash的name。

·key：

示例如下：



* * *



r.hget('student','name')



* * *



4）hmget（name，keys，*args）

说明：批量获取name对应的hash中多个key的值。

参数：

·name：hash对应的name。

·keys：要获取的key集合，如：['k1'，'k2'，'k3']。

·*args：要获取的key，如：k1，k2，k3。

示例如下：



* * *



print r.hmget('student',['name','age']) print r.hmget('student','name','age')



* * *



5.操作list类型

下面讲解一下常用的操作list类型的方法。

1）lpush（name，values）

说明：在name对应的list中添加元素，每个新的元素都添加到列表的最左边。

参数：

·name：list对应的name。

·values：要添加的元素。

示例如下：



* * *



r.lpush('digit', 11,22,33)



* * *



存储顺序为：33，22，11。添加到list右边使用rpush（name，values）方法。

2）linsert（name，where，refvalue，value）

说明：name对应的列表的某一个值前或后插入一个新值。

参数：

·name：list的name。

·where：before或after。

·refvalue：在它前后插入数据。

·value：插入的数据。

示例如下：



* * *



r.linsert("digit","before","22","aa")



* * *



这个例子的意思是往列表中左边第一个出现的元素22前插入元素aa。

3）r.lset（name，index，value）

说明：对name对应list中的某一个索引位置赋值。

参数：

·name：list的name。

·index：list的索引位置。

·value：要设置的值。

示例如下：



* * *



r.lset("digit",4,44)



* * *



4）lrem（name，value，num）

说明：在name对应的list中删除指定的值。

参数：

·name：list的name。

·value：要删除的值。

·num：第num次出现。当num=0，删除列表中所有的指定值。

示例如下：



* * *



r.lrem("digit","22",1)



* * *



5）lpop（name）

说明：在name对应列表的左侧获取第一个元素并在列表中移除和返回。

参数：

·name：list的name。

示例如下：



* * *



r.lpop("digit")



* * *



6.操作set类型

下面讲解一下常用的操作set类型的方法。

1）sadd（name，values）

说明：为name集合添加元素。

参数：

·name：set的name。

·values：要添加的元素。

示例如下：



* * *



r.sadd("num",33,44,55,66)



* * *



2）scard（name）

说明：获取name对应的集合中元素的个数。

参数：

·name：set的name。

示例如下：



* * *



r.scard(name)



* * *



3）smembers（name）

说明：获取name对应的集合的所有成员。

参数：

·name：set的name。

示例如下：



* * *



r. smembers (name)



* * *



4）sdiff（keys，*args）

说明：获取多个name对应集合的差集。

示例如下：



* * *



print(r.sdiff("num1 ","num2"))



* * *



含义是求num1和num2的差集。

5）sinter（keys，*args）

说明：获取多个name对应集合的交集。

示例如下：



* * *



print(r. sinter ("num1 ","num2"))



* * *



含义是求num1和num2的交集。

6）sunion（keys，*args）

说明：获取多个name对应集合的并集。

示例如下：



* * *



print(r. sunion ("num1 ","num2"))



* * *



含义是求num1和num2的并集。

7.操作sorted set类型

下面讲解一下常用的操作sorted set类型的方法。

1）zadd（name，*args，**kwargs）

说明：在name对应的有序集合中添加元素和元素对应的分数。

示例如下：



* * *



r.zadd("z_num",num1=11,num2=22)



* * *



2）zcard（name）

说明：获取name对应的有序集合中元素个数。

示例如下：



* * *



print r. zcard ("z_num")



* * *



3）zrange（name，start，end，desc=False，withscores=False，score_cast_func=float）

说明：按照索引范围获取name对应的有序集合的元素。

参数：

·name：sorted set的name。

·start：有序集合索引起始位置（非分数）。

·end：有序集合索引结束位置（非分数）。

·desc：排序规则，默认按照分数从小到大排序。

·withscores：是否获取元素的分数，默认只获取元素的值。

·score_cast_func：对分数进行数据转换的函数。

示例如下：



* * *



print r. zrange ("z_num",0,10)



* * *



4）zrem（name，values）

说明：删除name对应有序集合中值是values的成员。

示例如下：



* * *



zrem(' z_num ', ['num1', 'num2'])



* * *



5）zscore（name，value）

说明：获取name对应有序集合中value对应的分数。

示例如下：



* * *



print(r.zscore("z_num","num1"))



* * *





16.2.2　Scrapy集成Redis


在Scrapy中，如果想实现分布式，需要使用Redis作为消息队列，通过安装scrapy-redis组件就可以实现。scrapy-redis在Scrapy框架的哪一部分起作用呢？可以在使用前和使用后进行一下对比。原框架工作流程如图16-8所示。



图16-8　Request队列

使用scrapy-redis后的架构，如图16-9所示。

前后发生的主要变化是Request队列放到了Redis中，这样多个单机就可以通过Redis获取Request，实现分布式，同时将要存储的结构化数据存到Redis队列中。



图16-9　Redis Request队列

下面安装scrapy-redis，官方文档：https://scrapy-redis.readthedocs.org ，主要有两种安装方式：

·pip install scrapy_redis

·从https://github.com/rolando/scrapy-redis 下载源码，解压后，使用Python setup.py install命令。

安装完成后，需要在settings中进行配置才能使用，配置字段如下：



* * *



# 使用scrapy_redis的调度器： SCHEDULER = "scrapy_redis.scheduler.Scheduler" # 在Redis中保持scrapy-redis用到的各个队列，从而允许暂停和暂停后恢复： SCHEDULER_PERSIST = True # 使用scrapy_redis的去重方式： DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter" # 使用scrapy_redis的存储方式： ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 300 } # 定义Redis的IP和端口： REDIS_HOST = '127.0.0.1' REDIS_PORT = 6379



* * *





16.3　MongoDB集群


本节简单讲解一下MongoDB集群，集群的存在能很大程度上提高系统的稳定性。比如在分布式爬虫爬取过程中，MongoDB存储服务器宕机了，那整个爬虫系统将立刻陷入瘫痪。为了避免这种情况，可以使用多个MongoDB存储节点，当主节点挂掉，从节点可以立刻补充进来，保持系统的稳定运行。下面讲解通过MongoDB副本集的形式来搭建主从集群。副本集的工作模式如图16-10所示。



图16-10　副本集工作模式

从图可以看到应用服务器与主节点之间进行读写操作，主节点将数据实时地同步到从节点中，主节点和从节点之间通过心跳检测的方式进行沟通，判断是否存活。

假如主节点突然出现故障，这个时候两个从节点会通过仲裁的方式，判断谁作为新的主节点，如图16-11所示。



图16-11　节点自动切换

下面我们正式采用副本集的方式搭建MongoDB集群。首先准备三台机器，由于暂时没有这么多可用的主机，采取在一台主机上开启不同的端口，来实现三台机器的效果，搭建的方式都是一样的。选择127.0.0.1：27017作为主节点，127.0.0.1：27018和127.0.0.1：27019作为从节点，并且在主机上建立三个不同的文件夹，作为数据库存储。注意每个文件夹里面都要创建一个空的data文件夹。下面开启三个命令行窗口，来启动三个不同的mongodb服务，不同窗口的输入命令如下：

·主节点master：mongod--dbpath D：\mongodb\data--replSet repset

·从节点slave1：mongod--dbpath D：\mongodb_slave1\data--port 27018--replSet repset

·从节点slave2：mongod--dbpath D：\mongodb_slave2\data--port 27019--replSet repset

三个服务启动成功后，需要初始化副本集。随意登录其中一个服务，比如登录主节点，另启一个命令行窗口依次输入如下内容：



* * *



1.mongo 2.use admin 3.config = { _id:"repset", members:[ {_id:0,host:" 127.0.0.1:27017"}, {_id:1,host:" 127.0.0.1:27018"}, {_id:2,host:" 127.0.0.1:27019"}] }



* * *



注意这里的_id：“repset”和上面的命令参数--replSet repset要保持一样。最后输入：



* * *



rs.initiate(config);



* * *



用来初始化配置，效果如图16-12所示。



图16-12　初始化配置

这个时候接着输入：rs.status（），用来查看节点信息。图16-13表示已经成功搭建起主从节点。



图16-13　主从状态

搭建完成后，我们需要测试一下是否主从节点会自动同步。继续在命令行中输入如下命令：



* * *



use test; > db.testdb.insert({"test":"testslave"})



* * *



这个时候终止对主节点的连接，使用mongo 127.0.0.1：27018登录从节点，查看数据是否同步。依次输入：



* * *



connecting to: 127.0.0.1:27018/test repset:SECONDARY> use test switched to db test repset:SECONDARY> show tables 2016-12-04T05:49:46.273+0800 E QUERY [thread1] Error: listCollections failed: { "ok" : 0, "errmsg" : "not master and slaveOk=false", "code" : 13435 } : _getErrorWithCode@src/mongo/shell/utils.js:25:13 DB.prototype._getCollectionInfosCommand@src/mongo/shell/db.js:773:1 DB.prototype.getCollectionInfos@src/mongo/shell/db.js:785:19 DB.prototype.getCollectionNames@src/mongo/shell/db.js:796:16 shellHelper.show@src/mongo/shell/utils.js:754:9 shellHelper@src/mongo/shell/utils.js:651:15 @(shellhelp2):1:1



* * *



发生了错误，这是因为mongodb默认是从主节点读写数据的，副本节点上不允许读，需要设置副本节点可以读。继续输入：



* * *



repset:SECONDARY> db.getMongo().setSlaveOk(); repset:SECONDARY> db.testdb.find(); { "_id" : ObjectId("58433d64c360caed9f4a2b26"), "test" : "testslave" }



* * *



可以看到数据已经同步到从节点了，整体流程如图16-14所示。



图16-14　主从同步

最后一步测试故障发生时，主从节点是否能完成角色切换，现在强制关闭主节点master。经过一系列的投票选择操作，slave1当选主节点，如图16-15所示。

以上就是搭建MongoDB集群的过程，大家可能会问如何使用程序来访问副本集呢？方法很简单，使用Pymongo，代码如下：



* * *



from pymongo import MongoClient client = MongoClient("mongodb:// 127.0.0.1:27017,127.0.0.1:27018,127.0.0.1:27019", replicaset='repset') print client.test.testdb.find_one()



* * *





图16-15　主动切换





16.4　小结


本章主要讲解了通过scrapy-redis将Scrapy打造为分布式爬虫结构，还讲解了MongoDB集群的搭建，帮助大家对分布式爬虫和分布式存储有一个清晰的概念，下一章我们将开启实战项目，使用Scrapy打造一个分布式爬虫。





第17章　实战项目：Scrapy分布式爬虫


上一章已经讲解了分布式爬虫和如何使用Scrapy搭建分布式爬虫，本章将开始进行实战项目，本次项目的主题是爬取云起书院网站上的小说数据，下面会对整个分析过程和关键代码进行讲解，完整的项目代码在GitHub（https://github.com/qiyeboy/ ）上。





17.1　创建云起书院爬虫


在开始编程之前，我们首先需要根据项目需求对云起书院网站进行分析。目标是提取小说的名称、作者、分类、状态、更新时间、字数、点击量、人气和推荐等数据。首先来到云起书院的书库（http://yunqi.qq.com/bk ），如图17-1所示。

可以在图书列表中找到每一本书的名称、作者、分类、状态、更新时间、字数等信息。同时将页面滑到底部，可以看到翻页的按钮，如图17-2所示。

接着选其中一部小说点击进去，可以进到小说的详情页，在作品信息里，我们可以找到点击量、人气和推荐等数据，如图17-3所示。

以上将整个云起书院爬虫项目的流程分析完成，编程可以正式开始了。首先在命令行中切换到用于存储项目的路径，然后输入以下命令创建云起书院爬虫项目和爬虫模块：



* * *



scrapy startproject yunqiCrawl cd yunqiCrawl scrapy genspider -t crawl yunqi.qq.com yunqi.qq.com



* * *





图17-1　图书列表



图17-2　翻页按钮



图17-3　小说详情页





17.2　定义Item


创建完工程后，首先要做的是定义Item，确定我们需要提取的结构化数据。主要定义两个Item，一个负责装载小说的基本信息，一个负责装载小说热度（点击量和人气等）的信息。代码如下：



* * *



import scrapy class YunqiBookListItem(scrapy.Item): # 小说id novelId = scrapy.Field() # 小说名称 novelName = scrapy.Field() # 小说链接 novelLink = scrapy.Field() # 小说作者 novelAuthor = scrapy.Field() # 小说类型 novelType = scrapy.Field() # 小说状态 novelStatus = scrapy.Field() # 小说更新时间 novelUpdateTime = scrapy.Field() # 小说字数 novelWords = scrapy.Field() # 小说封面 novelImageUrl = scrapy.Field() class YunqiBookDetailItem(scrapy.Item): # 小说id novelId = scrapy.Field() # 小说标签 novelLabel =scrapy.Field() # 小说总点击量 novelAllClick = scrapy.Field() # 月点击量 novelMonthClick = scrapy.Field() # 周点击量 novelWeekClick = scrapy.Field() # 总人气 novelAllPopular = scrapy.Field() # 月人气 novelMonthPopular = scrapy.Field() # 周人气 novelWeekPopular = scrapy.Field() # 评论数 novelCommentNum = scrapy.Field() # 小说总推荐 novelAllComm = scrapy.Field() # 小说月推荐 novelMonthComm = scrapy.Field() # 小说周推荐 novelWeekComm = scrapy.Field()



* * *





17.3　编写爬虫模块


17.1节通过genspider命令已经创建了一个基于CrawlSpider类的爬虫模板，类名称为YunqiQqComSpider。下面开始进行页面的解析，主要有两个方法。parse_book_list方法用于解析图17-1所示的图书列表，抽取其中的小说基本信息。parse_book_detail方法用于解析图17-3所示页面中的小说点击量和人气等数据。对于翻页链接抽取，则是在rules中定义抽取规则，翻页链接基本上符合“/bk/so2/n30p\d+”这种形式，YunqiQqComSpider完整代码如下：



* * *



class YunqiQqComSpider(CrawlSpider): name = 'yunqi.qq.com' allowed_domains = ['yunqi.qq.com'] start_urls = ['http://yunqi.qq.com/bk/so2/n30p1'] rules = ( Rule(LinkExtractor(allow=r'/bk/so2/n30p\d+'), callback='parse_book_list', follow=True), ) def parse_book_list(self,response): books = response.xpath(".// div[@class='book']") for book in books: novelImageUrl = book.xpath("./a/img/@src").extract_first() novelId = book.xpath("./div[@class='book_info']/h3/a/@id").extract_first() novelName =book.xpath("./div[@class='book_info']/h3/a/text()"). extract_first() novelLink = book.xpath("./div[@class='book_info']/h3/a/@href"). extract_first() novelInfos = book.xpath("./div[@class='book_info']/dl/dd[@class='w_auth']") if len(novelInfos)>4: novelAuthor = novelInfos[0].xpath('./a/text()').extract_first() novelType = novelInfos[1].xpath('./a/text()').extract_first() novelStatus = novelInfos[2].xpath('./text()').extract_first() novelUpdateTime = novelInfos[3].xpath('./text()').extract_first() novelWords = novelInfos[4].xpath('./text()').extract_first() else: novelAuthor='' novelType ='' novelStatus='' novelUpdateTime='' novelWords=0 bookListItem = YunqiBookListItem(novelId=novelId,novelName=novelName, novelLink=novelLink,novelAuthor=novelAuthor, novelType=novelType,novelStatus=novelStatus, novelUpdateTime=novelUpdateTime,novelWords=novelWords, novelImageUrl=novelImageUrl) yield bookListItem request = scrapy.Request(url=novelLink,callback=self.parse_book_ detail) request.meta['novelId'] = novelId yield request def parse_book_detail(self,response): # from scrapy.shell import inspect_response # inspect_response(response, self) novelId = response.meta['novelId'] novelLabel = response.xpath("// div[@class='tags']/text()").extract_first() novelAllClick = response.xpath(".// *[@id='novelInfo']/table/tr[2]/td[1]/ text()").extract_first() novelAllPopular = response.xpath(".// *[@id='novelInfo']/table/tr[2]/td[2]/ text()").extract_first() novelAllComm = response.xpath(".// *[@id='novelInfo']/table/tr[2]/td[3]/ text()").extract_first() novelMonthClick = response.xpath(".// *[@id='novelInfo']/table/tr[3]/td[1]/ text()").extract_first() novelMonthPopular = response.xpath(".// *[@id='novelInfo']/table/tr[3]/td[2]/text()").extract_first() novelMonthComm = response.xpath(".// *[@id='novelInfo']/table/tr[3]/td[3]/text()").extract_first() novelWeekClick = response.xpath(".// *[@id='novelInfo']/table/tr[4]/td[1]/ text()").extract_first() novelWeekPopular = response.xpath(".// *[@id='novelInfo']/table/tr[4]/td[2]/text()").extract_first() novelWeekComm = response.xpath(".// *[@id='novelInfo']/table/tr[4]/td[3]/ text()").extract_first() novelCommentNum = response.xpath(".// *[@id='novelInfo_commentCount']/text()").extract_first() bookDetailItem = YunqiBookDetailItem(novelId=novelId,novelLabel=novelLabel, novelAllClick=novelAllClick,novelAllPopular=novelAllPopular, novelAllComm=novelAllComm,novelMonthClick=novelMonthClick, novelMonthPopular=novelMonthPopular, novelMonthComm=novelMonthComm,novelWeekClick=novelWeekClick, novelWeekPopular=novelWeekPopular, novelWeekComm=novelWeekComm,novelCommentNum=novelCommentNum) yield bookDetailItem



* * *



大家对页面的抽取应该很熟悉了，以上代码很简单，这里不再赘述。





17.4　Pipeline


上一节完成了爬虫模块的编写，下面开始编写Pipeline，主要是完成Item到MongoDB的存储，分成两个集合进行存储，并采用上一章搭建的MongoDB集群的方式。和之前编写的Pipeline大同小异，在其中加入了一部分数据清洗操作。代码如下：



* * *



class YunqicrawlPipeline(object): def __init__(self, mongo_uri, mongo_db,replicaset): self.mongo_uri = mongo_uri self.mongo_db = mongo_db self.replicaset = replicaset @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE', 'yunqi'), replicaset = crawler.settings.get('REPLICASET') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri,replicaset=self.replicaset) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): if isinstance(item,YunqiBookListItem): self._process_booklist_item(item) else: self._process_bookeDetail_item(item) return item def _process_booklist_item(self,item): ''' 处理小说信息 :param item: :return: ''' self.db.bookInfo.insert(dict(item)) def _process_bookeDetail_item(self,item): ''' 处理小说热度 :param item: :return: ''' # 需要对数据进行清洗，类似：总字数：10120，提取其中的数字 pattern = re.compile('\d+') # 去掉空格和换行 item['novelLabel'] = item['novelLabel'].strip().replace('\n','') match = pattern.search(item['novelAllClick']) item['novelAllClick'] = match.group() if match else item['novelAllClick'] match = pattern.search(item['novelMonthClick']) item['novelMonthClick'] = match.group() if match else item['novelMonthClick'] match = pattern.search(item['novelWeekClick']) item['novelWeekClick'] = match.group() if match else item['novelWeekClick'] match = pattern.search(item['novelAllPopular']) item['novelAllPopular'] = match.group() if match else item['novelAllPopular'] match = pattern.search(item['novelMonthPopular']) item['novelMonthPopular'] = match.group() if match else item['novelMonthPopular'] match = pattern.search(item['novelWeekPopular']) item['novelWeekPopular'] = match.group() if match else item['novelWeekPopular'] match = pattern.search(item['novelAllComm']) item['novelAllComm'] = match.group() if match else item['novelAllComm'] match = pattern.search(item['novelMonthComm']) item['novelMonthComm'] = match.group() if match else item['novelMonthComm'] match = pattern.search(item['novelWeekComm']) item['novelWeekComm'] = match.group() if match else item['novelWeekComm'] self.db.bookhot.insert(dict(item))



* * *



最后在settings中添加如下代码，激活Pipeline。



* * *



ITEM_PIPELINES = { 'zhihuCrawl.pipelines.ZhihucrawlPipeline': 300, }



* * *





17.5　应对反爬虫机制


为了不被反爬虫机制检测到，主要采用了伪造随机User-Agent、自动限速、禁用Cookie等措施。

1.伪造随机User-Agent

还是使用之前编写的中间件，代码如下：



* * *



class RandomUserAgent(object): def __init__(self,agents): self.agents = agents @classmethod def from_crawler(cls,crawler): return cls(crawler.settings.getlist('USER_AGENTS')) def process_request(self,request,spider): request.headers.setdefault('User-Agent', random.choice(self.agents))



* * *



在settings中设置USER_AGENTS的值：



* * *



USER_AGENTS = [ "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)", "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)", "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)", "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)", "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6", "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0", "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5", "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20", "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/ 11.52", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; 360SE)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)", "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/ 21.0.1180.89 Safari/537.1", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1", "Mozilla/5.0 (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5", "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0b13pre) Gecko/20110307 Firefox/ 4.0b13pre", "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:16.0) Gecko/20100101 Firefox/16.0", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11", "Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ub untu/10.10 (maverick) Firefox/3.6.10" ]



* * *



并启用该中间件：



* * *



DOWNLOADER_MIDDLEWARES = { 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'yunqiCrawl.middlewares.RandomUserAgent.RandomUserAgent': 410, }



* * *



2.自动限速的配置



* * *



DOWNLOAD_DELAY=2 AUTOTHROTTLE_ENABLED=True AUTOTHROTTLE_START_DELAY=5 AUTOTHROTTLE_MAX_DELAY=60



* * *



3.禁用Cookie



* * *



COOKIES_ENABLED=False



* * *



采取以上措施之后如果还是会被发现的话，可以写一个HTTP代理中间件来更换IP。





17.6　去重优化


最后在settings中配置scrapy_redis，代码如下：



* * *



# 使用scrapy_redis的调度器 SCHEDULER = "scrapy_redis.scheduler.Scheduler" # 在redis中保持scrapy-redis用到的各个队列，从而允许暂停和暂停后恢复 SCHEDULER_PERSIST = True # 使用scrapy_redis的去重方式 DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter" REDIS_HOST = '127.0.0.1' REDIS_PORT = 6379



* * *



经过以上步骤，一个分布式爬虫就搭建起来了，如果你想在远程服务器上使用，直接将IP和端口进行修改即可。

下面需要讲解一下去重优化的问题，我们看一下scrapy_redis中是如何实现的RFPDupeFilter。关键代码如下：



* * *



def request_seen(self, request): fp = request_fingerprint(request) added = self.server.sadd(self.key, fp) return not added



* * *



scrapy_redis是将生成的fingerprint放到Redis的set数据结构中进行去重的。接着看一下fingerprint是如何产生的，进入request_fingerprint方法中。



* * *



def request_fingerprint(request, include_headers=None): if include_headers: include_headers = tuple([h.lower() for h in sorted(include_headers)]) cache = _fingerprint_cache.setdefault(request, {}) if include_headers not in cache: fp = hashlib.sha1() fp.update(request.method) fp.update(canonicalize_url(request.url)) fp.update(request.body or '') if include_headers: for hdr in include_headers: if hdr in request.headers: fp.update(hdr) for v in request.headers.getlist(hdr): fp.update(v) cache[include_headers] = fp.hexdigest() return cache[include_headers]



* * *



从代码中看到依然调用的是scrapy自带的去重方式，只不过将fingerprint的存储换了个位置。之前我们提到过这是一种比较低效的去重方式，更好的方式是将Redis和BloomFilter结合起来。

推荐一个开源项目：https://github.com/qiyeboy/Scrapy_Redis_Bloomfilter ，它是在scrapy-redis的基础上加入了BloomFilter的功能。使用方法如下：



* * *



git clone https:// github.com/qiyeboy/Scrapy_Redis_Bloomfilter



* * *



将源码包clone到本地，并将BloomfilterOnRedis_Demo目录下的scrapy_redis文件夹拷贝到Scrapy项目中settings.py的同级文件夹，以yunqiCrawl项目为例，在settings.py中增加如下几个字段：

·FILTER_URL=None

·FILTER_HOST='localhost'

·FILTER_PORT=6379

·FILTER_DB=0

·SCHEDULER_QUEUE_CLASS='yunqiCrawl.scrapy_redis.queue.SpiderPriorityQueue'

将之前使用的官方SCHEDULER替换为本地目录的SCHEDULER：



* * *



SCHEDULER = "yunqiCrawl.scrapy_redis.scheduler.Scheduler"



* * *



最后将DUPEFILTER_CLASS=“scrapy_redis.dupefilter.RFPDupeFilter”删除即可。

到此为止实战项目全部完成，最终的抓取效果如图17-4所示。





17.7　小结


本章的实战项目将分布式爬虫和MongoDB集群结合起来，同时对去重进行了优化，整体上来说这个实战项目有实际的工程意义。本项目的源码在GitHub上https://github.com/qiyeboy/spiderbook ，上面会及时更新本项目和整本书的源代码。到本章结束，关于Scrapy的框架的内容也基本上告一段落，希望大家有精力可以阅读Scrapy源码，学习其中的框架思想。



图17-4　存储效果





第18章　人性化PySpider爬虫框架


PySpider是国人binux编写的强大的网络爬虫系统，并带有强大的WebUI。PySpider采用Python语言编写，分布式架构，支持多种数据库后端，包括强大的WebUI支持脚本编辑器、任务监视器、项目管理器以及结果查看器。PySpider源码见https://github.com/binux/pyspider ，已经有7000多star，是一个优秀的开源项目。在线示例位于http://demo.pyspider.org/ 。





18.1　PySpider与Scrapy


说到Python中的开源爬虫框架，Scrapy是最先被大家提及的，这是一个相对成熟的框架，有着丰富的文档和开放的社区交流空间。相对于Scrapy来说，PySpider算是一个新秀，但是不容小觑。下面说一下PySpider的具体特性：

1）Python脚本控制，可以用任何你喜欢的html解析包（内置pyquery）。

2）Web界面编写调试脚本、起停脚本、监控执行状态、查看活动历史、获取结果产出。

3）支持MySQL、MongoDB、Redis、SQLite等数据库。

4）支持抓取JavaScript的页面。

5）组件可替换，支持单机/分布式部署，支持Docker部署。

6）强大的调度控制。

7）支持RabbitMQ、Beanstalk、Redis和Kombu作为消息队列。

从内容上来说，两者具有的功能差不多，但还是有一些不同：

·原生的Scrapy并不支持js渲染，需要单独下载scrapy-splash进行配置，而PySpider则支持phantomjs第三方渲染。

·PySpider内置pyquery选择器，Scrapy有XPath和CSS选择器。

·Scrapy全部采用命令行操作，PySpider有较好的WebUI，更加直观。

·PySpider易于调试，Scrapy调试方式稍显复杂，并不直观。

·Scrapy扩展性更强，可以自定义功能，PySpider这方面稍显不足。

以上两种框架各有各的风格，总体来说PySpider使用更加简单，可以快速上手，适合工程化生产爬虫，而Scrapy适合用来进行二次开发，根据项目需求进行自定义拓展。





18.2　安装PySpider


安装PySpider非常简单，在命令行中输入pip install pyspider即可完成安装。对于Ubuntu用户，请提前安装好以下支持类库：



* * *



sudo apt-get install python python-dev python-distribute python-pip libcurl4-openssl-dev libxml2-dev libxslt1-dev python-lxml



* * *



如果安装过程中没有出现问题，基本上是成功的。

在命令行中输入pyspider all，效果如图18-1所示。



图18-1　PySpider all

此时在浏览器中输入http://127.0.0.1:5000 ，如果出现类似如图18-2所示的界面，则证明安装基本上没有问题。



图18-2　PySpider WebUI





18.3　创建豆瓣爬虫


从本节开始以豆瓣爬虫为例讲解PySpider的用法。豆瓣爬虫的目标是爬取豆瓣电影的详细信息。接下来我们在PySpider的WebUI界面中创建doubanMovie项目，点击Create按钮，在弹出的对话框中输入doubanMovie，如图18-3所示。



图18-3　创建doubanMovie

直接点击Create，即可实现项目创建，直接跳转到项目调试界面，如图18-4所示。



图18-4　项目调试区

从图18-4中，我们可以看到整个页面分为两栏，左侧是爬取页面预览区域，右侧是代码调试区域。下面对这两个区域进行说明。

左侧区域：

·代码区域：请求所对应的JSON变量。在PySpider中，每个请求都有与之对应的JSON变量，变量中包括回调函数、方法名、请求链接、请求数据等。

·代码区域右上角的Run：点击Run按钮，就会执行这个请求，可以在代码区域显示请求的结果，有可能是异常信息。

·底部enable css selector helper选项：抓取页面之后，点击此按钮，可以方便地获取页面中某个元素的CSS选择表达式。

·底部web选项：抓取页面的实时预览图。

·底部html选项：抓取页面的HTML代码。

·底部follows选项：如果当前抓取方法中又产生了新的爬取请求，那么新产生的请求就会出现在follows里。

·底部messages选项：爬取过程中输出的一些信息。

右侧区域：

·整个区域属于编码区域，代码编写完成后点击右上角的Save按钮进行保存。

·上端WebDAV Mod选项用于打开调试模式，使左侧最大化，便于观察调试过程。

接下来根据爬取目标“豆瓣电影”来分析抽取数据的方式。首先来到豆瓣电影的首页，发现从首页开始爬，并不能包含所有的电影，因此选择抓取分类下的所有标签页的电影，链接为https://movie.douban.com/tag/ ，如图18-5所示。



图18-5　豆瓣电影

这时候将https://movie.douban.com/tag/ 填到右侧代码区onstart方法中，点击Save保存。代码如下：



* * *



@every(minutes=24 * 60) def on_start(self): self.crawl('http://movie.douban.com/tag/', callback=self.index_page)



* * *



on_start方法说明：

·self.crawl告诉PySpider抓取指定页面，然后使用callback函数对结果进行解析。

·@every修饰器的括号中是时间，表示on_start每天会执行一次，来抓取最新的电影。

这个时候点击Run按钮生成第一个请求，新的请求可以在follows中查看，并点击请求右侧的箭头，开始执行，如图18-6所示。



图18-6　发送请求

请求发送成功后，切换到web选项可以预览实时的页面，同时follows又根据callback中指定的index_page方法产生了新的请求，如图18-7所示。

我们需要从响应页面中提取各种分类的链接，然后进入电影列表页，比如https://movie.douban.com/tag/ 爱情，如图18-8所示。

最后再从电影列表页提取每一部影片的链接，然后通过链接进入电影详情页，比如点击《七月与安生》这部电影，如图18-9所示。提取其中的标题、导演、主演、类型和评分等信息。



图18-7　页面预览



图18-8　电影列表页



图18-9　电影详情页

以上就是提取电影详情的逻辑步骤，但是这一切都源于对页面数据的提取，必然要提到选择器。





18.4　选择器


PySpider内置了PyQuery来解析网页数据。PyQuery是Python仿照jQuery的严格实现，语法与jQuery几乎完全相同，因此非常适合有Web前端基础的读者快速入手。下面讲解一下PyQuery的基本用法。





18.4.1　PyQuery的用法


PySpider已经内置了PyQuery库，不需要我们进行安装。下面从四个方面进行讲解。

1.PyQuery对象初始化

·使用HTML字符串进行初始化，示例如下：



* * *



from pyquery import PyQuery as pq d = pq("<html></html>")



* * *



·可以使用lxml对HTML代码进行规范化处理，将其转化为清晰完整的HTML代码，示例如下：



* * *



from pyquery import PyQuery as pq from lxml import etree d = pq(etree.fromstring("<html></html>"))



* * *



·通过传入URL的方式进行初始化，相当于直接访问网页。示例如下：



* * *



from pyquery import PyQuery as pq d = pq('http://www.google.com')



* * *



·通过指定HTML文件的路径完成初始化。示例如下：



* * *



from pyquery import PyQuery as pq d = pq(filename='index.html')



* * *



2.属性操作

在PyQuery中，可以完全按照jQuery的语法来进行PyQuery的操作。示例如下：



* * *



from pyquery import PyQuery as pq p = pq('<p id="hello" class="hello"></p>')('p') print p.attr("id") print p.attr["id"] print p.attr("id", "plop") print p.attr("id", "hello") print p.attr(id='hello', class_='hello2') p.attr.class_ = 'world' p.addClass("!!!") print p print p.css("font-size", "15px") print p.attr.style



* * *



输出结果为：



* * *



hello hello <p id="plop" class="hello"/> <p id="hello" class="hello"/> <p id="hello" class="hello2"/> <p id="hello" class="world !!!"/> <p id="hello" class="world !!!" style="font-size: 15px"/> font-size: 15px



* * *



PyQuery不仅可以读取属性和样式的值，还可以任意修改属性和样式的值。

3.DOM操作

DOM操作和jQuery一样，示例如下：



* * *



from pyquery import PyQuery as pq d = pq('<p class="hello" id="hello">you know Python rocks</p>') d('p').append(' check out <a href="http://reddit.com/r/python"><span>reddit</span> </a>') print d p = d('p') p.prepend('check out <a href="http://reddit.com/r/python">reddit</a>') print p



* * *



输出结果为：



* * *



<p class="hello" id="hello">you know Python rocks check out <a href="http://reddit.com/r/python"><span>reddit</span></a></p> <p class="hello" id="hello">check out <a href="http://reddit.com/r/python">reddit </a>you know Python rocks check out <a href="http://reddit.com/r/python"><span>reddit </span></a></p>



* * *



4.元素遍历

对于网页数据抽取来说，更多的时候是抽取出同一类型的数据，这就需要用到元素的遍历。示例代码如下：



* * *



from pyquery import PyQuery as pq html_cont = ''' <div> <ul> <li class="one">first item</li> <li class="two"><a href="link2.html">second</a></li> <li class="four"><a href="link3.html">third</a></li> <li class="three"><a href="link4.html"><span class="bold">fourth</span> </a></li> </ul> </div> ''' doc = pq(html_cont) lis = doc('li') for li in lis.items(): print li.html()



* * *



输出结果为：



* * *



first item <a href="link2.html">second</a> <a href="link3.html">third</a> <a href="link4.html"><span class="bold">fourth</span></a>



* * *



以上讲解了一些PyQuery的基础知识，如果你对jQuery语法不熟，建议先学习jQuery，再回来使用PyQuery或者使用第三方的包（比如lxml和bs4）对response进行解析。





18.4.2　解析数据


讲解完PyQuery，下面继续进行doubanMovie项目。我们需要从如图18-7的页面中提取出电影列表页的url，可以使用Firebug获取电影列表页的url的CSS表达式，也可以使用enable css selector helper工具获取（有时候不好用）。index_page代码如下：



* * *



@config(age=10 * 24 * 60 * 60) def index_page(self, response): for each in response.doc('.tagCol>tbody>tr>td>a').items(): self.crawl(each.attr.href, callback=self.list_page)



* * *



经过index_page方法之后生成新的请求，如图18-10所示。



图18-10　抽取效果

下面继续点击每个请求后面的箭头进行发送，获取响应后切换到web选项，对图18-8所示页面进行电影url的抽取和翻页操作。

·电影url的CSS表达式为.pl2>a。

·翻页链接的CSS表达式为.next>a。

list_page代码如下：



* * *



def list_page(self,response): # 获取电影url，然后调用detail_page方法解析电影详情 for each in response.doc('.pl2>a').items(): self.crawl(each.attr.href, callback=self.detail_page) # 进行翻页操作 for each in response.doc('.next>a').items(): self.crawl(each.attr.href, callback=self.list_page)



* * *



保存代码，点击Run就会看到抽取到的电影url，如图18-11所示。

继续重复上述步骤，点击每一行后面的箭头，发送请求。获取响应后切换到web选项，对图18-9所示页面进行电影详情的分析和抽取。下面直接给出详情页的CSS表达式：

·电影名称：#content>h1>span[property=“v：itemreviewed”]

·电影年份：#content>h1>span[class=“year”]

·电影导演：.attrs>a[rel=“v：directedBy”]

·电影主演：.attrs>span>a[rel=“v：starring”]

·电影类型：#info>span[property=“v：genre”]

·电影评分：.ll.rating_num



图18-11　电影URL

detail_page方法代码如下：



* * *



def detail_page(self, response): title = response.doc('# content>h1>span[property="v:itemreviewed"]').text() time = response.doc('# content>h1>span[class="year"]').text() director = response.doc('.attrs>a[rel="v:directedBy"]').text() actor=[] genre=[] for each in response.doc('a[rel="v:starring"]').items(): actor.append(each.text()) for each in response.doc('# info>span[property="v:gen



* * *



将代码进行保存，点击Run就会看到抽取到的电影详情，如图18-12所示。



图18-12　电影详情

经过以上步骤，代码编写和调试基本完成，代码调试直观是PySpider非常明显的优点。

最后从代码编辑界面回到Dashboard项目主界面，找到你的项目，将项目中的status状态修改为RUNNING或者DEBUG，点击Run按钮，爬虫就可以工作了，如图18-13所示。



图18-13　启动爬虫项目

如果想查看抓取结果，点击项目那一栏中的Results按钮，就可以看到如图18-14所示的效果。



图18-14　抓取结果

可以通过右上角的按钮，将结果保存为JSON或CSV等格式。





18.5　Ajax和HTTP请求


现在越来越多的网站都采用了Ajax异步加载技术，爬取难度加大，同时反爬手段对HTTP请求的检测也越来越成熟，那么如何使用PySpider爬取Ajax请求和修改HTTP请求呢？这就是本节要讲的内容。





18.5.1　Ajax爬取


一般对于Ajax请求的爬取主要有两种方法，已经在第9章进行了详细的讲解，同样PySpider也支持这两种方法。其中一种方式是直接模拟Ajax请求。还是以MTime电影网为例，如果大家不熟悉，可以回顾一下第9章。如图18-15所示，票房链接为：http://service.library.mtime.com/Movie.apiAjax_CallBack=true&？Ajax_CallBackType=Mtime.Library.Services&Ajax_CallBackMethod=GetMovieOverviewRating&Ajax_CrossDomain=1&Ajax_RequestUrl=http%3A%2F%2Fmovie.mtime.com%2F217130%2F&t=2016111321341844484&Ajax_CallBackArgument0=217130



图18-15　Ajax请求

响应信息为：



* * *



var result_2016111321341844484 = { "val：ue":{"isRelease":true,"movieRating":{"Mov- ieId":217130,"RatingFinal":8,"RDirectorFinal":8.3,"ROtherFinal":7.5,"RPictureFinal": 8.8,"RShowFinal":0,"RStoryFinal":7.7,"RTotalFinal":0,"Usercount":3106,"AttitudeCount":2736,"UserId":0,"EnterTime":0,"JustTotal":0,"RatingCount":0,"TitleCn":"","TitleEn":"","Year":"","IP":0},"movieTitle":"比利·林恩的中场战事","tweetId":0,"userLastComment ":"","userLastCommentUrl":"","releaseType":1,"boxOffice":{"Rank":14,"TotalBoxOffice":"1.59","TotalBoxOfficeUnit":"亿","TodayBoxOffice":"9.5","TodayBoxOfficeUnit":"万","ShowDays":22,"EndDate":"2016-12-02 11:30","FirstDayBoxOffice":"2542.51","First DayBoxOfficeUnit":"万"}},"error":null};var movieOverviewRatingResult=result_20161113 21341844484;



* * *



我们可以通过如下代码对请求进行模拟。



* * *



from pyspider.libs.base_handler import * class Handler(BaseHandler): @every(minutes=10) def on_start(self): self.crawl('http://service.library.mtime.com/Movie.api' 'Ajax_CallBack=true&Ajax_CallBackType=Mtime.Library.Services' '&Ajax_CallBackMethod=GetMovieOverviewRating' '&Ajax_CrossDomain=1' '&Ajax_RequestUrl=http%3A%2F%2Fmovie.mtime.com%2F217130%2F' '&t=2016111321341844484' '&Ajax_CallBackArgument0=217130', callback=self.index_page) @config(age=10*60) def index_page(self, response): return response.json



* * *



如果想解析响应的内容，直接可以使用第9章的代码，这里不再赘述。





18.5.2　HTTP请求实现


1.设置请求头



* * *



headers ={'User-Agent':random.choice(USER_AGENTS), 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3', 'Accept-Encoding': 'gzip, deflate, br'} @every(minutes=24 * 60) def on_start(self): self.crawl('http://movie.douban.com/tag/', headers = self.headers,callback= self.index_page)



* * *



2.设置cookie



* * *



def on_start(self): self.crawl('http://movie.douban.com/tag/', cookies={"key": value},callback=self.index_page)



* * *



3.GET请求（带参数）



* * *



self.crawl('http://httpbin.org/get', callback=self.callback,params={'a': 123, 'b': 'c'}) self.crawl('http://httpbin.org/geta=123&b=c', callback=self.callback)



* * *



这两个请求效果是一样的。

4.POST请求



* * *



def on_start(self): self.crawl('http://httpbin.org/post', callback=self.callback, method='POST', data={'a': 123, 'b': 'c'})



* * *



5.代理设置

PySpider仅支持HTTP代理，代理格式：username：password@hostname：port。可以使用如下代码对项目进行代理设置：



* * *



class Handler(BaseHandler): crawl_config = { 'proxy': 'localhost:8080' }



* * *





18.6　PySpider和PhantomJS


除了模拟Ajax请求来突破动态网站之外，PhantomJS也是一个不错的选择，PySpider对PhantomJS提供了很好的支持，而且使用简单。





18.6.1　使用PhantomJS


在使用PhantomJS之前，首先确保已经正确安装了PhantomJS，而且安装路径配置到Path环境变量中，这些内容已经在第9章讲过。

当你在运行PySpider命令时，启动的是all模式，也就是在命令行中输入：pyspider all。这时候PhantomJS已经启动，这一点通过查看当前进程可以发现。使用PhantomJS时，我们只需要在crawl方法中添加fetch_type='js'参数即可。我们可以对比一下使用前与使用后的不同，以MTime电影网为例，还是观察电影评分的位置。提前创建一个MTime项目，在默认代码基础上将起始URL换成http://movie.mtime.com/217130/:



* * *



from pyspider.libs.base_handler import * class Handler(BaseHandler): crawl_config = { } @every(minutes=24 * 60) def on_start(self): self.crawl('http://movie.mtime.com/217130/', callback=self.index_page) @config(age=10 * 24 * 60 * 60) def index_page(self, response): for each in response.doc('a[href^="http"]').items(): self.crawl(each.attr.href, callback=self.detail_page) @config(priority=2) def detail_page(self, response): return { "url": response.url, "title": response.doc('title').text(), }



* * *



保存代码并运行，如图18-16所示，在Web预览界面，我们看到是没有票房和评分信息的页面。

这个时候，我们在crawl方法中添加fetch_type='js'，修改如下：



* * *



self.crawl('http://movie.mtime.com/217130/', fetch_type='js', callback=self.index_ page)



* * *



保存代码并运行，如图18-17所示，在web预览界面，我们已经看到完整的票房和评分信息的页面。

在渲染之后的页面上，使用enable css selector helper功能就可以直接获取选定元素的CSS表达式。



图18-16　无PhantomJS渲染



图18-17　PhantomJS渲染





18.6.2　运行JavaScript


不知道大家是否还记得，在第9章爬取去哪网时，两次加载才能获取当前页面完整的数据，采取的措施是执行js代码：window.scrollTo（0，document.body.scrollHeight）；，将页面滑到底部实现第二次加载，PySpider也提供了接口来实现js脚本的执行，通过在crawl基础上额外添加js_script的方式。示例代码如下：



* * *



class Handler(BaseHandler): def on_start(self): self.crawl('http://www.pinterest.com/categories/popular/', fetch_type='js', js_script=""" function() { window.scrollTo(0,document.body.scrollHeight); } """, callback=self.index_page) def index_page(self, response): return { "url": response.url, "images": [{ "title": x('.richPinGridTitle').text(), "img": x('.pinImg').attr('src'), "author": x('.creditName').text(), } for x in response.doc('.item').items() if x('.pinImg')] }



* * *





18.7　数据存储


对于数据存储，PySpider采用自带的ResultDB方式，这种设计是为了方便在WebUI预览，之后可以将数据下载成JSON等格式的文件，但是这种做法对于稍微大量的数据会很不实用，不适合工程化。要实现自定义存储，我们需要重写on_result方法，doubanMovie项目修改如下：



* * *



# coding:utf-8 from pymongo import MongoClient from pyspider.libs.base_handler import * class MongoStore(object): def __init__(self): client = MongoClient() db = client.douban self.movies = db.movies def insert(self,result): if result: self.movies.insert(result) class Handler(BaseHandler): crawl_config = { } mongo = MongoStore() headers ={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/ 20100101 Firefox/50.0', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9, */*;q=0.8', 'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3', 'Accept-Encoding': 'gzip, deflate, br', 'Referer':'http://www.douban.com/'} @every(minutes=24 * 60) def on_start(self): self.crawl('http://movie.douban.com/tag/', headers = self.headers,callback=self.index_page,validate_cert=False) @config(age=10 * 24 * 60 * 60) def index_page(self, response): for each in response.doc('.tagCol>tbody>tr>td>a').items(): self.crawl(each.attr.href, headers = self.headers,callback=self.　 list_page,validate_cert=False) def list_page(self,response): for each in response.doc('.pl2>a').items(): self.crawl(each.attr.href, headers = self.headers,callback=self.　 detail_page,validate_cert=False) for each in response.doc('.next>a').items(): self.crawl(each.attr.href, headers = self.headers,callback=self.　 list_page,validate_cert=False) def detail_page(self, response): title = response.doc('# content>h1>span[property="v:itemreviewed"]').text() time = response.doc('# content>h1>span[class="year"]').text() director = response.doc('.attrs>a[rel="v:directedBy"]').text() actor=[] genre=[] for each in response.doc('a[rel="v:starring"]').items(): actor.append(each.text()) for each in response.doc('# info>span[property="v:genre"]').items(): genre.append(each.text()) rating = response.doc('.ll.rating_num').text() return { "url": response.url, "title": title, "time":time, "director":director, "actor":actor, "genre":genre, "rating":rating } def on_result(self, result): self.mongo.insert(result) super(Handler, self).on_result(result)



* * *



代码中添加了MongoStore用于初始化数据库连接和实现插入操作，在handler类中重写基类BaseHandler的on_result方法，实现对数据的存储和插入，同时调用BaseHandler中的on_result方法添加到默认的ResultDB中。最终存储的效果如图18-18所示。



图18-18　MongoDB存储





18.8　PySpider爬虫架构


前几节已经讲解了PySpider的基础用法，下面说明一下PySpider爬虫架构的各个部分。PySpider的架构主要分为scheduler（调度器）、fetcher（抓取器）、processor（脚本执行），如图18-19所示。



图18-19　PySpider架构

各个组件间使用消息队列连接，scheduler负责整体的调度控制，除了scheduler是单点的，fetcher和processor都可以实现多实例分布式部署。

任务由scheduler发起调度，fetcher抓取网页内容，processor执行预先编写的Python脚本，输出结果或产生新的提链任务（发往scheduler），形成闭环。

每个脚本可以灵活使用各种Python库对页面进行解析，使用框架API控制下一步抓取动作，通过设置回调控制解析动作。

具体功能如下。

webui的功能：

·web的可视化任务监控。

·web脚本编写，单步调试。

·异常捕获、log捕获、print捕获等。

scheduler的功能：

·任务优先级。

·周期定时任务。

·流量控制。

·基于时间周期或前链标签（例如更新时间）的重抓取调度。

fetcher的功能：

·dataurl支持，用于假抓取模拟传递。

·method、header、cookie、proxy、etag、last_modified、timeout等等抓取调度控制。

·可以通过适配类似phantomjs的webkit引擎支持渲染

processor的功能：

·内置的pyquery，以jQuery解析页面。

·在脚本中完全控制调度抓取的各项参数。

·可以向后链传递信息。

·异常捕获。





18.9　小结


本章主要介绍了PySpider框架的各项特性和基本用法，大家如果想深入学习，请到http://docs.pyspider.org/en/latest/ 仔细阅读官方文档。PySpider是一种工程化批量生产爬虫的思路，虽然功能已经很强大了，但是还是存在扩展性不足、在线脚本编辑略显不方便的问题，不过仍然是一个非常值得期待的爬虫框架。

本书由“行行”整理，如果你不知道读什么书或者想获得更多免费电子书请加小编微信或QQ：2338856113 小编也和结交一些喜欢读书的朋友 或者关注小编个人微信公众号名称：幸福的味道 为了方便书友朋友找书和看书，小编自己做了一个电子书下载网站，网站的名称为：周读 网址：www.ireadweek.com





如果你不知道读什么书，

就关注这个微信号。



微信公众号名称：幸福的味道

加小编微信一起读书

小编微信号：2338856113



【幸福的味道】已提供200个不同类型的书单

1、 历届茅盾文学奖获奖作品

2、 每年豆瓣，当当，亚马逊年度图书销售排行榜

3、 25岁前一定要读的25本书

4、 有生之年，你一定要看的25部外国纯文学名著

5、 有生之年，你一定要看的20部中国现当代名著

6、 美国亚马逊编辑推荐的一生必读书单100本

7、 30个领域30本不容错过的入门书

8、 这20本书，是各领域的巅峰之作

9、 这7本书，教你如何高效读书

10、 80万书虫力荐的“给五星都不够”的30本书

关注“幸福的味道”微信公众号，即可查看对应书单和得到电子书

也可以在我的网站（周读）www.ireadweek.com 自行下载





